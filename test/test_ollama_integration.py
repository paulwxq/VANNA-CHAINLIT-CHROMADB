#!/usr/bin/env python3
"""
æµ‹è¯•Ollamaé›†æˆåŠŸèƒ½çš„è„šæœ¬
ç”¨äºéªŒè¯Ollama LLMå’ŒEmbeddingæ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

def test_ollama_llm():
    """æµ‹è¯•Ollama LLMåŠŸèƒ½"""
    print("=== æµ‹è¯•Ollama LLM ===")
    
    try:
        from customollama.ollama_chat import OllamaChat
        
        # æµ‹è¯•é…ç½®
        config = {
            "base_url": "http://localhost:11434",
            "model": "qwen2.5:7b",
            "temperature": 0.7,
            "timeout": 60
        }
        
        # åˆ›å»ºå®ä¾‹
        ollama_chat = OllamaChat(config=config)
        
        # æµ‹è¯•è¿æ¥
        print("æµ‹è¯•Ollamaè¿æ¥...")
        test_result = ollama_chat.test_connection()
        
        if test_result["success"]:
            print(f"âœ… Ollama LLMè¿æ¥æˆåŠŸ: {test_result['message']}")
        else:
            print(f"âŒ Ollama LLMè¿æ¥å¤±è´¥: {test_result['message']}")
            return False
            
        # æµ‹è¯•ç®€å•å¯¹è¯
        print("\næµ‹è¯•ç®€å•å¯¹è¯...")
        response = ollama_chat.chat_with_llm("ä½ å¥½ï¼Œè¯·ç®€å•ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
        print(f"LLMå“åº”: {response}")
        
        return True
        
    except Exception as e:
        print(f"âŒ Ollama LLMæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ollama_embedding():
    """æµ‹è¯•Ollama EmbeddingåŠŸèƒ½"""
    print("\n=== æµ‹è¯•Ollama Embedding ===")
    
    try:
        from customollama.ollama_embedding import OllamaEmbeddingFunction
        
        # åˆ›å»ºå®ä¾‹
        embedding_func = OllamaEmbeddingFunction(
            model_name="nomic-embed-text",
            base_url="http://localhost:11434",
            embedding_dimension=768
        )
        
        # æµ‹è¯•è¿æ¥
        print("æµ‹è¯•Ollama Embeddingè¿æ¥...")
        test_result = embedding_func.test_connection()
        
        if test_result["success"]:
            print(f"âœ… Ollama Embeddingè¿æ¥æˆåŠŸ: {test_result['message']}")
        else:
            print(f"âŒ Ollama Embeddingè¿æ¥å¤±è´¥: {test_result['message']}")
            return False
            
        # æµ‹è¯•ç”Ÿæˆembedding
        print("\næµ‹è¯•ç”Ÿæˆembedding...")
        test_texts = ["è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬", "å¦ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"]
        embeddings = embedding_func(test_texts)
        
        print(f"ç”Ÿæˆäº† {len(embeddings)} ä¸ªembeddingå‘é‡")
        for i, emb in enumerate(embeddings):
            print(f"æ–‡æœ¬ {i+1} çš„embeddingç»´åº¦: {len(emb)}")
            
        return True
        
    except Exception as e:
        print(f"âŒ Ollama Embeddingæµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_ollama_with_config():
    """æµ‹è¯•ä½¿ç”¨é…ç½®æ–‡ä»¶çš„OllamaåŠŸèƒ½"""
    print("\n=== æµ‹è¯•é…ç½®æ–‡ä»¶ä¸­çš„Ollamaè®¾ç½® ===")
    
    try:
        import app_config
        from common.utils import print_current_config, is_using_ollama_llm, is_using_ollama_embedding
        
        # ä¿å­˜åŸå§‹é…ç½®
        original_llm_type = app_config.LLM_MODEL_TYPE
        original_embedding_type = app_config.EMBEDDING_MODEL_TYPE
        
        try:
            # è®¾ç½®ä¸ºOllamaæ¨¡å¼
            app_config.LLM_MODEL_TYPE = "ollama"
            app_config.EMBEDDING_MODEL_TYPE = "ollama"
            
            print("å½“å‰é…ç½®:")
            print_current_config()
            
            print(f"\nä½¿ç”¨Ollama LLM: {is_using_ollama_llm()}")
            print(f"ä½¿ç”¨Ollama Embedding: {is_using_ollama_embedding()}")
            
            # æµ‹è¯•embeddingå‡½æ•°
            print("\næµ‹è¯•é€šè¿‡é…ç½®è·å–embeddingå‡½æ•°...")
            from embedding_function import get_embedding_function
            
            embedding_func = get_embedding_function()
            print(f"æˆåŠŸåˆ›å»ºembeddingå‡½æ•°: {type(embedding_func).__name__}")
            
            # æµ‹è¯•å·¥å‚å‡½æ•°ï¼ˆå¦‚æœOllamaæœåŠ¡å¯ç”¨çš„è¯ï¼‰
            print("\næµ‹è¯•å·¥å‚å‡½æ•°...")
            try:
                from vanna_llm_factory import create_vanna_instance
                vn = create_vanna_instance()
                print(f"âœ… æˆåŠŸåˆ›å»ºVannaå®ä¾‹: {type(vn).__name__}")
                return True
            except Exception as e:
                print(f"âš ï¸  å·¥å‚å‡½æ•°æµ‹è¯•å¤±è´¥ï¼ˆå¯èƒ½æ˜¯OllamaæœåŠ¡æœªå¯åŠ¨ï¼‰: {e}")
                return True  # è¿™ä¸ç®—å¤±è´¥ï¼Œåªæ˜¯æœåŠ¡æœªå¯åŠ¨
                
        finally:
            # æ¢å¤åŸå§‹é…ç½®
            app_config.LLM_MODEL_TYPE = original_llm_type
            app_config.EMBEDDING_MODEL_TYPE = original_embedding_type
            
    except Exception as e:
        print(f"âŒ é…ç½®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def test_mixed_configurations():
    """æµ‹è¯•æ··åˆé…ç½®ï¼ˆAPI + Ollamaï¼‰"""
    print("\n=== æµ‹è¯•æ··åˆé…ç½® ===")
    
    try:
        import app_config
        from common.utils import print_current_config
        
        # ä¿å­˜åŸå§‹é…ç½®
        original_llm_type = app_config.LLM_MODEL_TYPE
        original_embedding_type = app_config.EMBEDDING_MODEL_TYPE
        
        try:
            # æµ‹è¯•é…ç½®1ï¼šAPI LLM + Ollama Embedding
            print("\n--- æµ‹è¯•: API LLM + Ollama Embedding ---")
            app_config.LLM_MODEL_TYPE = "api"
            app_config.EMBEDDING_MODEL_TYPE = "ollama"
            print_current_config()
            
            from embedding_function import get_embedding_function
            embedding_func = get_embedding_function()
            print(f"Embeddingå‡½æ•°ç±»å‹: {type(embedding_func).__name__}")
            
            # æµ‹è¯•é…ç½®2ï¼šOllama LLM + API Embedding
            print("\n--- æµ‹è¯•: Ollama LLM + API Embedding ---")
            app_config.LLM_MODEL_TYPE = "ollama"
            app_config.EMBEDDING_MODEL_TYPE = "api"
            print_current_config()
            
            embedding_func = get_embedding_function()
            print(f"Embeddingå‡½æ•°ç±»å‹: {type(embedding_func).__name__}")
            
            print("âœ… æ··åˆé…ç½®æµ‹è¯•é€šè¿‡")
            return True
            
        finally:
            # æ¢å¤åŸå§‹é…ç½®
            app_config.LLM_MODEL_TYPE = original_llm_type
            app_config.EMBEDDING_MODEL_TYPE = original_embedding_type
            
    except Exception as e:
        print(f"âŒ æ··åˆé…ç½®æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹æµ‹è¯•Ollamaé›†æˆåŠŸèƒ½...")
    print("æ³¨æ„: è¿™äº›æµ‹è¯•éœ€è¦OllamaæœåŠ¡è¿è¡Œåœ¨ http://localhost:11434")
    print("=" * 60)
    
    results = []
    
    # æµ‹è¯•é…ç½®å’Œå·¥å…·å‡½æ•°ï¼ˆä¸éœ€è¦OllamaæœåŠ¡ï¼‰
    results.append(("é…ç½®æ–‡ä»¶æµ‹è¯•", test_ollama_with_config()))
    results.append(("æ··åˆé…ç½®æµ‹è¯•", test_mixed_configurations()))
    
    # æµ‹è¯•å®é™…çš„OllamaåŠŸèƒ½ï¼ˆéœ€è¦OllamaæœåŠ¡ï¼‰
    print(f"\n{'='*60}")
    print("ä»¥ä¸‹æµ‹è¯•éœ€è¦OllamaæœåŠ¡è¿è¡Œï¼Œå¦‚æœå¤±è´¥å¯èƒ½æ˜¯æœåŠ¡æœªå¯åŠ¨")
    print("=" * 60)
    
    results.append(("Ollama LLM", test_ollama_llm()))
    results.append(("Ollama Embedding", test_ollama_embedding()))
    
    # æ€»ç»“
    print(f"\n{'='*60}")
    print("æµ‹è¯•ç»“æœæ€»ç»“:")
    print("=" * 60)
    
    for test_name, success in results:
        status = "âœ… é€šè¿‡" if success else "âŒ å¤±è´¥"
        print(f"{test_name}: {status}")
    
    total_passed = sum(1 for _, success in results if success)
    print(f"\næ€»è®¡: {total_passed}/{len(results)} ä¸ªæµ‹è¯•é€šè¿‡")
    
    if total_passed == len(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼Ollamaé›†æˆåŠŸèƒ½æ­£å¸¸ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥OllamaæœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚")

if __name__ == "__main__":
    main() 