# Data Pipeline API è¯¦ç»†è®¾è®¡æ–‡æ¡£

## é¡¹ç›®æ¦‚è¿°

æœ¬æ–‡æ¡£æ˜¯åŸºäºæ¦‚è¦è®¾è®¡æ–‡æ¡£å’Œç°æœ‰ä»£ç ç»“æ„ï¼Œå¯¹Data Pipeline APIç³»ç»Ÿçš„è¯¦ç»†æŠ€æœ¯å®ç°è®¾è®¡ã€‚è¯¥ç³»ç»Ÿå°†ä¸ºWeb UIæä¾›å®Œæ•´çš„æ•°æ®ç®¡é“è°ƒåº¦ã€æ‰§è¡Œç›‘æ§å’Œæ—¥å¿—ç®¡ç†åŠŸèƒ½ã€‚

## æ ¸å¿ƒéœ€æ±‚åˆ†æ

### 1. ä¸šåŠ¡éœ€æ±‚
- **APIè°ƒåº¦æ‰§è¡Œ**ï¼šé€šè¿‡REST APIè°ƒåº¦æ‰§è¡Œ `./data_pipeline/schema_workflow.py`
- **æ‰§è¡Œç›‘æ§**ï¼šå®æ—¶æŸ¥çœ‹ä»»åŠ¡æ‰§è¡ŒçŠ¶æ€å’Œè¿›åº¦
- **æ—¥å¿—é›†ä¸­ç®¡ç†**ï¼šæ‰€æœ‰æ—¥å¿—å†™å…¥ä»»åŠ¡ç‰¹å®šçš„å­ç›®å½•
- **æ­¥éª¤æ§åˆ¶**ï¼šæ”¯æŒé€šè¿‡å‚æ•°æ§åˆ¶æ‰§è¡Œç‰¹å®šæ­¥éª¤
- **æ•°æ®åº“æ—¥å¿—è®°å½•**ï¼šå…³é”®æ­¥éª¤ä¿¡æ¯å†™å…¥PostgreSQLæ•°æ®åº“

### 2. æŠ€æœ¯çº¦æŸ
- å¤ç”¨ç°æœ‰çš„ `SchemaWorkflowOrchestrator` æ¶æ„
- é›†æˆç°æœ‰çš„æ—¥å¿—ç³»ç»Ÿ (`core.logging`)
- ä½¿ç”¨ç°æœ‰çš„Flaskåº”ç”¨ (`citu_app.py`) ä½œä¸ºAPIæ‰¿è½½
- ä¿æŒä¸ç°æœ‰æ•°æ®åº“é…ç½®çš„å…¼å®¹æ€§

## ç³»ç»Ÿæ¶æ„è®¾è®¡

### 1. æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Web Frontend      â”‚    â”‚   Flask API         â”‚    â”‚  Schema Workflow    â”‚
â”‚                     â”‚ â”€â†’ â”‚   (citu_app.py)     â”‚ â”€â†’ â”‚  (subprocess)       â”‚
â”‚ - ä»»åŠ¡åˆ›å»ºè¡¨å•      â”‚    â”‚ - ä»»åŠ¡è°ƒåº¦          â”‚    â”‚ - DDLç”Ÿæˆ           â”‚
â”‚ - è¿›åº¦ç›‘æ§ç•Œé¢      â”‚    â”‚ - çŠ¶æ€æŸ¥è¯¢          â”‚    â”‚ - Q&Aç”Ÿæˆ           â”‚
â”‚ - æ—¥å¿—æŸ¥çœ‹å™¨        â”‚    â”‚ - æ—¥å¿—API           â”‚    â”‚ - SQLéªŒè¯           â”‚
â”‚ - æ–‡ä»¶ç®¡ç†å™¨        â”‚    â”‚ - æ–‡ä»¶ç®¡ç†          â”‚    â”‚ - è®­ç»ƒæ•°æ®åŠ è½½      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â”‚                           â”‚
                                    â–¼                           â–¼
                           â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                           â”‚  PostgreSQL DB      â”‚    â”‚  File System        â”‚
                           â”‚ - ä»»åŠ¡çŠ¶æ€è¡¨        â”‚    â”‚ - ä»»åŠ¡ç›®å½•          â”‚
                           â”‚ - æ—¥å¿—è®°å½•è¡¨        â”‚    â”‚ - è¾“å‡ºæ–‡ä»¶          â”‚
                           â”‚ - æ–‡ä»¶è¾“å‡ºè¡¨        â”‚    â”‚ - æ—¥å¿—æ–‡ä»¶          â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2. è¿›ç¨‹åˆ†ç¦»è®¾è®¡

```
HTTP Request â”€â”€â”
               â”‚
               â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    subprocess.Popen    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚ Flask API   â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ task_executor.py â”‚
        â”‚ Process     â”‚                        â”‚ Process          â”‚
        â”‚             â”‚    Database Bridge     â”‚                  â”‚
        â”‚ - ä»»åŠ¡è°ƒåº¦  â”‚ â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ â”‚ - SimpleWorkflow â”‚
        â”‚ - çŠ¶æ€æŸ¥è¯¢  â”‚                        â”‚ - è¿›åº¦æ›´æ–°       â”‚
        â”‚ - æ–‡ä»¶ç®¡ç†  â”‚                        â”‚ - åŒæ—¥å¿—è®°å½•     â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚                                        â”‚
               â–¼                                        â–¼
        ç«‹å³è¿”å›task_id                     ç‹¬ç«‹æ‰§è¡Œå·¥ä½œæµ+æ—¥å¿—åˆ°ä»»åŠ¡ç›®å½•
```

## æ•°æ®åº“è®¾è®¡è¯¦ç»†è¯´æ˜

### 1. è¡¨ç»“æ„è®¾è®¡

#### ä»»åŠ¡ä¸»è¡¨ (data_pipeline_tasks)
```sql
CREATE TABLE data_pipeline_tasks (
    -- ä¸»é”®ï¼šæ—¶é—´æˆ³æ ¼å¼çš„ä»»åŠ¡ID
    id VARCHAR(32) PRIMARY KEY,                    -- 'task_20250627_143052'
    
    -- ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
    task_type VARCHAR(50) NOT NULL DEFAULT 'data_workflow',
    status VARCHAR(20) NOT NULL DEFAULT 'pending', -- pending/in_progress/partial_completed/completed/failed
    
    -- é…ç½®å’Œç»“æœï¼ˆJSONæ ¼å¼ï¼‰
    parameters JSONB NOT NULL,                     -- ä»»åŠ¡é…ç½®å‚æ•°
    result JSONB,                                  -- æœ€ç»ˆæ‰§è¡Œç»“æœ
    
    -- é”™è¯¯å¤„ç†
    error_message TEXT,                            -- é”™è¯¯è¯¦ç»†ä¿¡æ¯
    
    -- æ­¥éª¤çŠ¶æ€è·Ÿè¸ª
    step_status JSONB DEFAULT '{                   -- å„æ­¥éª¤çŠ¶æ€
        "ddl_generation": "pending",
        "qa_generation": "pending", 
        "sql_validation": "pending",
        "training_load": "pending"
    }',
    
    -- æ—¶é—´æˆ³
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    
    -- åˆ›å»ºè€…ä¿¡æ¯
    created_by VARCHAR(50) DEFAULT 'api',          -- 'api', 'manual', 'system'
    
    -- è¾“å‡ºç›®å½•
    output_directory TEXT,                         -- ä»»åŠ¡è¾“å‡ºç›®å½•è·¯å¾„
    
    -- ç´¢å¼•å­—æ®µ
    db_name VARCHAR(100),                          -- æ•°æ®åº“åç§°ï¼ˆä¾¿äºç­›é€‰ï¼‰
    business_context TEXT                          -- ä¸šåŠ¡ä¸Šä¸‹æ–‡ï¼ˆä¾¿äºæœç´¢ï¼‰
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_tasks_status ON data_pipeline_tasks(status);
CREATE INDEX idx_tasks_created_at ON data_pipeline_tasks(created_at DESC);
CREATE INDEX idx_tasks_db_name ON data_pipeline_tasks(db_name);
CREATE INDEX idx_tasks_created_by ON data_pipeline_tasks(created_by);
```

#### ä»»åŠ¡æ‰§è¡Œè®°å½•è¡¨ (data_pipeline_task_executions)
```sql
CREATE TABLE data_pipeline_task_executions (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(32) REFERENCES data_pipeline_tasks(id) ON DELETE CASCADE,
    execution_step VARCHAR(50) NOT NULL,          -- 'ddl_generation', 'qa_generation', 'sql_validation', 'training_load', 'complete'
    status VARCHAR(20) NOT NULL,                  -- 'running', 'completed', 'failed'
    started_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT,
    execution_result JSONB,                       -- æ­¥éª¤æ‰§è¡Œç»“æœ
    execution_id VARCHAR(100) UNIQUE,             -- {task_id}_step_{step_name}_exec_{timestamp}
    force_executed BOOLEAN DEFAULT FALSE,         -- æ˜¯å¦å¼ºåˆ¶æ‰§è¡Œ
    files_cleaned BOOLEAN DEFAULT FALSE,          -- æ˜¯å¦æ¸…ç†äº†æ—§æ–‡ä»¶
    duration_seconds INTEGER                      -- æ‰§è¡Œæ—¶é•¿ï¼ˆç§’ï¼‰
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_executions_task_id ON data_pipeline_task_executions(task_id);
CREATE INDEX idx_executions_step ON data_pipeline_task_executions(execution_step);
CREATE INDEX idx_executions_status ON data_pipeline_task_executions(status);
CREATE INDEX idx_executions_started_at ON data_pipeline_task_executions(started_at DESC);
```

#### ä»»åŠ¡æ—¥å¿—è¡¨ (data_pipeline_task_logs)
```sql
CREATE TABLE data_pipeline_task_logs (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(32) REFERENCES data_pipeline_tasks(id) ON DELETE CASCADE,
    execution_id VARCHAR(100) REFERENCES data_pipeline_task_executions(execution_id),
    
    -- æ—¥å¿—å†…å®¹
    log_level VARCHAR(10) NOT NULL,               -- 'DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'
    message TEXT NOT NULL,                        -- æ—¥å¿—æ¶ˆæ¯å†…å®¹
    
    -- ä¸Šä¸‹æ–‡ä¿¡æ¯
    step_name VARCHAR(50),                        -- æ‰§è¡Œæ­¥éª¤åç§°
    module_name VARCHAR(100),                     -- æ¨¡å—åç§°
    function_name VARCHAR(100),                   -- å‡½æ•°åç§°
    
    -- æ—¶é—´æˆ³
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- é¢å¤–ä¿¡æ¯ï¼ˆJSONæ ¼å¼ï¼‰
    extra_data JSONB DEFAULT '{}'                 -- é¢å¤–çš„ç»“æ„åŒ–ä¿¡æ¯
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_logs_task_id ON data_pipeline_task_logs(task_id);
CREATE INDEX idx_logs_execution_id ON data_pipeline_task_logs(execution_id);
CREATE INDEX idx_logs_timestamp ON data_pipeline_task_logs(timestamp DESC);
CREATE INDEX idx_logs_level ON data_pipeline_task_logs(log_level);
CREATE INDEX idx_logs_step ON data_pipeline_task_logs(step_name);
```

#### ä»»åŠ¡è¾“å‡ºæ–‡ä»¶è¡¨ (data_pipeline_task_outputs)
```sql
CREATE TABLE data_pipeline_task_outputs (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(32) REFERENCES data_pipeline_tasks(id) ON DELETE CASCADE,
    execution_id VARCHAR(100) REFERENCES data_pipeline_task_executions(execution_id),
    
    -- æ–‡ä»¶ä¿¡æ¯
    file_type VARCHAR(50) NOT NULL,               -- 'ddl', 'md', 'json', 'log', 'report'
    file_name VARCHAR(255) NOT NULL,              -- æ–‡ä»¶å
    file_path TEXT NOT NULL,                      -- ç›¸å¯¹è·¯å¾„
    file_size BIGINT DEFAULT 0,                   -- æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
    
    -- æ–‡ä»¶å†…å®¹æ‘˜è¦
    content_hash VARCHAR(64),                     -- æ–‡ä»¶å†…å®¹hash
    description TEXT,                             -- æ–‡ä»¶æè¿°
    
    -- æ—¶é—´æˆ³
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    modified_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    -- çŠ¶æ€
    is_primary BOOLEAN DEFAULT FALSE,             -- æ˜¯å¦ä¸ºä¸»è¦è¾“å‡ºæ–‡ä»¶
    is_downloadable BOOLEAN DEFAULT TRUE          -- æ˜¯å¦å¯ä¸‹è½½
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX idx_outputs_task_id ON data_pipeline_task_outputs(task_id);
CREATE INDEX idx_outputs_execution_id ON data_pipeline_task_outputs(execution_id);
CREATE INDEX idx_outputs_file_type ON data_pipeline_task_outputs(file_type);
CREATE INDEX idx_outputs_primary ON data_pipeline_task_outputs(is_primary) WHERE is_primary = TRUE;
```

### 2. æ•°æ®åº“æ“ä½œç±»è®¾è®¡

```python
# data_pipeline/api/simple_db_manager.py
class SimpleTaskManager:
    """ç®€åŒ–çš„æ•°æ®ç®¡é“ä»»åŠ¡æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self):
        self.logger = get_data_pipeline_logger("SimpleTaskManager")
        self._connection = None
        self._connect_to_pgvector()
    
    def create_task(self, db_connection: str, table_list_file: str, 
                   business_context: str, **kwargs) -> str:
        """åˆ›å»ºæ–°ä»»åŠ¡è®°å½•ï¼Œè¿”å›task_id"""
        
    def update_task_status(self, task_id: str, status: str, 
                          error_message: str = None) -> bool:
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        
    def update_step_status(self, task_id: str, step_name: str, 
                          status: str) -> bool:
        """æ›´æ–°æ­¥éª¤çŠ¶æ€"""
        
    def get_task(self, task_id: str) -> dict:
        """è·å–ä»»åŠ¡è¯¦æƒ…"""
        
    def get_tasks_list(self, limit: int = 50, status: str = None) -> list:
        """è·å–ä»»åŠ¡åˆ—è¡¨"""
        
    def create_execution(self, task_id: str, step_name: str) -> str:
        """åˆ›å»ºæ‰§è¡Œè®°å½•ï¼Œè¿”å›execution_id"""
        
    def complete_execution(self, execution_id: str, status: str, 
                          error_message: str = None) -> bool:
        """å®Œæˆæ‰§è¡Œè®°å½•"""
        
    def record_log(self, task_id: str, level: str, message: str, 
                  execution_id: str = None, step_name: str = None) -> bool:
        """è®°å½•ä»»åŠ¡æ—¥å¿—"""
        
    def get_task_logs(self, task_id: str, limit: int = 100) -> list:
        """è·å–ä»»åŠ¡æ—¥å¿—"""
        
    def get_task_outputs(self, task_id: str) -> list:
        """è·å–ä»»åŠ¡è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"""
```

## APIæ¥å£è¯¦ç»†è®¾è®¡

### 1. APIè·¯ç”±è®¾è®¡

æ‰€æœ‰APIéƒ½åœ¨ `citu_app.py` ä¸­å®ç°ï¼Œè·¯ç”±å‰ç¼€ä¸º `/api/v0/data_pipeline/`

```python
# citu_app.py ä¸­æ·»åŠ çš„è·¯ç”±
@app.flask_app.route('/api/v0/data_pipeline/tasks', methods=['POST'])
def create_data_pipeline_task():
    """åˆ›å»ºæ•°æ®ç®¡é“ä»»åŠ¡"""

@app.flask_app.route('/api/v0/data_pipeline/tasks', methods=['GET'])
def get_data_pipeline_tasks():
    """è·å–ä»»åŠ¡åˆ—è¡¨"""

@app.flask_app.route('/api/v0/data_pipeline/tasks/<task_id>', methods=['GET'])
def get_data_pipeline_task(task_id):
    """è·å–å•ä¸ªä»»åŠ¡è¯¦æƒ…"""

@app.flask_app.route('/api/v0/data_pipeline/tasks/active', methods=['GET'])
def get_active_data_pipeline_task():
    """è·å–å½“å‰æ´»è·ƒä»»åŠ¡"""

@app.flask_app.route('/api/v0/data_pipeline/tasks/<task_id>/logs', methods=['GET'])
def get_data_pipeline_task_logs(task_id):
    """è·å–ä»»åŠ¡æ—¥å¿—"""

@app.flask_app.route('/api/v0/data_pipeline/tasks/<task_id>/files', methods=['GET'])
def get_data_pipeline_task_files(task_id):
    """è·å–ä»»åŠ¡è¾“å‡ºæ–‡ä»¶åˆ—è¡¨"""

@app.flask_app.route('/api/v0/data_pipeline/tasks/<task_id>/files/download/<filename>', methods=['GET'])
def download_data_pipeline_task_file(task_id, filename):
    """ä¸‹è½½ä»»åŠ¡è¾“å‡ºæ–‡ä»¶"""

@app.flask_app.route('/api/v0/data_pipeline/tasks/<task_id>', methods=['DELETE'])
def delete_data_pipeline_task(task_id):
    """åˆ é™¤ä»»åŠ¡ï¼ˆæ¸…ç†ï¼‰"""
```

### 2. APIæ¥å£å®ç°è¯¦æƒ…

#### 2.1 åˆ›å»ºä»»åŠ¡æ¥å£

```python
@app.flask_app.route('/api/v0/data_pipeline/tasks', methods=['POST'])
def create_data_pipeline_task():
    """
    åˆ›å»ºæ•°æ®ç®¡é“ä»»åŠ¡
    
    Request Body:
    {
        "task_type": "complete_workflow",
        "parameters": {
            "db_connection": "postgresql://...",
            "table_list_file": "tables.txt", 
            "business_context": "ä¸šåŠ¡æè¿°",
            "output_dir": "./data_pipeline/training_data/",
            "execution_mode": "complete",
            "single_step": null
        }
    }
    """
    try:
        # 1. å‚æ•°éªŒè¯
        req_data = request.get_json()
        if not req_data:
            return jsonify(bad_request_response("è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º")), 400
            
        task_type = req_data.get('task_type', 'complete_workflow')
        parameters = req_data.get('parameters', {})
        
        # éªŒè¯å¿…éœ€å‚æ•°
        required_params = ['db_connection', 'table_list_file', 'business_context']
        missing_params = [p for p in required_params if not parameters.get(p)]
        if missing_params:
            return jsonify(bad_request_response(
                f"ç¼ºå°‘å¿…éœ€å‚æ•°: {', '.join(missing_params)}",
                missing_params=missing_params
            )), 400
        
        # éªŒè¯æ‰§è¡Œæ¨¡å¼å‚æ•°
        execution_mode = parameters.get('execution_mode', 'complete')
        single_step = parameters.get('single_step')
        
        if execution_mode not in ['complete', 'single']:
            return jsonify(bad_request_response("execution_modeå¿…é¡»æ˜¯completeæˆ–single")), 400
            
        if execution_mode == 'single':
            if not single_step or single_step not in [1, 2, 3, 4]:
                return jsonify(bad_request_response("å•æ­¥æ¨¡å¼ä¸‹single_stepå¿…é¡»æ˜¯1ã€2ã€3ã€4ä¸­çš„ä¸€ä¸ª")), 400
        elif execution_mode == 'complete' and single_step:
            return jsonify(bad_request_response("å®Œæ•´æ¨¡å¼ä¸‹ä¸åº”æä¾›single_stepå‚æ•°")), 400
        
        # 2. å¹¶å‘æ£€æŸ¥ - ç®€åŒ–ç‰ˆæœ¬ï¼ˆä¾èµ–SimpleWorkflowManagerï¼‰
        workflow_manager = SimpleWorkflowManager()
        
        # 3. åˆ›å»ºä»»åŠ¡è®°å½•ï¼ˆè¿”å›task_idï¼‰
        task_id = workflow_manager.create_task(
            db_connection=parameters['db_connection'],
            table_list_file=parameters['table_list_file'],
            business_context=parameters['business_context'],
            **{k: v for k, v in parameters.items() 
               if k not in ['db_connection', 'table_list_file', 'business_context']}
        )
        
        # 4. å¯åŠ¨åå°è¿›ç¨‹
        import subprocess
        import sys
        from pathlib import Path
        
        # æ„å»ºä»»åŠ¡æ‰§è¡Œå™¨å‘½ä»¤
        cmd_args = [
            sys.executable, 
            str(Path(__file__).parent / "data_pipeline" / "task_executor.py"),
            '--task-id', task_id,
            '--execution-mode', 'complete'
        ]
        
        # å¦‚æœæ˜¯å•æ­¥æ‰§è¡Œï¼Œæ·»åŠ æ­¥éª¤å‚æ•°
        if execution_mode == 'step' and single_step:
            cmd_args.extend(['--step-name', f'step_{single_step}'])
        
        # å¯åŠ¨åå°è¿›ç¨‹
        try:
            process = subprocess.Popen(
                cmd_args,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                cwd=Path(__file__).parent
            )
            logger.info(f"å¯åŠ¨ä»»åŠ¡è¿›ç¨‹: PID={process.pid}, task_id={task_id}")
        except Exception as e:
            # æ¸…ç†ä»»åŠ¡è®°å½•
            workflow_manager.cleanup()
            return jsonify(internal_error_response(f"å¯åŠ¨åå°è¿›ç¨‹å¤±è´¥: {str(e)}")), 500
        
        # 5. è¿”å›æˆåŠŸå“åº”
        
        # å¯åŠ¨è¿›ç¨‹
        try:
            log_file_path = os.path.join(task_dir, 'data_pipeline.log')
            process = subprocess.Popen(
                cmd_args,
                stdout=open(log_file_path, 'w', encoding='utf-8'),
                stderr=subprocess.STDOUT,
                cwd=os.getcwd(),
                start_new_session=True
            )
            
            logger.info(f"å¯åŠ¨åå°ä»»åŠ¡: {task_id}, PID: {process.pid}")
            
        except Exception as e:
            # æ¸…ç†èµ„æº
            task_manager.update_task_status(task_id, 'failed', error_message=f"å¯åŠ¨è¿›ç¨‹å¤±è´¥: {str(e)}")
            shutil.rmtree(task_dir, ignore_errors=True)
            return jsonify(internal_error_response(f"å¯åŠ¨ä»»åŠ¡å¤±è´¥: {str(e)}")), 500
        
        # 9. è¿”å›æˆåŠŸå“åº”
        return jsonify(success_response(
            message="ä»»åŠ¡åˆ›å»ºæˆåŠŸ",
            data={
                "task_id": task_id,
                "status": "pending",
                "created_at": datetime.now().isoformat(),
                "output_directory": task_dir
            }
        )), 201
        
    except Exception as e:
        logger.exception(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response("åˆ›å»ºä»»åŠ¡å¤±è´¥")), 500
```

#### 2.2 è·å–ä»»åŠ¡è¯¦æƒ…æ¥å£

```python
@app.flask_app.route('/api/v0/data_pipeline/tasks/<task_id>', methods=['GET'])
def get_data_pipeline_task(task_id):
    """
    è·å–å•ä¸ªä»»åŠ¡è¯¦æƒ…
    
    Response:
    {
        "success": true,
        "data": {
            "task_id": "task_20250627_143052",
            "task_type": "complete_workflow",
            "status": "running",
            "progress": 45,
            "current_step": "question_sql_generation",
            "parameters": {...},
            "result": {...},
            "error_message": null,
            "step_details": [...],
            "created_at": "2025-06-27T14:30:52",
            "started_at": "2025-06-27T14:30:53",
            "completed_at": null,
            "duration": 125.5,
            "output_directory": "./data_pipeline/training_data/task_20250627_143052"
        }
    }
    """
    try:
        # å‚æ•°éªŒè¯
        if not task_id or not task_id.startswith('task_'):
            return jsonify(bad_request_response("æ— æ•ˆçš„ä»»åŠ¡IDæ ¼å¼")), 400
        
        workflow_manager = SimpleWorkflowManager()
        task_data = workflow_manager.get_task_status(task_id)
        
        if not task_data:
            return jsonify(not_found_response(f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}")), 404
        
        # è®¡ç®—æ‰§è¡Œæ—¶é•¿
        duration = None
        if task_data.get('started_at'):
            end_time = task_data.get('completed_at') or datetime.now()
            start_time = task_data['started_at']
            if isinstance(start_time, str):
                start_time = datetime.fromisoformat(start_time)
            if isinstance(end_time, str):
                end_time = datetime.fromisoformat(end_time)
            duration = (end_time - start_time).total_seconds()
        
        # è·å–æ­¥éª¤è¯¦æƒ…
        step_details = []
        step_stats = task_data.get('step_stats', {})
        
        for step_name in ['ddl_md_generation', 'question_sql_generation', 'sql_validation', 'training_data_load']:
            step_info = step_stats.get(step_name, {})
            step_details.append({
                "step": step_name,
                "status": step_info.get('status', 'pending'),
                "started_at": step_info.get('started_at'),
                "completed_at": step_info.get('completed_at'),
                "duration": step_info.get('duration'),
                "error_message": step_info.get('error_message')
            })
        
        response_data = {
            **task_data,
            "duration": duration,
            "step_details": step_details
        }
        
        return jsonify(success_response("è·å–ä»»åŠ¡è¯¦æƒ…æˆåŠŸ", data=response_data))
        
    except Exception as e:
        logger.exception(f"è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response("è·å–ä»»åŠ¡è¯¦æƒ…å¤±è´¥")), 500
```

## Schema Workflow é›†æˆè®¾è®¡

### 1. å‘½ä»¤è¡Œå‚æ•°æ‰©å±•

åœ¨ç°æœ‰çš„ `setup_argument_parser()` å‡½æ•°ä¸­æ·»åŠ æ–°å‚æ•°ï¼š

```python
def setup_argument_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description="Schemaå·¥ä½œæµç¼–æ’å™¨ - ç«¯åˆ°ç«¯çš„Schemaå¤„ç†æµç¨‹",
        formatter_class=argparse.RawDescriptionHelpFormatter
    )
    
    # ... ç°æœ‰å‚æ•° ...
    
    # æ–°å¢APIé›†æˆå‚æ•°
    parser.add_argument(
        "--task-id",
        required=False,
        help="ä»»åŠ¡IDï¼ˆAPIè°ƒç”¨æ—¶æä¾›ï¼Œæ‰‹åŠ¨æ‰§è¡Œæ—¶è‡ªåŠ¨ç”Ÿæˆï¼‰"
    )
    
    parser.add_argument(
        "--no-db-tracking",
        action="store_true",
        help="ç¦ç”¨æ•°æ®åº“ä»»åŠ¡è¿½è¸ªï¼ˆä¸è®°å½•åˆ°ä»»åŠ¡è¡¨ï¼‰"
    )
    
    # æ–°å¢æ‰§è¡Œæ¨¡å¼å‚æ•°
    parser.add_argument(
        "--execution-mode",
        choices=['complete', 'single'],
        default='complete',
        help="æ‰§è¡Œæ¨¡å¼ï¼šcomplete=å®Œæ•´å·¥ä½œæµï¼Œsingle=å•æ­¥æ‰§è¡Œ"
    )
    
    parser.add_argument(
        "--single-step",
        type=int,
        choices=[1, 2, 3, 4],
        help="å•æ­¥æ‰§è¡Œæ—¶æŒ‡å®šæ­¥éª¤å·ï¼ˆ1=DDLç”Ÿæˆï¼Œ2=Q&Aç”Ÿæˆï¼Œ3=SQLéªŒè¯ï¼Œ4=è®­ç»ƒæ•°æ®åŠ è½½ï¼‰"
    )
    
    return parser
```

### 2. SchemaWorkflowOrchestrator ç±»ä¿®æ”¹

```python
class SchemaWorkflowOrchestrator:
    """ç«¯åˆ°ç«¯çš„Schemaå¤„ç†ç¼–æ’å™¨ - å®Œæ•´å·¥ä½œæµç¨‹"""
    
    def __init__(self, 
                 db_connection: str,
                 table_list_file: str, 
                 business_context: str,
                 output_dir: str = None,
                 enable_sql_validation: bool = True,
                 enable_llm_repair: bool = True,
                 modify_original_file: bool = True,
                 enable_training_data_load: bool = True,
                 # æ–°å¢å‚æ•°
                 task_id: str = None,
                 db_logger: 'DatabaseProgressLogger' = None,
                 execution_mode: str = 'complete',
                 single_step: int = None):
        """
        åˆå§‹åŒ–Schemaå·¥ä½œæµç¼–æ’å™¨
        
        Args:
            # ... ç°æœ‰å‚æ•° ...
            task_id: ä»»åŠ¡IDï¼ˆå¯é€‰ï¼‰
            db_logger: æ•°æ®åº“è¿›åº¦è®°å½•å™¨ï¼ˆå¯é€‰ï¼‰
            execution_mode: æ‰§è¡Œæ¨¡å¼ ('complete' æˆ– 'single')
            single_step: å•æ­¥æ‰§è¡Œæ—¶çš„æ­¥éª¤å· (1-4)
        """
        # ... ç°æœ‰åˆå§‹åŒ–ä»£ç  ...
        
        # æ–°å¢å±æ€§
        self.task_id = task_id
        self.db_logger = db_logger
        self.execution_mode = execution_mode
        self.single_step = single_step
        
        # å¦‚æœæä¾›äº†task_idä½†æ²¡æœ‰db_loggerï¼Œå°è¯•åˆ›å»ºä¸€ä¸ª
        if self.task_id and not self.db_logger:
            try:
                self.db_logger = self._create_db_logger()
            except Exception as e:
                self.logger.warning(f"æ— æ³•åˆ›å»ºæ•°æ®åº“è®°å½•å™¨: {e}")
    
    def _create_db_logger(self):
        """åˆ›å»ºæ•°æ®åº“è¿›åº¦è®°å½•å™¨"""
        from data_pipeline.api.database_logger import DatabaseProgressLogger
        return DatabaseProgressLogger(self.task_id, self.db_connection)
    
    def _should_execute_step(self, step_number: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦åº”è¯¥æ‰§è¡ŒæŒ‡å®šæ­¥éª¤"""
        if self.execution_mode == 'complete':
            # å®Œæ•´æ¨¡å¼ï¼šæ‰§è¡Œæ‰€æœ‰æ­¥éª¤
            return True
        elif self.execution_mode == 'single':
            # å•æ­¥æ¨¡å¼ï¼šåªæ‰§è¡ŒæŒ‡å®šçš„æ­¥éª¤
            return step_number == self.single_step
        else:
            return False
    
    async def execute_complete_workflow(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´çš„Schemaå¤„ç†å·¥ä½œæµç¨‹"""
        self.workflow_state["start_time"] = time.time()
        
        # æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸ºrunning
        if self.db_logger:
            self.db_logger.update_task_status('running')
            self.db_logger.add_log('INFO', f'å¼€å§‹æ‰§è¡ŒSchemaå·¥ä½œæµç¼–æ’', 'workflow_start')
        
        self.logger.info("ğŸš€ å¼€å§‹æ‰§è¡ŒSchemaå·¥ä½œæµç¼–æ’")
        # ... ç°æœ‰æ—¥å¿— ...
        
        try:
            # æ­¥éª¤1: ç”ŸæˆDDLå’ŒMDæ–‡ä»¶
            if self._should_execute_step(1):
                await self._execute_step_1_ddl_md_generation()
            
            # æ­¥éª¤2: ç”ŸæˆQuestion-SQLå¯¹
            if self._should_execute_step(2):
                await self._execute_step_2_question_sql_generation()
            
            # æ­¥éª¤3: éªŒè¯å’Œä¿®æ­£SQL
            if self._should_execute_step(3):
                await self._execute_step_3_sql_validation()
            
            # æ­¥éª¤4: è®­ç»ƒæ•°æ®åŠ è½½
            if self._should_execute_step(4):
                await self._execute_step_4_training_data_load()
            
            # è®¾ç½®ç»“æŸæ—¶é—´
            self.workflow_state["end_time"] = time.time()
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = await self._generate_final_report()
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸ºcompleted
            if self.db_logger:
                self.db_logger.update_task_status('completed', result=final_report)
                self.db_logger.add_log('INFO', 'å·¥ä½œæµæ‰§è¡Œå®Œæˆ', 'workflow_complete')
            
            self.logger.info("âœ… Schemaå·¥ä½œæµç¼–æ’å®Œæˆ")
            return final_report
            
        except Exception as e:
            self.workflow_state["end_time"] = time.time()
            
            # æ›´æ–°æ•°æ®åº“çŠ¶æ€ä¸ºfailed
            if self.db_logger:
                self.db_logger.update_task_status('failed', error_message=str(e))
                self.db_logger.add_log('ERROR', f'å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {str(e)}', 'workflow_error')
            
            self.logger.exception(f"âŒ å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
            error_report = await self._generate_error_report(e)
            return error_report
    
    async def _execute_step_1_ddl_md_generation(self):
        """æ­¥éª¤1: ç”ŸæˆDDLå’ŒMDæ–‡ä»¶"""
        self.workflow_state["current_step"] = "ddl_md_generation"
        
        # æ›´æ–°æ•°æ®åº“è¿›åº¦
        if self.db_logger:
            self.db_logger.update_progress(10, 'ddl_md_generation')
            self.db_logger.add_log('INFO', 'DDL/MDç”Ÿæˆå¼€å§‹', 'ddl_md_generation')
        
        # ... ç°æœ‰æ‰§è¡Œä»£ç  ...
        
        try:
            # ... DDL/MDç”Ÿæˆé€»è¾‘ ...
            
            # æ›´æ–°è¿›åº¦
            if self.db_logger:
                self.db_logger.update_progress(40, 'ddl_md_generation')
                self.db_logger.add_log('INFO', f'DDL/MDç”Ÿæˆå®Œæˆ: æˆåŠŸå¤„ç† {processed_tables} ä¸ªè¡¨', 'ddl_md_generation')
            
        except Exception as e:
            if self.db_logger:
                self.db_logger.add_log('ERROR', f'DDL/MDç”Ÿæˆå¤±è´¥: {str(e)}', 'ddl_md_generation')
            raise
    
    # ç±»ä¼¼åœ°ä¿®æ”¹å…¶ä»–æ­¥éª¤æ–¹æ³•...
```

### 3. æ•°æ®åº“è¿›åº¦è®°å½•å™¨

```python
# data_pipeline/api/database_logger.py
class DatabaseProgressLogger:
    """æ•°æ®åº“è¿›åº¦è®°å½•å™¨"""
    
    def __init__(self, task_id: str, db_connection_string: str):
        self.task_id = task_id
        self.task_manager = DataPipelineTaskManager(db_connection_string)
        self.logger = get_data_pipeline_logger("DatabaseLogger")
    
    def update_task_status(self, status: str, current_step: str = None, 
                          error_message: str = None, result: dict = None):
        """æ›´æ–°ä»»åŠ¡çŠ¶æ€"""
        try:
            success = self.task_manager.update_task_status(
                self.task_id, status, current_step, error_message
            )
            if result and status == 'completed':
                self.task_manager.update_task_result(self.task_id, result)
            return success
        except Exception as e:
            self.logger.warning(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
            return False
    
    def update_progress(self, progress: int, current_step: str = None):
        """æ›´æ–°ä»»åŠ¡è¿›åº¦"""
        try:
            return self.task_manager.update_task_progress(
                self.task_id, progress, current_step
            )
        except Exception as e:
            self.logger.warning(f"æ›´æ–°ä»»åŠ¡è¿›åº¦å¤±è´¥: {e}")
            return False
    
    def add_log(self, level: str, message: str, step_name: str = None, 
               extra_data: dict = None):
        """æ·»åŠ ä»»åŠ¡æ—¥å¿—"""
        try:
            return self.task_manager.add_task_log(
                self.task_id, level, message, step_name, extra_data
            )
        except Exception as e:
            self.logger.warning(f"æ·»åŠ ä»»åŠ¡æ—¥å¿—å¤±è´¥: {e}")
            return False
```

## æ—¥å¿—ç³»ç»Ÿé›†æˆè®¾è®¡

### 1. æ—¥å¿—è·¯å¾„ç®¡ç†

ä¿®æ”¹ `core/logging/log_manager.py` ä»¥æ”¯æŒä»»åŠ¡ç‰¹å®šçš„æ—¥å¿—ç›®å½•ï¼š

```python
def _create_file_handler(self, file_config: dict, module: str) -> logging.Handler:
    """åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨"""
    
    # å¯¹äºdata_pipelineæ¨¡å—ï¼Œæ£€æŸ¥æ˜¯å¦æœ‰ä»»åŠ¡ç‰¹å®šçš„æ—¥å¿—ç›®å½•
    if module == 'data_pipeline' and 'DATA_PIPELINE_LOG_DIR' in os.environ:
        log_file = Path(os.environ['DATA_PIPELINE_LOG_DIR']) / 'data_pipeline.log'
        # ç¦ç”¨è½®è½¬ï¼Œå› ä¸ºæ¯ä¸ªä»»åŠ¡çš„æ—¥å¿—æ˜¯ç‹¬ç«‹çš„
        file_config = file_config.copy()
        file_config['enable_rotation'] = False
    else:
        log_file = self.base_log_dir / file_config.get('filename', f'{module}.log')
    
    # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
    log_file.parent.mkdir(parents=True, exist_ok=True)
    
    # ... å…¶ä½™ä»£ç ä¿æŒä¸å˜ ...
```

### 2. ä»»åŠ¡æ—¥å¿—åˆå§‹åŒ–

åœ¨ `schema_workflow.py` çš„ `main()` å‡½æ•°ä¸­ï¼š

```python
async def main():
    """å‘½ä»¤è¡Œå…¥å£ç‚¹"""
    parser = setup_argument_parser()
    args = parser.parse_args()
    
    # åˆå§‹åŒ–å˜é‡
    task_id = None
    db_logger = None
    
    # å¦‚æœä¸ç¦ç”¨æ•°æ®åº“è¿½è¸ª
    if not args.no_db_tracking:
        # å¦‚æœæ²¡æœ‰task_idï¼Œè‡ªåŠ¨ç”Ÿæˆ
        if not args.task_id:
            from datetime import datetime
            args.task_id = f"manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            logger.info(f"ğŸ“ è‡ªåŠ¨ç”Ÿæˆä»»åŠ¡ID: {args.task_id}")
        
        task_id = args.task_id
        
        # ç¡®å®šä»»åŠ¡ç›®å½•
        if task_id.startswith('task_'):
            # APIè°ƒç”¨çš„ä»»åŠ¡ï¼Œè¾“å‡ºç›®å½•å·²ç»æ˜¯ä»»åŠ¡ç‰¹å®šçš„
            task_dir = args.output_dir
        else:
            # æ‰‹åŠ¨æ‰§è¡Œçš„ä»»åŠ¡ï¼Œåˆ›å»ºä»»åŠ¡ç‰¹å®šç›®å½•
            task_dir = os.path.join(args.output_dir, task_id)
            os.makedirs(task_dir, exist_ok=True)
            args.output_dir = task_dir
        
        # è®¾ç½®ç¯å¢ƒå˜é‡ï¼Œè®©æ—¥å¿—ç³»ç»ŸçŸ¥é“å½“å‰çš„ä»»åŠ¡ç›®å½•
        os.environ['DATA_PIPELINE_LOG_DIR'] = task_dir
        
        # é‡æ–°åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ
        from core.logging import initialize_logging
        initialize_logging()
        
        try:
            # åˆ›å»ºä»»åŠ¡è®°å½•ï¼ˆå¦‚æœæ˜¯æ‰‹åŠ¨æ‰§è¡Œï¼‰
            if task_id.startswith('manual_'):
                task_manager = DataPipelineTaskManager(args.db_connection)
                task_manager.create_task(
                    task_id=task_id,
                    task_type='complete_workflow',
                    parameters={
                        'db_connection': args.db_connection,
                        'table_list': args.table_list,
                        'business_context': args.business_context,
                        'output_dir': args.output_dir,
                        # ... å…¶ä»–å‚æ•°
                    },
                    created_by='manual'
                )
            
            # åˆå§‹åŒ–æ•°æ®åº“è®°å½•å™¨
            db_logger = DatabaseProgressLogger(task_id, args.db_connection)
            logger.info(f"âœ… å·²å¯ç”¨æ•°æ®åº“ä»»åŠ¡è¿½è¸ª: {task_id}")
            
        except Exception as e:
            logger.warning(f"âš ï¸ æ— æ³•åˆå§‹åŒ–ä»»åŠ¡è¿½è¸ª: {e}")
            db_logger = None
    else:
        logger.info("â„¹ï¸ å·²ç¦ç”¨æ•°æ®åº“ä»»åŠ¡è¿½è¸ª")
    
    # å‚æ•°éªŒè¯ï¼šå•æ­¥æ¨¡å¼å¿…é¡»æä¾›æ­¥éª¤å·
    if args.execution_mode == 'single' and not args.single_step:
        logger.error("å•æ­¥æ¨¡å¼ä¸‹å¿…é¡»æä¾› --single-step å‚æ•°")
        sys.exit(1)
    
    # åˆ›å»ºç¼–æ’å™¨ï¼Œä¼ å…¥æ–°å‚æ•°
    orchestrator = SchemaWorkflowOrchestrator(
        db_connection=args.db_connection,
        table_list_file=args.table_list,
        business_context=args.business_context,
        output_dir=args.output_dir,
        enable_sql_validation=not args.skip_validation,
        enable_llm_repair=not args.disable_llm_repair,
        modify_original_file=not args.no_modify_file,
        enable_training_data_load=not args.skip_training_load,
        task_id=task_id,
        db_logger=db_logger,
        execution_mode=args.execution_mode,
        single_step=args.single_step
    )
    
    # æ‰§è¡Œå·¥ä½œæµ
    report = await orchestrator.execute_complete_workflow()
    
    # ... å…¶ä½™ä»£ç ä¿æŒä¸å˜ ...
```

## é”™è¯¯å¤„ç†å’Œç›‘æ§

### 1. åƒµå°¸ä»»åŠ¡æ£€æµ‹

```python
# data_pipeline/api/task_monitor.py
class TaskMonitor:
    """ä»»åŠ¡ç›‘æ§å™¨"""
    
    def __init__(self, db_connection_string: str):
        self.task_manager = DataPipelineTaskManager(db_connection_string)
        self.logger = get_data_pipeline_logger("TaskMonitor")
    
    def check_zombie_tasks(self, timeout_hours: int = 2):
        """æ£€æŸ¥åƒµå°¸ä»»åŠ¡"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=timeout_hours)
            
            # æŸ¥æ‰¾è¶…æ—¶çš„è¿è¡Œä¸­ä»»åŠ¡
            zombie_tasks = self.task_manager.get_zombie_tasks(cutoff_time)
            
            for task in zombie_tasks:
                task_id = task['id']
                self.logger.warning(f"å‘ç°åƒµå°¸ä»»åŠ¡: {task_id}")
                
                # æ ‡è®°ä¸ºå¤±è´¥
                self.task_manager.update_task_status(
                    task_id, 
                    'failed', 
                    error_message=f"ä»»åŠ¡è¶…æ—¶ï¼ˆè¶…è¿‡{timeout_hours}å°æ—¶ï¼‰ï¼Œå¯èƒ½å·²åœæ­¢è¿è¡Œ"
                )
                
                # è®°å½•æ—¥å¿—
                self.task_manager.add_task_log(
                    task_id, 
                    'ERROR', 
                    f"ä»»åŠ¡è¢«æ ‡è®°ä¸ºåƒµå°¸ä»»åŠ¡ï¼Œæ‰§è¡Œæ—¶é—´è¶…è¿‡{timeout_hours}å°æ—¶", 
                    'system_check'
                )
        
        except Exception as e:
            self.logger.error(f"æ£€æŸ¥åƒµå°¸ä»»åŠ¡å¤±è´¥: {e}")

# åœ¨citu_app.pyä¸­æ·»åŠ å®šæœŸæ£€æŸ¥
import threading
import time

def start_task_monitor():
    """å¯åŠ¨ä»»åŠ¡ç›‘æ§å™¨"""
    def monitor_loop():
        monitor = TaskMonitor(app_config.PGVECTOR_CONFIG)
        while True:
            try:
                monitor.check_zombie_tasks()
                time.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            except Exception as e:
                logger.error(f"ä»»åŠ¡ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(60)  # å‡ºé”™æ—¶ç­‰å¾…1åˆ†é’Ÿå†é‡è¯•
    
    monitor_thread = threading.Thread(target=monitor_loop, daemon=True)
    monitor_thread.start()
    logger.info("ä»»åŠ¡ç›‘æ§å™¨å·²å¯åŠ¨")

# åœ¨åº”ç”¨å¯åŠ¨æ—¶è°ƒç”¨
if __name__ == '__main__':
    start_task_monitor()
    app.run()
```

### 2. æ–‡ä»¶è¾“å‡ºç®¡ç†

```python
# data_pipeline/api/file_manager.py
class TaskFileManager:
    """ä»»åŠ¡æ–‡ä»¶ç®¡ç†å™¨"""
    
    def __init__(self, task_id: str, output_dir: str, db_connection_string: str):
        self.task_id = task_id
        self.output_dir = Path(output_dir)
        self.task_manager = DataPipelineTaskManager(db_connection_string)
        self.logger = get_data_pipeline_logger("FileManager")
    
    def scan_and_register_files(self):
        """æ‰«æå¹¶æ³¨å†Œè¾“å‡ºæ–‡ä»¶"""
        try:
            if not self.output_dir.exists():
                return
            
            # æ–‡ä»¶ç±»å‹æ˜ å°„
            file_type_mapping = {
                '.ddl': 'ddl',
                '.md': 'md', 
                '.json': 'json',
                '.log': 'log',
                '.txt': 'txt'
            }
            
            for file_path in self.output_dir.iterdir():
                if file_path.is_file():
                    file_ext = file_path.suffix.lower()
                    file_type = file_type_mapping.get(file_ext, 'other')
                    file_size = file_path.stat().st_size
                    
                    # åˆ¤æ–­æ˜¯å¦ä¸ºä¸»è¦è¾“å‡ºæ–‡ä»¶
                    is_primary = (
                        file_path.name.endswith('_pair.json') or
                        file_path.name == 'metadata.txt' or
                        file_path.name.endswith('_summary.log')
                    )
                    
                    # æ³¨å†Œæ–‡ä»¶
                    self.task_manager.register_output_file(
                        task_id=self.task_id,
                        file_type=file_type,
                        file_name=file_path.name,
                        file_path=str(file_path.relative_to(self.output_dir)),
                        file_size=file_size,
                        is_primary=is_primary
                    )
        
        except Exception as e:
            self.logger.error(f"æ‰«ææ–‡ä»¶å¤±è´¥: {e}")
    
    def cleanup_task_files(self):
        """æ¸…ç†ä»»åŠ¡æ–‡ä»¶"""
        try:
            if self.output_dir.exists():
                shutil.rmtree(self.output_dir)
                self.logger.info(f"å·²æ¸…ç†ä»»åŠ¡æ–‡ä»¶: {self.output_dir}")
        except Exception as e:
            self.logger.error(f"æ¸…ç†ä»»åŠ¡æ–‡ä»¶å¤±è´¥: {e}")
```

## éƒ¨ç½²å’Œåˆå§‹åŒ–

### 1. æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬

```sql
-- data_pipeline/sql/init_tables.sql

-- åˆ›å»ºä»»åŠ¡è¡¨
CREATE TABLE IF NOT EXISTS data_pipeline_tasks (
    id VARCHAR(32) PRIMARY KEY,
    task_type VARCHAR(50) NOT NULL DEFAULT 'complete_workflow',
    status VARCHAR(20) NOT NULL DEFAULT 'pending',
    parameters JSONB NOT NULL,
    result JSONB,
    error_message TEXT,
    error_step VARCHAR(100),
    progress INTEGER DEFAULT 0 CHECK (progress >= 0 AND progress <= 100),
    current_step VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    created_by VARCHAR(50) DEFAULT 'api',
    step_stats JSONB DEFAULT '{}',
    output_directory TEXT,
    db_name VARCHAR(100),
    business_context TEXT
);

-- åˆ›å»ºæ—¥å¿—è¡¨
CREATE TABLE IF NOT EXISTS data_pipeline_task_logs (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(32) REFERENCES data_pipeline_tasks(id) ON DELETE CASCADE,
    log_level VARCHAR(10) NOT NULL,
    message TEXT NOT NULL,
    step_name VARCHAR(100),
    module_name VARCHAR(100),
    function_name VARCHAR(100),
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    extra_data JSONB DEFAULT '{}'
);

-- åˆ›å»ºè¾“å‡ºæ–‡ä»¶è¡¨
CREATE TABLE IF NOT EXISTS data_pipeline_task_outputs (
    id SERIAL PRIMARY KEY,
    task_id VARCHAR(32) REFERENCES data_pipeline_tasks(id) ON DELETE CASCADE,
    file_type VARCHAR(50) NOT NULL,
    file_name VARCHAR(255) NOT NULL,
    file_path TEXT NOT NULL,
    file_size BIGINT DEFAULT 0,
    content_hash VARCHAR(64),
    description TEXT,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    modified_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_primary BOOLEAN DEFAULT FALSE,
    is_downloadable BOOLEAN DEFAULT TRUE
);

-- åˆ›å»ºç´¢å¼•
CREATE INDEX IF NOT EXISTS idx_tasks_status ON data_pipeline_tasks(status);
CREATE INDEX IF NOT EXISTS idx_tasks_created_at ON data_pipeline_tasks(created_at DESC);
CREATE INDEX IF NOT EXISTS idx_tasks_db_name ON data_pipeline_tasks(db_name);
CREATE INDEX IF NOT EXISTS idx_tasks_created_by ON data_pipeline_tasks(created_by);

CREATE INDEX IF NOT EXISTS idx_logs_task_id ON data_pipeline_task_logs(task_id);
CREATE INDEX IF NOT EXISTS idx_logs_timestamp ON data_pipeline_task_logs(timestamp DESC);
CREATE INDEX IF NOT EXISTS idx_logs_level ON data_pipeline_task_logs(log_level);
CREATE INDEX IF NOT EXISTS idx_logs_step ON data_pipeline_task_logs(step_name);

CREATE INDEX IF NOT EXISTS idx_outputs_task_id ON data_pipeline_task_outputs(task_id);
CREATE INDEX IF NOT EXISTS idx_outputs_file_type ON data_pipeline_task_outputs(file_type);
CREATE INDEX IF NOT EXISTS idx_outputs_primary ON data_pipeline_task_outputs(is_primary) WHERE is_primary = TRUE;

-- åˆ›å»ºæ¸…ç†å‡½æ•°
CREATE OR REPLACE FUNCTION cleanup_old_data_pipeline_tasks(days_to_keep INTEGER DEFAULT 30)
RETURNS INTEGER AS $$
DECLARE
    deleted_count INTEGER;
    cutoff_date TIMESTAMP;
BEGIN
    cutoff_date := NOW() - INTERVAL '1 day' * days_to_keep;
    
    -- åˆ é™¤æ—§ä»»åŠ¡ï¼ˆçº§è”åˆ é™¤ç›¸å…³æ—¥å¿—å’Œæ–‡ä»¶è®°å½•ï¼‰
    DELETE FROM data_pipeline_tasks 
    WHERE created_at < cutoff_date 
    AND status IN ('completed', 'failed');
    
    GET DIAGNOSTICS deleted_count = ROW_COUNT;
    
    RETURN deleted_count;
END;
$$ LANGUAGE plpgsql;
```

### 2. é…ç½®æ–‡ä»¶æ›´æ–°

éœ€è¦åœ¨ `app_config.py` ä¸­æ·»åŠ Data Pipelineç›¸å…³é…ç½®ï¼š

```python
# Data Pipeline APIé…ç½®
DATA_PIPELINE_CONFIG = {
    "max_concurrent_tasks": 1,           # æœ€å¤§å¹¶å‘ä»»åŠ¡æ•°
    "task_timeout_hours": 2,             # ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆå°æ—¶ï¼‰
    "log_retention_days": 30,            # æ—¥å¿—ä¿ç•™å¤©æ•°
    "file_retention_days": 30,           # æ–‡ä»¶ä¿ç•™å¤©æ•°
    "monitor_interval_seconds": 300,     # ç›‘æ§æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
    "enable_file_download": True,        # æ˜¯å¦å…è®¸æ–‡ä»¶ä¸‹è½½
    "max_download_file_size": 100 * 1024 * 1024,  # æœ€å¤§ä¸‹è½½æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
}
```

## æ€»ç»“

æœ¬è¯¦ç»†è®¾è®¡æ–‡æ¡£æä¾›äº†Data Pipeline APIç³»ç»Ÿçš„å®Œæ•´æŠ€æœ¯å®ç°æ–¹æ¡ˆï¼š

### ä¸»è¦ç‰¹ç‚¹

1. **APIä¸æ‰§è¡Œåˆ†ç¦»**ï¼šä½¿ç”¨subprocesså®ç°çœŸæ­£çš„åå°æ‰§è¡Œï¼ŒAPIä¸é˜»å¡
2. **æ•°æ®åº“é©±åŠ¨çš„çŠ¶æ€ç®¡ç†**ï¼šæ‰€æœ‰ä»»åŠ¡çŠ¶æ€ã€è¿›åº¦ã€æ—¥å¿—éƒ½è®°å½•åœ¨PostgreSQLä¸­
3. **çµæ´»çš„æ­¥éª¤æ§åˆ¶**ï¼šæ”¯æŒä»æŒ‡å®šæ­¥éª¤å¼€å§‹ã€ç»“æŸï¼Œä»¥åŠè·³è¿‡ç‰¹å®šæ­¥éª¤
4. **ç»Ÿä¸€çš„æ—¥å¿—ç®¡ç†**ï¼šæ¯ä¸ªä»»åŠ¡çš„æ—¥å¿—éƒ½å†™å…¥ç‹¬ç«‹çš„ä»»åŠ¡ç›®å½•
5. **å®Œæ•´çš„æ–‡ä»¶ç®¡ç†**ï¼šè‡ªåŠ¨æ‰«æã€æ³¨å†Œå’Œç®¡ç†ä»»åŠ¡è¾“å‡ºæ–‡ä»¶
6. **å¥å£®çš„é”™è¯¯å¤„ç†**ï¼šåŒ…æ‹¬åƒµå°¸ä»»åŠ¡æ£€æµ‹ã€è¶…æ—¶å¤„ç†ç­‰

### å®ç°è¦ç‚¹

1. **æœ€å°åŒ–ä»£ç ä¿®æ”¹**ï¼šä¸»è¦ä¿®æ”¹é›†ä¸­åœ¨ `schema_workflow.py` å’Œ `citu_app.py`
2. **å‘åå…¼å®¹**ï¼šæ‰‹åŠ¨æ‰§è¡Œæ–¹å¼ä»ç„¶å®Œå…¨æ”¯æŒ
3. **æ‰©å±•æ€§å¥½**ï¼šæ˜“äºæ·»åŠ æ–°çš„ä»»åŠ¡ç±»å‹å’Œæ‰§è¡Œæ­¥éª¤
4. **ç›‘æ§å‹å¥½**ï¼šæä¾›å®Œæ•´çš„ä»»åŠ¡ç›‘æ§å’Œæ¸…ç†æœºåˆ¶

### å…³é”®æ–‡ä»¶

1. `citu_app.py` - æ·»åŠ APIè·¯ç”±å®ç°
2. `data_pipeline/schema_workflow.py` - ä¿®æ”¹ä»¥æ”¯æŒAPIé›†æˆ
3. `data_pipeline/api/database_manager.py` - æ•°æ®åº“æ“ä½œå°è£…ï¼ˆæ–°å»ºï¼‰
4. `data_pipeline/api/database_logger.py` - è¿›åº¦è®°å½•å™¨ï¼ˆæ–°å»ºï¼‰
5. `data_pipeline/sql/init_tables.sql` - æ•°æ®åº“åˆå§‹åŒ–è„šæœ¬ï¼ˆæ–°å»ºï¼‰

è¿™ä¸ªè®¾è®¡å……åˆ†è€ƒè™‘äº†ç°æœ‰ä»£ç ç»“æ„ï¼Œæä¾›äº†å®Œæ•´çš„APIåŠŸèƒ½ï¼ŒåŒæ—¶ä¿æŒäº†ç³»ç»Ÿçš„ç®€æ´æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚