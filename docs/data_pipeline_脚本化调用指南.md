# Data Pipeline è„šæœ¬åŒ–è°ƒç”¨æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»data_pipelineæ¨¡å—çš„è„šæœ¬åŒ–è°ƒç”¨æ–¹å¼ã€æ”¯æŒçš„å‚æ•°åˆ—è¡¨ä»¥åŠè„šæœ¬æ‰§è¡Œæ—¶çš„æ—¥å¿—é…ç½®æ–¹å¼ã€‚data_pipelineæ˜¯ä¸€ä¸ªå®Œæ•´çš„æ•°æ®åº“é€†å‘å·¥ç¨‹å’Œè®­ç»ƒæ•°æ®ç”Ÿæˆç³»ç»Ÿï¼Œæ”¯æŒä»PostgreSQLæ•°æ®åº“ç”Ÿæˆvanna.aiæ ¼å¼çš„è®­ç»ƒæ•°æ®ã€‚

## ç›®å½•

1. [æ¨¡å—æ¶æ„æ¦‚è§ˆ](#1-æ¨¡å—æ¶æ„æ¦‚è§ˆ)
2. [æ ¸å¿ƒè„šæœ¬å…¥å£](#2-æ ¸å¿ƒè„šæœ¬å…¥å£)
3. [ä¸€é”®å·¥ä½œæµè„šæœ¬](#3-ä¸€é”®å·¥ä½œæµè„šæœ¬)
4. [åˆ†æ­¥æ‰§è¡Œè„šæœ¬](#4-åˆ†æ­¥æ‰§è¡Œè„šæœ¬)
5. [æ—¥å¿—é…ç½®](#5-æ—¥å¿—é…ç½®)
6. [é…ç½®æ–‡ä»¶](#6-é…ç½®æ–‡ä»¶)
7. [ä½¿ç”¨ç¤ºä¾‹](#7-ä½¿ç”¨ç¤ºä¾‹)
8. [æ•…éšœæ’æŸ¥](#8-æ•…éšœæ’æŸ¥)

## 1. æ¨¡å—æ¶æ„æ¦‚è§ˆ

```
data_pipeline/
â”œâ”€â”€ schema_workflow.py          # ğŸš€ ä¸€é”®å·¥ä½œæµç¼–æ’å™¨ï¼ˆä¸»è¦å…¥å£ï¼‰
â”œâ”€â”€ ddl_generation/
â”‚   â””â”€â”€ ddl_md_generator.py     # DDL/MDæ–‡æ¡£ç”Ÿæˆå™¨
â”œâ”€â”€ qa_generation/
â”‚   â””â”€â”€ qs_generator.py         # Question-SQLå¯¹ç”Ÿæˆå™¨
â”œâ”€â”€ validators/
â”‚   â””â”€â”€ sql_validate_cli.py     # SQLéªŒè¯å‘½ä»¤è¡Œå·¥å…·
â”œâ”€â”€ trainer/
â”‚   â””â”€â”€ run_training.py         # è®­ç»ƒæ•°æ®åŠ è½½è„šæœ¬
â”œâ”€â”€ task_executor.py            # ç‹¬ç«‹ä»»åŠ¡æ‰§è¡Œå™¨ï¼ˆAPIä¸“ç”¨ï¼‰
â”œâ”€â”€ config.py                   # æ¨¡å—é…ç½®æ–‡ä»¶
â””â”€â”€ dp_logging/                 # æ—¥å¿—ç®¡ç†æ¨¡å—
```

## 2. æ ¸å¿ƒè„šæœ¬å…¥å£

### 2.1 ä¸»è¦è„šæœ¬ç±»å‹

| è„šæœ¬ç±»å‹ | å…¥å£ç‚¹ | ç”¨é€” | æ¨èä½¿ç”¨åœºæ™¯ |
|---------|--------|------|-------------|
| **ä¸€é”®å·¥ä½œæµ** | `schema_workflow.py` | å®Œæ•´çš„4æ­¥å·¥ä½œæµ | âœ… æ¨èï¼šç”Ÿäº§ç¯å¢ƒ |
| **DDLç”Ÿæˆ** | `ddl_generation/ddl_md_generator.py` | ä»…ç”ŸæˆDDLå’ŒMDæ–‡æ¡£ | è°ƒè¯•ã€åˆ†æ­¥æ‰§è¡Œ |
| **QAç”Ÿæˆ** | `qa_generation/qs_generator.py` | ä»…ç”ŸæˆQuestion-SQLå¯¹ | è°ƒè¯•ã€åˆ†æ­¥æ‰§è¡Œ |
| **SQLéªŒè¯** | `validators/sql_validate_cli.py` | ä»…éªŒè¯SQLè¯­å¥ | è°ƒè¯•ã€è´¨é‡æ£€æŸ¥ |
| **è®­ç»ƒåŠ è½½** | `trainer/run_training.py` | ä»…åŠ è½½è®­ç»ƒæ•°æ® | è°ƒè¯•ã€åˆ†æ­¥æ‰§è¡Œ |

### 2.2 æ‰§è¡Œæ–¹å¼

```bash
# ä½¿ç”¨python -mæ–¹å¼ï¼ˆæ¨èï¼‰
python -m data_pipeline.schema_workflow [å‚æ•°]
python -m data_pipeline.ddl_generation.ddl_md_generator [å‚æ•°]
python -m data_pipeline.qa_generation.qs_generator [å‚æ•°]
python -m data_pipeline.validators.sql_validate_cli [å‚æ•°]
python -m data_pipeline.trainer.run_training [å‚æ•°]

# ç›´æ¥æ‰§è¡Œæ–¹å¼
python data_pipeline/schema_workflow.py [å‚æ•°]
```

## 3. ä¸€é”®å·¥ä½œæµè„šæœ¬

### 3.1 è„šæœ¬æ¦‚è¿°

**å…¥å£ç‚¹**: `data_pipeline/schema_workflow.py`  
**ä¸»è¦ç±»**: `SchemaWorkflowOrchestrator`  
**åŠŸèƒ½**: ç«¯åˆ°ç«¯æ‰§è¡Œå®Œæ•´çš„4æ­¥å·¥ä½œæµç¨‹

### 3.2 æ‰§è¡Œæ­¥éª¤

1. **DDL/MDç”Ÿæˆ** (0% â†’ 40%)
2. **Question-SQLç”Ÿæˆ** (40% â†’ 70%)
3. **SQLéªŒè¯** (70% â†’ 90%)
4. **è®­ç»ƒæ•°æ®åŠ è½½** (90% â†’ 100%)

### 3.3 å‘½ä»¤è¡Œå‚æ•°

#### å¿…éœ€å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| `--db-connection` | string | PostgreSQLè¿æ¥å­—ç¬¦ä¸² | `postgresql://user:pass@host:5432/dbname` |
| `--table-list` | string | è¡¨æ¸…å•æ–‡ä»¶è·¯å¾„ | `./tables.txt` |
| `--business-context` | string | ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿° | `"é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ"` |

#### å¯é€‰å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `--output-dir` | string | `./data_pipeline/training_data/` | è¾“å‡ºç›®å½•è·¯å¾„ |
| `--skip-validation` | flag | `False` | è·³è¿‡SQLéªŒè¯æ­¥éª¤ |
| `--disable-llm-repair` | flag | `False` | ç¦ç”¨LLMä¿®å¤åŠŸèƒ½ |
| `--no-modify-file` | flag | `False` | ä¸ä¿®æ”¹åŸå§‹JSONæ–‡ä»¶ |
| `--skip-training-load` | flag | `False` | è·³è¿‡è®­ç»ƒæ•°æ®åŠ è½½æ­¥éª¤ |
| `--verbose` | flag | `False` | å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º |
| `--log-file` | string | æ—  | æŒ‡å®šæ—¥å¿—æ–‡ä»¶è·¯å¾„ |

### 3.4 ä½¿ç”¨ç¤ºä¾‹

#### åŸºæœ¬ä½¿ç”¨ï¼ˆå®Œæ•´å·¥ä½œæµï¼‰
```bash
python -m data_pipeline.schema_workflow \
  --db-connection "postgresql://postgres:postgres@localhost:5432/highway_db" \
  --table-list ./tables.txt \
  --business-context "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ"
```

#### è·³è¿‡SQLéªŒè¯
```bash
python -m data_pipeline.schema_workflow \
  --db-connection "postgresql://user:pass@localhost:5432/ecommerce_db" \
  --table-list ./tables.txt \
  --business-context "ç”µå•†ç³»ç»Ÿ" \
  --skip-validation
```

#### ç¦ç”¨LLMä¿®å¤
```bash
python -m data_pipeline.schema_workflow \
  --db-connection "postgresql://user:pass@localhost:5432/management_db" \
  --table-list ./tables.txt \
  --business-context "ç®¡ç†ç³»ç»Ÿ" \
  --disable-llm-repair
```

#### è¯¦ç»†æ—¥å¿—è¾“å‡º
```bash
python -m data_pipeline.schema_workflow \
  --db-connection "postgresql://user:pass@localhost:5432/business_db" \
  --table-list ./tables.txt \
  --business-context "ä¸šåŠ¡ç³»ç»Ÿ" \
  --verbose \
  --log-file ./logs/workflow.log
```

## 4. åˆ†æ­¥æ‰§è¡Œè„šæœ¬

### 4.1 DDL/MDæ–‡æ¡£ç”Ÿæˆ

**å…¥å£ç‚¹**: `data_pipeline/ddl_generation/ddl_md_generator.py`

#### å‚æ•°åˆ—è¡¨

| å‚æ•° | å¿…éœ€ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `--db-connection` | âœ… | string | - | æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸² |
| `--table-list` | âœ… | string | - | è¡¨æ¸…å•æ–‡ä»¶è·¯å¾„ |
| `--business-context` | âœ… | string | - | ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿° |
| `--output-dir` | âŒ | string | configå€¼ | è¾“å‡ºç›®å½•è·¯å¾„ |
| `--pipeline` | âŒ | enum | `full` | å¤„ç†é“¾ç±»å‹ï¼š`full`/`ddl_only`/`analysis_only` |
| `--max-concurrent` | âŒ | int | `1` | æœ€å¤§å¹¶å‘è¡¨æ•°é‡ |
| `--verbose` | âŒ | flag | `False` | å¯ç”¨è¯¦ç»†æ—¥å¿— |
| `--log-file` | âŒ | string | - | æ—¥å¿—æ–‡ä»¶è·¯å¾„ |
| `--no-filter-system-tables` | âŒ | flag | `False` | ç¦ç”¨ç³»ç»Ÿè¡¨è¿‡æ»¤ |
| `--check-permissions-only` | âŒ | flag | `False` | ä»…æ£€æŸ¥æ•°æ®åº“æƒé™ |

#### ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºæœ¬ä½¿ç”¨
python -m data_pipeline.ddl_generation.ddl_md_generator \
  --db-connection "postgresql://postgres:postgres@localhost:5432/highway_db" \
  --table-list ./tables.txt \
  --business-context "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ"

# ä»…ç”ŸæˆDDLæ–‡ä»¶
python -m data_pipeline.ddl_generation.ddl_md_generator \
  --db-connection "postgresql://user:pass@localhost:5432/db" \
  --table-list ./tables.txt \
  --business-context "ç³»ç»Ÿ" \
  --pipeline ddl_only

# æƒé™æ£€æŸ¥
python -m data_pipeline.ddl_generation.ddl_md_generator \
  --db-connection "postgresql://user:pass@localhost:5432/db" \
  --check-permissions-only
```

### 4.2 Question-SQLå¯¹ç”Ÿæˆ

**å…¥å£ç‚¹**: `data_pipeline/qa_generation/qs_generator.py`

#### å‚æ•°åˆ—è¡¨

| å‚æ•° | å¿…éœ€ | ç±»å‹ | è¯´æ˜ |
|------|------|------|------|
| `--output-dir` | âœ… | string | åŒ…å«DDLå’ŒMDæ–‡ä»¶çš„ç›®å½• |
| `--table-list` | âœ… | string | è¡¨æ¸…å•æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºéªŒè¯ï¼‰ |
| `--business-context` | âœ… | string | ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿° |
| `database_name` | âœ… | positional | æ•°æ®åº“åç§° |
| `--verbose` | âŒ | flag | å¯ç”¨è¯¦ç»†æ—¥å¿— |

#### ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºæœ¬ä½¿ç”¨
python -m data_pipeline.qa_generation.qs_generator \
  --output-dir ./data_pipeline/training_data/ \
  --table-list ./tables.txt \
  --business-context "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ" \
  highway_db

# è¯¦ç»†æ—¥å¿—
python -m data_pipeline.qa_generation.qs_generator \
  --output-dir ./data_pipeline/training_data/ \
  --table-list ./tables.txt \
  --business-context "ç”µå•†ç³»ç»Ÿ" \
  ecommerce_db \
  --verbose
```

### 4.3 SQLéªŒè¯å·¥å…·

**å…¥å£ç‚¹**: `data_pipeline/validators/sql_validate_cli.py`

#### å‚æ•°åˆ—è¡¨

| å‚æ•° | å¿…éœ€ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `--db-connection` | âœ… | string | - | æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸² |
| `--input-file` | âœ… | string | - | Question-SQLæ–‡ä»¶è·¯å¾„ |
| `--output-dir` | âŒ | string | è¾“å…¥æ–‡ä»¶åŒç›®å½• | éªŒè¯æŠ¥å‘Šè¾“å‡ºç›®å½• |
| `--disable-llm-repair` | âŒ | flag | `False` | ç¦ç”¨LLMä¿®å¤åŠŸèƒ½ |
| `--no-modify-file` | âŒ | flag | `False` | ä¸ä¿®æ”¹åŸå§‹JSONæ–‡ä»¶ |
| `--max-concurrent` | âŒ | int | `5` | æœ€å¤§å¹¶å‘éªŒè¯æ•° |
| `--batch-size` | âŒ | int | `10` | æ‰¹å¤„ç†å¤§å° |
| `--timeout` | âŒ | int | `30` | å•ä¸ªéªŒè¯è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰ |
| `--verbose` | âŒ | flag | `False` | å¯ç”¨è¯¦ç»†æ—¥å¿— |
| `--dry-run` | âŒ | flag | `False` | ä»…è§£ææ–‡ä»¶ä¸æ‰§è¡ŒéªŒè¯ |
| `--save-json` | âŒ | flag | `False` | ä¿å­˜è¯¦ç»†JSONæŠ¥å‘Š |

#### ä½¿ç”¨ç¤ºä¾‹

```bash
# åŸºæœ¬éªŒè¯ï¼ˆé»˜è®¤ï¼šå¯ç”¨LLMä¿®å¤å’Œæ–‡ä»¶ä¿®æ”¹ï¼‰
python -m data_pipeline.validators.sql_validate_cli \
  --db-connection "postgresql://user:pass@localhost:5432/dbname" \
  --input-file ./qs_highway_db_20240123_143052_pair.json

# ä»…ç”ŸæˆæŠ¥å‘Šï¼Œä¸ä¿®æ”¹æ–‡ä»¶
python -m data_pipeline.validators.sql_validate_cli \
  --db-connection "postgresql://user:pass@localhost:5432/dbname" \
  --input-file ./data.json \
  --no-modify-file

# æ€§èƒ½è°ƒä¼˜å‚æ•°
python -m data_pipeline.validators.sql_validate_cli \
  --db-connection "postgresql://user:pass@localhost:5432/dbname" \
  --input-file ./data.json \
  --max-concurrent 10 \
  --batch-size 20 \
  --timeout 60 \
  --verbose
```

### 4.4 è®­ç»ƒæ•°æ®åŠ è½½

**å…¥å£ç‚¹**: `data_pipeline/trainer/run_training.py`

#### å‚æ•°åˆ—è¡¨

| å‚æ•° | å¿…éœ€ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `--data_path` | âŒ | string | configå€¼ | è®­ç»ƒæ•°æ®ç›®å½•è·¯å¾„ |

#### æ”¯æŒçš„æ–‡ä»¶æ ¼å¼

- **`.ddl`** æ–‡ä»¶ â†’ `train_ddl_statements()`
- **`.md/.markdown`** â†’ `train_documentation_blocks()`
- **`_pair.json/_pairs.json`** â†’ `train_json_question_sql_pairs()`
- **`_pair.sql/_pairs.sql`** â†’ `train_formatted_question_sql_pairs()`
- **`.sql`** (å…¶ä»–) â†’ `train_sql_examples()`

#### ä½¿ç”¨ç¤ºä¾‹

```bash
# ä½¿ç”¨é»˜è®¤è·¯å¾„
python -m data_pipeline.trainer.run_training

# æŒ‡å®šè·¯å¾„
python -m data_pipeline.trainer.run_training \
  --data_path ./data_pipeline/training_data/task_20250627_143052/
```

## 5. æ—¥å¿—é…ç½®

### 5.1 æ—¥å¿—ç³»ç»Ÿæ¶æ„

data_pipelineä½¿ç”¨ç»Ÿä¸€çš„æ—¥å¿—ç®¡ç†ç³»ç»Ÿï¼Œæ”¯æŒä¸¤ç§æ¨¡å¼ï¼š

1. **è„šæœ¬æ¨¡å¼**: ç”Ÿæˆ`manual_YYYYMMDD_HHMMSS`æ ¼å¼çš„task_id
2. **APIæ¨¡å¼**: ä½¿ç”¨ä¼ å…¥çš„task_id

### 5.2 æ—¥å¿—æ–‡ä»¶ä½ç½®

#### è„šæœ¬æ¨¡å¼æ—¥å¿—
```
data_pipeline/training_data/manual_20250627_143052/
â””â”€â”€ data_pipeline.log                    # è¯¦ç»†æ‰§è¡Œæ—¥å¿—
```

#### APIæ¨¡å¼æ—¥å¿—
```
data_pipeline/training_data/task_20250627_143052/
â””â”€â”€ data_pipeline.log                    # è¯¦ç»†æ‰§è¡Œæ—¥å¿—
```

#### ç³»ç»Ÿæ—¥å¿—
```
logs/
â”œâ”€â”€ app.log                              # åº”ç”¨ç³»ç»Ÿæ—¥å¿—
â”œâ”€â”€ agent.log                            # Agentæ—¥å¿—
â”œâ”€â”€ vanna.log                            # Vannaæ—¥å¿—
â””â”€â”€ data_pipeline.log                    # data_pipelineæ¨¡å—æ—¥å¿—ï¼ˆå·²å¼ƒç”¨ï¼‰
```

### 5.3 æ—¥å¿—é…ç½®æ–¹å¼

#### 5.3.1 ä½¿ç”¨å†…ç½®æ—¥å¿—ç³»ç»Ÿ

```python
from data_pipeline.dp_logging import get_logger

# è„šæœ¬æ¨¡å¼
task_id = f"manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
logger = get_logger("SchemaWorkflow", task_id)

# APIæ¨¡å¼
logger = get_logger("SchemaWorkflow", "task_20250627_143052")
```

#### 5.3.2 æ—¥å¿—çº§åˆ«é…ç½®

| çº§åˆ« | ç”¨é€” | è¾“å‡ºä½ç½® |
|------|------|----------|
| `DEBUG` | è¯¦ç»†è°ƒè¯•ä¿¡æ¯ | æ–‡ä»¶ |
| `INFO` | æ­£å¸¸æµç¨‹ä¿¡æ¯ | æ§åˆ¶å° + æ–‡ä»¶ |
| `WARNING` | è­¦å‘Šä¿¡æ¯ | æ§åˆ¶å° + æ–‡ä»¶ |
| `ERROR` | é”™è¯¯ä¿¡æ¯ | æ§åˆ¶å° + æ–‡ä»¶ |
| `CRITICAL` | ä¸¥é‡é”™è¯¯ | æ§åˆ¶å° + æ–‡ä»¶ |

#### 5.3.3 æ—¥å¿—æ ¼å¼

```
2025-07-01 14:30:52 [INFO] [SchemaWorkflowOrchestrator] schema_workflow.py:123 - ğŸš€ å¼€å§‹æ‰§è¡ŒSchemaå·¥ä½œæµç¼–æ’
2025-07-01 14:30:53 [INFO] [DDLMDGenerator] ddl_md_generator.py:45 - å¼€å§‹å¤„ç†è¡¨: bss_business_day_data
2025-07-01 14:31:20 [WARNING] [SQLValidator] sql_validator.py:78 - SQLéªŒè¯å¤±è´¥ï¼Œå°è¯•LLMä¿®å¤
2025-07-01 14:31:25 [ERROR] [TrainingDataLoader] run_training.py:234 - è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥: è¿æ¥è¶…æ—¶
```

### 5.4 æ—¥å¿—é…ç½®å‚æ•°

#### 5.4.1 å‘½ä»¤è¡Œå‚æ•°

```bash
# å¯ç”¨è¯¦ç»†æ—¥å¿—
python -m data_pipeline.schema_workflow \
  --verbose \
  [å…¶ä»–å‚æ•°]

# æŒ‡å®šæ—¥å¿—æ–‡ä»¶
python -m data_pipeline.schema_workflow \
  --log-file ./custom_logs/workflow.log \
  [å…¶ä»–å‚æ•°]
```

#### 5.4.2 ç¯å¢ƒå˜é‡é…ç½®

```bash
# è®¾ç½®æ—¥å¿—çº§åˆ«
export DATA_PIPELINE_LOG_LEVEL=DEBUG

# è®¾ç½®æ—¥å¿—ç›®å½•
export DATA_PIPELINE_LOG_DIR=./logs/data_pipeline/
```

#### 5.4.3 ç¼–ç¨‹æ–¹å¼é…ç½®

```python
import logging
from data_pipeline.dp_logging import get_logger

# é…ç½®æ—¥å¿—çº§åˆ«
logging.getLogger("data_pipeline").setLevel(logging.DEBUG)

# è·å–logger
logger = get_logger("CustomModule", "manual_20250627_143052")
logger.info("è‡ªå®šä¹‰æ—¥å¿—æ¶ˆæ¯")
```

## 6. é…ç½®æ–‡ä»¶

### 6.1 ä¸»é…ç½®æ–‡ä»¶

**ä½ç½®**: `data_pipeline/config.py`  
**å˜é‡**: `SCHEMA_TOOLS_CONFIG`

### 6.2 ä¸»è¦é…ç½®é¡¹

#### 6.2.1 æ ¸å¿ƒé…ç½®

```python
{
    "output_directory": "./data_pipeline/training_data/",
    "default_business_context": "æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ",
    "default_pipeline": "full"
}
```

#### 6.2.2 å¤„ç†é“¾é…ç½®

```python
{
    "available_pipelines": {
        "full": ["database_inspector", "data_sampler", "comment_generator", "ddl_generator", "doc_generator"],
        "ddl_only": ["database_inspector", "data_sampler", "comment_generator", "ddl_generator"],
        "analysis_only": ["database_inspector", "data_sampler", "comment_generator"]
    }
}
```

#### 6.2.3 æ•°æ®å¤„ç†é…ç½®

```python
{
    "sample_data_limit": 20,                    # LLMåˆ†æçš„é‡‡æ ·æ•°æ®é‡
    "enum_detection_sample_limit": 5000,        # æšä¸¾æ£€æµ‹é‡‡æ ·é™åˆ¶
    "enum_max_distinct_values": 20,             # æšä¸¾å­—æ®µæœ€å¤§ä¸åŒå€¼æ•°é‡
    "large_table_threshold": 1000000            # å¤§è¡¨é˜ˆå€¼ï¼ˆè¡Œæ•°ï¼‰
}
```

#### 6.2.4 å¹¶å‘é…ç½®

```python
{
    "max_concurrent_tables": 1,                 # æœ€å¤§å¹¶å‘å¤„ç†è¡¨æ•°
    "max_concurrent_themes": 1,                 # å¹¶è¡Œå¤„ç†çš„ä¸»é¢˜æ•°é‡
}
```

#### 6.2.5 Question-SQLç”Ÿæˆé…ç½®

```python
{
    "qs_generation": {
        "max_tables": 20,                       # æœ€å¤§è¡¨æ•°é‡é™åˆ¶
        "theme_count": 5,                       # LLMç”Ÿæˆçš„ä¸»é¢˜æ•°é‡
        "questions_per_theme": 10,              # æ¯ä¸ªä¸»é¢˜ç”Ÿæˆçš„é—®é¢˜æ•°
        "continue_on_theme_error": True,        # ä¸»é¢˜ç”Ÿæˆå¤±è´¥æ˜¯å¦ç»§ç»­
        "save_intermediate": True,              # æ˜¯å¦ä¿å­˜ä¸­é—´ç»“æœ
        "output_file_prefix": "qs"              # è¾“å‡ºæ–‡ä»¶å‰ç¼€
    }
}
```

#### 6.2.6 SQLéªŒè¯é…ç½®

```python
{
    "sql_validation": {
        "max_concurrent_validations": 5,        # å¹¶å‘éªŒè¯æ•°
        "validation_timeout": 30,               # å•ä¸ªéªŒè¯è¶…æ—¶(ç§’)
        "batch_size": 10,                       # æ‰¹å¤„ç†å¤§å°
        "enable_sql_repair": False,             # SQLä¿®å¤åŠŸèƒ½
        "modify_original_file": False,          # æ–‡ä»¶ä¿®æ”¹åŠŸèƒ½
        "readonly_mode": True                   # å¯ç”¨åªè¯»æ¨¡å¼
    }
}
```

### 6.3 é…ç½®ä¼˜å…ˆçº§

```
å‘½ä»¤è¡Œå‚æ•° > data_pipeline/config.py > é»˜è®¤å€¼
```

### 6.4 ä¿®æ”¹é…ç½®

#### æ–¹æ³•1: ç›´æ¥ä¿®æ”¹é…ç½®æ–‡ä»¶

ç¼–è¾‘ `data_pipeline/config.py` æ–‡ä»¶ä¸­çš„ `SCHEMA_TOOLS_CONFIG` å­—å…¸ã€‚

#### æ–¹æ³•2: ç¼–ç¨‹æ–¹å¼ä¿®æ”¹

```python
from data_pipeline.config import SCHEMA_TOOLS_CONFIG, update_config

# ä¿®æ”¹å•ä¸ªé…ç½®é¡¹
update_config("max_concurrent_tables", 2)

# æ‰¹é‡ä¿®æ”¹é…ç½®
update_config({
    "sample_data_limit": 50,
    "qs_generation.theme_count": 8
})
```

## 7. ä½¿ç”¨ç¤ºä¾‹

### 7.1 å…¸å‹å·¥ä½œæµåœºæ™¯

#### åœºæ™¯1: é¦–æ¬¡å¤„ç†æ–°æ•°æ®åº“
```bash
# 1. æƒé™æ£€æŸ¥
python -m data_pipeline.ddl_generation.ddl_md_generator \
  --db-connection "postgresql://user:pass@localhost:5432/new_db" \
  --check-permissions-only

# 2. å®Œæ•´å·¥ä½œæµ
python -m data_pipeline.schema_workflow \
  --db-connection "postgresql://user:pass@localhost:5432/new_db" \
  --table-list ./tables.txt \
  --business-context "æ–°ä¸šåŠ¡ç³»ç»Ÿ" \
  --verbose
```

#### åœºæ™¯2: è°ƒè¯•æ¨¡å¼åˆ†æ­¥æ‰§è¡Œ
```bash
# 1. ä»…ç”ŸæˆDDLå’ŒMD
python -m data_pipeline.ddl_generation.ddl_md_generator \
  --db-connection "postgresql://user:pass@localhost:5432/db" \
  --table-list ./tables.txt \
  --business-context "æµ‹è¯•ç³»ç»Ÿ" \
  --output-dir ./debug_output/

# 2. æ£€æŸ¥ç»“æœåç”ŸæˆQ&A
python -m data_pipeline.qa_generation.qs_generator \
  --output-dir ./debug_output/ \
  --table-list ./tables.txt \
  --business-context "æµ‹è¯•ç³»ç»Ÿ" \
  test_db

# 3. éªŒè¯SQL
python -m data_pipeline.validators.sql_validate_cli \
  --db-connection "postgresql://user:pass@localhost:5432/db" \
  --input-file ./debug_output/qs_test_db_*.json \
  --verbose
```

#### åœºæ™¯3: ç”Ÿäº§ç¯å¢ƒæ‰¹é‡å¤„ç†
```bash
# åˆ›å»ºå¤„ç†è„šæœ¬
cat > process_databases.sh << 'EOF'
#!/bin/bash

DATABASES=("db1" "db2" "db3")
BASE_CONNECTION="postgresql://user:pass@localhost:5432"

for db in "${DATABASES[@]}"; do
    echo "Processing database: $db"
    
    python -m data_pipeline.schema_workflow \
      --db-connection "${BASE_CONNECTION}/${db}" \
      --table-list "./tables_${db}.txt" \
      --business-context "${db}ä¸šåŠ¡ç³»ç»Ÿ" \
      --output-dir "./output/${db}/" \
      --verbose
    
    if [ $? -eq 0 ]; then
        echo "âœ… $db processed successfully"
    else
        echo "âŒ $db processing failed"
    fi
done
EOF

chmod +x process_databases.sh
./process_databases.sh
```

### 7.2 è¡¨æ¸…å•æ–‡ä»¶æ ¼å¼

#### åŸºæœ¬æ ¼å¼
```
# tables.txt
bss_business_day_data
bss_car_day_count
bss_company
bss_service_area
```

#### åŒ…å«Schemaçš„æ ¼å¼
```
# tables_with_schema.txt
public.bss_business_day_data
public.bss_car_day_count
ods.dim_company
ods.fact_revenue
```

#### å¸¦æ³¨é‡Šçš„æ ¼å¼
```
# tables_commented.txt
# ä¸šåŠ¡æ•°æ®è¡¨
bss_business_day_data    # è¥ä¸šæ—¥æ•°æ®
bss_car_day_count        # è½¦æµé‡ç»Ÿè®¡

# åŸºç¡€æ•°æ®è¡¨
bss_company              # å…¬å¸ä¿¡æ¯
bss_service_area         # æœåŠ¡åŒºä¿¡æ¯
```

### 7.3 è¾“å‡ºæ–‡ä»¶ç»“æ„

```
data_pipeline/training_data/manual_20250627_143052/
â”œâ”€â”€ data_pipeline.log                           # è¯¦ç»†æ‰§è¡Œæ—¥å¿—
â”œâ”€â”€ bss_business_day_data.ddl                   # DDLæ–‡ä»¶
â”œâ”€â”€ bss_business_day_data_detail.md             # MDæ–‡æ¡£
â”œâ”€â”€ bss_car_day_count.ddl
â”œâ”€â”€ bss_car_day_count_detail.md
â”œâ”€â”€ qs_highway_db_20250627_143052_pair.json     # Question-SQLå¯¹
â”œâ”€â”€ qs_highway_db_20250627_143052_pair.json.backup  # å¤‡ä»½æ–‡ä»¶
â”œâ”€â”€ sql_validation_20250627_143052_summary.log  # SQLéªŒè¯æ‘˜è¦
â”œâ”€â”€ sql_validation_20250627_143052_report.json  # SQLéªŒè¯è¯¦ç»†æŠ¥å‘Š
â”œâ”€â”€ file_modifications_20250627_143052.log      # æ–‡ä»¶ä¿®æ”¹æ—¥å¿—
â”œâ”€â”€ metadata.txt                                # å…ƒæ•°æ®æ–‡ä»¶
â””â”€â”€ filename_mapping.txt                        # æ–‡ä»¶åæ˜ å°„
```

## 8. æ•…éšœæ’æŸ¥

### 8.1 å¸¸è§é”™è¯¯

#### 8.1.1 è¡¨æ•°é‡è¶…è¿‡é™åˆ¶
```
é”™è¯¯ä¿¡æ¯: è¡¨æ•°é‡(25)è¶…è¿‡é™åˆ¶(20)ã€‚è¯·åˆ†æ‰¹å¤„ç†æˆ–è°ƒæ•´é…ç½®ä¸­çš„max_tableså‚æ•°ã€‚

è§£å†³æ–¹æ¡ˆ:
1. åˆ†æ‰¹å¤„ç†ï¼šå°†è¡¨æ¸…å•åˆ†æˆå¤šä¸ªæ–‡ä»¶
2. ä¿®æ”¹é…ç½®ï¼šåœ¨config.pyä¸­å¢åŠ max_tablesé™åˆ¶
```

#### 8.1.2 DDLå’ŒMDæ–‡ä»¶æ•°é‡ä¸ä¸€è‡´
```
é”™è¯¯ä¿¡æ¯: DDLæ–‡ä»¶æ•°é‡(5)ä¸è¡¨æ•°é‡(6)ä¸ä¸€è‡´

è§£å†³æ–¹æ¡ˆ:
1. æ£€æŸ¥æ—¥å¿—æ–‡ä»¶æ‰¾å‡ºå¤±è´¥çš„è¡¨
2. é‡æ–°è¿è¡ŒDDL/MDç”Ÿæˆ
3. æ£€æŸ¥æ•°æ®åº“æƒé™
```

#### 8.1.3 LLMè°ƒç”¨å¤±è´¥
```
é”™è¯¯ä¿¡æ¯: LLMè°ƒç”¨è¶…æ—¶æˆ–å¤±è´¥

è§£å†³æ–¹æ¡ˆ:
1. æ£€æŸ¥ç½‘ç»œè¿æ¥
2. æŸ¥çœ‹ä¸­é—´ç»“æœæ–‡ä»¶ï¼Œä»æ–­ç‚¹ç»§ç»­
3. å‡å°‘è¡¨æ•°é‡æˆ–åˆ†æ‰¹å¤„ç†
4. æ£€æŸ¥LLMæœåŠ¡é…ç½®
```

#### 8.1.4 æƒé™ä¸è¶³
```
é”™è¯¯ä¿¡æ¯: æ•°æ®åº“æŸ¥è¯¢æƒé™ä¸è¶³

è§£å†³æ–¹æ¡ˆ:
1. ä½¿ç”¨--check-permissions-onlyæ£€æŸ¥æƒé™
2. ç¡®ä¿æ•°æ®åº“ç”¨æˆ·æœ‰SELECTæƒé™
3. Data Pipelineæ”¯æŒåªè¯»æ•°æ®åº“
```

### 8.2 æ—¥å¿—åˆ†æ

#### 8.2.1 æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
```bash
# æŸ¥çœ‹æœ€æ–°çš„ä»»åŠ¡æ—¥å¿—
find data_pipeline/training_data/ -name "data_pipeline.log" -exec ls -t {} + | head -1 | xargs tail -f

# æœç´¢é”™è¯¯ä¿¡æ¯
grep -i "error" data_pipeline/training_data/manual_*/data_pipeline.log

# æœç´¢ç‰¹å®šè¡¨çš„å¤„ç†æ—¥å¿—
grep "bss_company" data_pipeline/training_data/manual_*/data_pipeline.log
```

#### 8.2.2 æ—¥å¿—çº§åˆ«è°ƒæ•´
```bash
# å¯ç”¨DEBUGçº§åˆ«æ—¥å¿—
python -m data_pipeline.schema_workflow \
  --verbose \
  [å…¶ä»–å‚æ•°]
```

### 8.3 æ€§èƒ½ä¼˜åŒ–

#### 8.3.1 å¹¶å‘é…ç½®è°ƒä¼˜
```python
# åœ¨config.pyä¸­è°ƒæ•´
"max_concurrent_tables": 2,              # å¢åŠ å¹¶å‘æ•°ï¼ˆè°¨æ…ï¼‰
"max_concurrent_validations": 10,        # å¢åŠ SQLéªŒè¯å¹¶å‘æ•°
"batch_size": 20                         # å¢åŠ æ‰¹å¤„ç†å¤§å°
```

#### 8.3.2 æ•°æ®é‡‡æ ·ä¼˜åŒ–
```python
# å‡å°‘é‡‡æ ·æ•°æ®é‡
"sample_data_limit": 10,                 # ä»20å‡å°‘åˆ°10
"enum_detection_sample_limit": 1000      # ä»5000å‡å°‘åˆ°1000
```

#### 8.3.3 è·³è¿‡è€—æ—¶æ­¥éª¤
```bash
# è·³è¿‡SQLéªŒè¯
python -m data_pipeline.schema_workflow \
  --skip-validation \
  [å…¶ä»–å‚æ•°]

# è·³è¿‡è®­ç»ƒæ•°æ®åŠ è½½
python -m data_pipeline.schema_workflow \
  --skip-training-load \
  [å…¶ä»–å‚æ•°]
```

### 8.4 ç¯å¢ƒæ£€æŸ¥

#### 8.4.1 ä¾èµ–æ£€æŸ¥
```bash
# æ£€æŸ¥Pythonç‰ˆæœ¬
python --version

# æ£€æŸ¥å¿…éœ€åŒ…
pip list | grep -E "(asyncpg|psycopg2|vanna)"

# æ£€æŸ¥æ•°æ®åº“è¿æ¥
python -c "
import asyncpg
import asyncio
async def test():
    conn = await asyncpg.connect('postgresql://user:pass@host:5432/db')
    await conn.close()
    print('âœ… æ•°æ®åº“è¿æ¥æ­£å¸¸')
asyncio.run(test())
"
```

#### 8.4.2 æƒé™æ£€æŸ¥
```bash
# ä½¿ç”¨å†…ç½®æƒé™æ£€æŸ¥å·¥å…·
python -m data_pipeline.ddl_generation.ddl_md_generator \
  --db-connection "postgresql://user:pass@localhost:5432/db" \
  --check-permissions-only
```

#### 8.4.3 ç£ç›˜ç©ºé—´æ£€æŸ¥
```bash
# æ£€æŸ¥è¾“å‡ºç›®å½•ç©ºé—´
df -h data_pipeline/training_data/

# æ¸…ç†æ—§ä»»åŠ¡ï¼ˆè°¨æ…æ“ä½œï¼‰
find data_pipeline/training_data/ -type d -name "manual_*" -mtime +7 -exec rm -rf {} +
```

## æ€»ç»“

data_pipelineæ¨¡å—æä¾›äº†å®Œæ•´çš„è„šæœ¬åŒ–è°ƒç”¨æ–¹å¼ï¼Œæ”¯æŒä»ç®€å•çš„ä¸€é”®å·¥ä½œæµåˆ°å¤æ‚çš„åˆ†æ­¥è°ƒè¯•ã€‚é€šè¿‡åˆç†é…ç½®å‚æ•°å’Œæ—¥å¿—ç³»ç»Ÿï¼Œå¯ä»¥æ»¡è¶³å„ç§æ•°æ®å¤„ç†éœ€æ±‚ã€‚å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ä½¿ç”¨ä¸€é”®å·¥ä½œæµè„šæœ¬ï¼Œåœ¨å¼€å‘è°ƒè¯•æ—¶ä½¿ç”¨åˆ†æ­¥æ‰§è¡Œè„šæœ¬ã€‚ 