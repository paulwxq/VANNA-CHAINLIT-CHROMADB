# Data Pipeline APIè°ƒç”¨æŒ‡å—

## æ¦‚è¿°

æœ¬æ–‡æ¡£è¯¦ç»†ä»‹ç»data_pipelineæ¨¡å—çš„APIè°ƒç”¨æ–¹å¼ã€æ”¯æŒçš„å‚æ•°åˆ—è¡¨ä»¥åŠAPIè°ƒç”¨æ—¶çš„æ—¥å¿—é…ç½®æ–¹å¼ã€‚data_pipeline APIç³»ç»ŸåŸºäºFlaskæ¡†æ¶æ„å»ºï¼Œæä¾›RESTfulæ¥å£æ”¯æŒåå°ä»»åŠ¡æ‰§è¡Œã€è¿›åº¦è¿½è¸ªå’Œç»“æœç®¡ç†ã€‚

## ç›®å½•

1. [APIæ¶æ„æ¦‚è§ˆ](#1-apiæ¶æ„æ¦‚è§ˆ)
2. [æ ¸å¿ƒAPIç«¯ç‚¹](#2-æ ¸å¿ƒapiç«¯ç‚¹)
3. [ä»»åŠ¡ç®¡ç†API](#3-ä»»åŠ¡ç®¡ç†api)
4. [æ–‡ä»¶ç®¡ç†API](#4-æ–‡ä»¶ç®¡ç†api)
5. [æ—¥å¿—ç®¡ç†API](#5-æ—¥å¿—ç®¡ç†api)
6. [æ•°æ®åº“å·¥å…·API](#6-æ•°æ®åº“å·¥å…·api)
7. [æ—¥å¿—é…ç½®](#7-æ—¥å¿—é…ç½®)
8. [ä½¿ç”¨ç¤ºä¾‹](#8-ä½¿ç”¨ç¤ºä¾‹)
9. [é”™è¯¯å¤„ç†](#9-é”™è¯¯å¤„ç†)
10. [æœ€ä½³å®è·µ](#10-æœ€ä½³å®è·µ)

## 1. APIæ¶æ„æ¦‚è§ˆ

### 1.1 ç³»ç»Ÿæ¶æ„

```
unified_api.py (Flaskåº”ç”¨)
â”œâ”€â”€ ä»»åŠ¡ç®¡ç†API
â”‚   â”œâ”€â”€ POST /api/v0/data_pipeline/tasks                    # åˆ›å»ºä»»åŠ¡
â”‚   â”œâ”€â”€ POST /api/v0/data_pipeline/tasks/{task_id}/execute  # æ‰§è¡Œä»»åŠ¡
â”‚   â”œâ”€â”€ GET  /api/v0/data_pipeline/tasks/{task_id}          # è·å–ä»»åŠ¡çŠ¶æ€
â”‚   â””â”€â”€ GET  /api/v0/data_pipeline/tasks                    # è·å–ä»»åŠ¡åˆ—è¡¨
â”œâ”€â”€ æ–‡ä»¶ç®¡ç†API
â”‚   â”œâ”€â”€ GET  /api/v0/data_pipeline/tasks/{task_id}/files    # è·å–æ–‡ä»¶åˆ—è¡¨
â”‚   â”œâ”€â”€ GET  /api/v0/data_pipeline/tasks/{task_id}/files/{filename} # ä¸‹è½½æ–‡ä»¶
â”‚   â””â”€â”€ POST /api/v0/data_pipeline/tasks/{task_id}/upload-table-list # ä¸Šä¼ è¡¨æ¸…å•
â”œâ”€â”€ æ—¥å¿—ç®¡ç†API
â”‚   â”œâ”€â”€ GET  /api/v0/data_pipeline/tasks/{task_id}/logs     # è·å–ä»»åŠ¡æ—¥å¿—
â”‚   â””â”€â”€ POST /api/v0/data_pipeline/tasks/{task_id}/logs/query # é«˜çº§æ—¥å¿—æŸ¥è¯¢
â””â”€â”€ æ•°æ®åº“å·¥å…·API
    â”œâ”€â”€ POST /api/v0/database/tables                        # è·å–æ•°æ®åº“è¡¨åˆ—è¡¨
    â””â”€â”€ POST /api/v0/database/table/ddl                     # è·å–è¡¨DDL/MDæ–‡æ¡£
```

### 1.2 æ ¸å¿ƒç»„ä»¶

| ç»„ä»¶ | åŠŸèƒ½ | å®ç°ä½ç½® |
|------|------|----------|
| **SimpleWorkflowManager** | ä»»åŠ¡ç®¡ç†å™¨ | `data_pipeline.api.simple_workflow` |
| **SimpleFileManager** | æ–‡ä»¶ç®¡ç†å™¨ | `data_pipeline.api.simple_file_manager` |
| **SimpleTaskManager** | æ•°æ®åº“ç®¡ç†å™¨ | `data_pipeline.api.simple_db_manager` |
| **TableInspectorAPI** | æ•°æ®åº“æ£€æŸ¥å™¨ | `data_pipeline.api.table_inspector_api` |
| **task_executor.py** | ç‹¬ç«‹ä»»åŠ¡æ‰§è¡Œå™¨ | `data_pipeline.task_executor` |

### 1.3 ä»»åŠ¡æ‰§è¡Œæ¨¡å¼

| æ¨¡å¼ | æè¿° | ä½¿ç”¨åœºæ™¯ |
|------|------|----------|
| **å®Œæ•´å·¥ä½œæµ** | ä¸€æ¬¡æ€§æ‰§è¡Œ4ä¸ªæ­¥éª¤ | ç”Ÿäº§ç¯å¢ƒï¼Œè‡ªåŠ¨åŒ–å¤„ç† |
| **åˆ†æ­¥æ‰§è¡Œ** | é€æ­¥æ‰§è¡Œå„ä¸ªæ­¥éª¤ | è°ƒè¯•ï¼Œè´¨é‡æ§åˆ¶ |
| **åå°æ‰§è¡Œ** | ä½¿ç”¨subprocessç‹¬ç«‹è¿›ç¨‹ | é•¿æ—¶é—´ä»»åŠ¡ï¼Œä¸é˜»å¡API |
| **Vectorè¡¨ç®¡ç†** | å¤‡ä»½å’Œæ¸…ç©ºvectorè¡¨æ•°æ® | é‡æ–°è®­ç»ƒå‰æ¸…ç†æ—§æ•°æ® |

## 2. æ ¸å¿ƒAPIç«¯ç‚¹

### 2.1 åŸºç¡€ä¿¡æ¯

- **åŸºç¡€URL**: `http://localhost:8084`
- **APIç‰ˆæœ¬**: `v0`
- **å†…å®¹ç±»å‹**: `application/json`
- **å­—ç¬¦ç¼–ç **: `UTF-8`

### 2.2 é€šç”¨å“åº”æ ¼å¼

#### æˆåŠŸå“åº”
```json
{
    "success": true,
    "code": 200,
    "message": "æ“ä½œæˆåŠŸ",
    "data": {
        // å…·ä½“æ•°æ®
    }
}
```

#### é”™è¯¯å“åº”
```json
{
    "success": false,
    "code": 400,
    "message": "è¯·æ±‚å‚æ•°é”™è¯¯",
    "error": {
        "type": "ValidationError",
        "details": "ç¼ºå°‘å¿…éœ€å‚æ•°: table_list_file"
    }
}
```

### 2.3 HTTPçŠ¶æ€ç 

| çŠ¶æ€ç  | è¯´æ˜ | ä½¿ç”¨åœºæ™¯ |
|--------|------|----------|
| `200` | æˆåŠŸ | è·å–æ•°æ®æˆåŠŸ |
| `201` | åˆ›å»ºæˆåŠŸ | ä»»åŠ¡åˆ›å»ºæˆåŠŸ |
| `202` | å·²æ¥å— | ä»»åŠ¡æ‰§è¡Œå·²å¯åŠ¨ |
| `400` | è¯·æ±‚é”™è¯¯ | å‚æ•°éªŒè¯å¤±è´¥ |
| `404` | æœªæ‰¾åˆ° | ä»»åŠ¡ä¸å­˜åœ¨ |
| `409` | å†²çª | ä»»åŠ¡å·²åœ¨æ‰§è¡Œ |
| `500` | æœåŠ¡å™¨é”™è¯¯ | å†…éƒ¨é”™è¯¯ |

### 2.4 Vectorè¡¨ç®¡ç†åŠŸèƒ½

data_pipeline APIç°åœ¨æ”¯æŒVectorè¡¨ç®¡ç†åŠŸèƒ½ï¼Œç”¨äºå¤‡ä»½å’Œæ¸…ç©ºå‘é‡æ•°æ®ï¼š

#### å…³é”®å‚æ•°
- `backup_vector_tables`: å¤‡ä»½vectorè¡¨æ•°æ®åˆ°ä»»åŠ¡ç›®å½•
- `truncate_vector_tables`: æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰

#### å‚æ•°ä¾èµ–å…³ç³»
- âœ… å¯ä»¥å•ç‹¬ä½¿ç”¨ `backup_vector_tables`
- âŒ ä¸èƒ½å•ç‹¬ä½¿ç”¨ `truncate_vector_tables`  
- ğŸ”„ ä½¿ç”¨ `truncate_vector_tables` æ—¶è‡ªåŠ¨å¯ç”¨ `backup_vector_tables`

#### å½±å“çš„è¡¨
- `langchain_pg_collection`: åªå¤‡ä»½ï¼Œä¸æ¸…ç©º
- `langchain_pg_embedding`: å¤‡ä»½å¹¶æ¸…ç©º

#### åº”ç”¨åœºæ™¯
- **é‡æ–°è®­ç»ƒ**: åœ¨åŠ è½½æ–°è®­ç»ƒæ•°æ®å‰æ¸…ç©ºæ—§çš„embeddingæ•°æ®
- **æ•°æ®è¿ç§»**: å¤‡ä»½vectoræ•°æ®ç”¨äºç¯å¢ƒè¿ç§»
- **ç‰ˆæœ¬ç®¡ç†**: ä¿ç•™ä¸åŒç‰ˆæœ¬çš„vectoræ•°æ®å¤‡ä»½

## 3. ä»»åŠ¡ç®¡ç†API

### 3.1 åˆ›å»ºä»»åŠ¡

**ç«¯ç‚¹**: `POST /api/v0/data_pipeline/tasks`

#### è¯·æ±‚å‚æ•°

| å‚æ•° | å¿…éœ€ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `table_list_file` | âŒ | string | - | è¡¨æ¸…å•æ–‡ä»¶è·¯å¾„ï¼Œä¸æä¾›åˆ™éœ€è¦åç»­ä¸Šä¼  |
| `business_context` | âŒ | string | - | ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿° |
| `db_name` | âŒ | string | - | æ•°æ®åº“åç§° |
| `db_connection` | âŒ | string | - | æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ï¼Œä¸æä¾›åˆ™ä½¿ç”¨é»˜è®¤é…ç½® |
| `task_name` | âŒ | string | - | ä»»åŠ¡åç§° |
| `enable_sql_validation` | âŒ | boolean | `true` | æ˜¯å¦å¯ç”¨SQLéªŒè¯ |
| `enable_llm_repair` | âŒ | boolean | `true` | æ˜¯å¦å¯ç”¨LLMä¿®å¤ |
| `modify_original_file` | âŒ | boolean | `true` | æ˜¯å¦ä¿®æ”¹åŸå§‹æ–‡ä»¶ |
| `enable_training_data_load` | âŒ | boolean | `true` | æ˜¯å¦å¯ç”¨è®­ç»ƒæ•°æ®åŠ è½½ |
| `backup_vector_tables` | âŒ | boolean | `false` | æ˜¯å¦å¤‡ä»½vectorè¡¨æ•°æ® |
| `truncate_vector_tables` | âŒ | boolean | `false` | æ˜¯å¦æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰ |

#### è¯·æ±‚ç¤ºä¾‹

```bash
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "table_list_file": "tables.txt",
    "business_context": "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ",
    "db_name": "highway_db",
    "task_name": "é«˜é€Ÿå…¬è·¯æ•°æ®å¤„ç†",
    "enable_sql_validation": true,
    "enable_llm_repair": true,
    "modify_original_file": true,
    "enable_training_data_load": true,
    "backup_vector_tables": false,
    "truncate_vector_tables": false
  }'
```

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 201,
    "message": "ä»»åŠ¡åˆ›å»ºæˆåŠŸ",
    "data": {
        "task_id": "task_20250627_143052",
        "task_name": "é«˜é€Ÿå…¬è·¯æ•°æ®å¤„ç†",
        "status": "pending",
        "created_at": "2025-06-27T14:30:52"
    }
}
```

### 3.2 æ‰§è¡Œä»»åŠ¡

**ç«¯ç‚¹**: `POST /api/v0/data_pipeline/tasks/{task_id}/execute`

#### è¯·æ±‚å‚æ•°

| å‚æ•° | å¿…éœ€ | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|------|--------|------|
| `execution_mode` | âŒ | enum | `complete` | æ‰§è¡Œæ¨¡å¼ï¼š`complete`/`step` |
| `step_name` | âŒ | string | - | æ­¥éª¤åç§°ï¼Œæ­¥éª¤æ¨¡å¼æ—¶å¿…éœ€ |
| `backup_vector_tables` | âŒ | boolean | `false` | æ˜¯å¦å¤‡ä»½vectorè¡¨æ•°æ® |
| `truncate_vector_tables` | âŒ | boolean | `false` | æ˜¯å¦æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰ |

#### æœ‰æ•ˆæ­¥éª¤åç§°

- `ddl_generation` - DDL/MDæ–‡æ¡£ç”Ÿæˆ
- `qa_generation` - Question-SQLå¯¹ç”Ÿæˆ
- `sql_validation` - SQLéªŒè¯å’Œä¿®å¤
- `training_load` - è®­ç»ƒæ•°æ®åŠ è½½

#### è¯·æ±‚ç¤ºä¾‹

```bash
# æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/execute \
  -H "Content-Type: application/json" \
  -d '{
    "execution_mode": "complete",
    "backup_vector_tables": false,
    "truncate_vector_tables": false
  }'

# æ‰§è¡Œå®Œæ•´å·¥ä½œæµå¹¶æ¸…ç©ºvectorè¡¨
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/execute \
  -H "Content-Type: application/json" \
  -d '{
    "execution_mode": "complete",
    "truncate_vector_tables": true
  }'

# æ‰§è¡Œå•ä¸ªæ­¥éª¤
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/execute \
  -H "Content-Type: application/json" \
  -d '{
    "execution_mode": "step",
    "step_name": "ddl_generation",
    "backup_vector_tables": false,
    "truncate_vector_tables": false
  }'
```

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 202,
    "message": "ä»»åŠ¡æ‰§è¡Œå·²å¯åŠ¨",
    "data": {
        "task_id": "task_20250627_143052",
        "execution_mode": "complete",
        "step_name": null,
        "message": "ä»»åŠ¡æ­£åœ¨åå°æ‰§è¡Œï¼Œè¯·é€šè¿‡çŠ¶æ€æ¥å£æŸ¥è¯¢è¿›åº¦"
    }
}
```

### 3.3 è·å–ä»»åŠ¡çŠ¶æ€

**ç«¯ç‚¹**: `GET /api/v0/data_pipeline/tasks/{task_id}`

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 200,
    "message": "è·å–ä»»åŠ¡çŠ¶æ€æˆåŠŸ",
    "data": {
        "task_id": "task_20250627_143052",
        "task_name": "é«˜é€Ÿå…¬è·¯æ•°æ®å¤„ç†",
        "status": "in_progress",
        "step_status": {
            "ddl_generation": "completed",
            "qa_generation": "running",
            "sql_validation": "pending",
            "training_load": "pending"
        },
        "created_at": "2025-06-27T14:30:52",
        "started_at": "2025-06-27T14:31:00",
        "completed_at": null,
        "parameters": {
            "business_context": "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ",
            "enable_sql_validation": true,
            "backup_vector_tables": false,
            "truncate_vector_tables": false
        },
        "current_step": {
            "execution_id": "task_20250627_143052_step_qa_generation_exec_20250627143521",
            "step": "qa_generation",
            "status": "running",
            "started_at": "2025-06-27T14:35:21"
        },
        "total_steps": 4,
        "steps": [
            {
                "step_name": "ddl_generation",
                "step_status": "completed",
                "started_at": "2025-06-27T14:30:53",
                "completed_at": "2025-06-27T14:35:20",
                "error_message": null
            }
        ]
    }
}
```

#### ä»»åŠ¡çŠ¶æ€è¯´æ˜

| çŠ¶æ€ | è¯´æ˜ |
|------|------|
| `pending` | å¾…æ‰§è¡Œ |
| `in_progress` | æ‰§è¡Œä¸­ |
| `completed` | å·²å®Œæˆ |
| `failed` | æ‰§è¡Œå¤±è´¥ |
| `cancelled` | å·²å–æ¶ˆ |

### 3.4 è·å–ä»»åŠ¡åˆ—è¡¨

**ç«¯ç‚¹**: `GET /api/v0/data_pipeline/tasks`

#### æŸ¥è¯¢å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `limit` | int | `50` | è¿”å›æ•°é‡é™åˆ¶ |
| `offset` | int | `0` | åç§»é‡ |
| `status` | string | - | çŠ¶æ€è¿‡æ»¤ |

#### è¯·æ±‚ç¤ºä¾‹

```bash
curl "http://localhost:8084/api/v0/data_pipeline/tasks?limit=20&status=completed"
```

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 200,
    "message": "è·å–ä»»åŠ¡åˆ—è¡¨æˆåŠŸ",
    "data": {
        "tasks": [
            {
                "task_id": "task_20250627_143052",
                "task_name": "é«˜é€Ÿå…¬è·¯æ•°æ®å¤„ç†",
                "status": "completed",
                "step_status": {
                    "ddl_generation": "completed",
                    "qa_generation": "completed",
                    "sql_validation": "completed",
                    "training_load": "completed"
                },
                "created_at": "2025-06-27T14:30:52",
                "started_at": "2025-06-27T14:31:00",
                "completed_at": "2025-06-27T14:45:30",
                "created_by": "user123",
                "db_name": "highway_db",
                "business_context": "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ",
                "directory_exists": true,
                "updated_at": "2025-06-27T14:45:30"
            }
        ],
        "total": 1,
        "limit": 20,
        "offset": 0
    }
}
```

### 3.5 é«˜çº§ä»»åŠ¡æŸ¥è¯¢

**ç«¯ç‚¹**: `POST /api/v0/data_pipeline/tasks/query`

#### è¯·æ±‚å‚æ•°

| å‚æ•° | ç±»å‹ | è¯´æ˜ |
|------|------|------|
| `page` | int | é¡µç ï¼Œé»˜è®¤1 |
| `page_size` | int | æ¯é¡µå¤§å°ï¼Œé»˜è®¤20 |
| `status` | string | ä»»åŠ¡çŠ¶æ€ç­›é€‰ |
| `task_name` | string | ä»»åŠ¡åç§°æ¨¡ç³Šæœç´¢ |
| `created_by` | string | åˆ›å»ºè€…ç²¾ç¡®åŒ¹é… |
| `db_name` | string | æ•°æ®åº“åç§°ç²¾ç¡®åŒ¹é… |
| `created_time_start` | string | åˆ›å»ºæ—¶é—´èŒƒå›´å¼€å§‹ |
| `created_time_end` | string | åˆ›å»ºæ—¶é—´èŒƒå›´ç»“æŸ |

#### è¯·æ±‚ç¤ºä¾‹

```bash
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/query \
  -H "Content-Type: application/json" \
  -d '{
    "page": 1,
    "page_size": 10,
    "status": "completed",
    "task_name": "highway",
    "created_time_start": "2025-06-01T00:00:00",
    "created_time_end": "2025-06-30T23:59:59"
  }'
```

## 4. æ–‡ä»¶ç®¡ç†API

### 4.1 è·å–ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨

**ç«¯ç‚¹**: `GET /api/v0/data_pipeline/tasks/{task_id}/files`

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 200,
    "message": "è·å–æ–‡ä»¶åˆ—è¡¨æˆåŠŸ",
    "data": {
        "task_id": "task_20250627_143052",
        "files": [
            {
                "file_name": "qs_highway_db_20250627_143052_pair.json",
                "file_type": "json",
                "file_size": 102400,
                "file_size_human": "100.0 KB",
                "created_at": "2025-06-27T14:40:30",
                "is_primary": true,
                "description": "Question-SQLè®­ç»ƒæ•°æ®å¯¹",
                "download_url": "/api/v0/data_pipeline/tasks/task_20250627_143052/files/qs_highway_db_20250627_143052_pair.json"
            },
            {
                "file_name": "data_pipeline.log",
                "file_type": "log",
                "file_size": 51200,
                "file_size_human": "50.0 KB",
                "created_at": "2025-06-27T14:30:52",
                "is_primary": false,
                "description": "ä»»åŠ¡æ‰§è¡Œæ—¥å¿—",
                "download_url": "/api/v0/data_pipeline/tasks/task_20250627_143052/files/data_pipeline.log"
            }
        ],
        "total_files": 2,
        "total_size": 153600,
        "total_size_human": "150.0 KB"
    }
}
```

### 4.2 ä¸‹è½½æ–‡ä»¶

**ç«¯ç‚¹**: `GET /api/v0/data_pipeline/tasks/{task_id}/files/{filename}`

#### è¯·æ±‚ç¤ºä¾‹

```bash
curl -O http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/files/qs_highway_db_20250627_143052_pair.json
```

#### å“åº”
- æˆåŠŸæ—¶è¿”å›æ–‡ä»¶å†…å®¹ï¼Œè®¾ç½®é€‚å½“çš„Content-Type
- å¤±è´¥æ—¶è¿”å›JSONé”™è¯¯ä¿¡æ¯

### 4.3 ä¸Šä¼ è¡¨æ¸…å•æ–‡ä»¶

**ç«¯ç‚¹**: `POST /api/v0/data_pipeline/tasks/{task_id}/upload-table-list`

#### è¯·æ±‚å‚æ•°
- ä½¿ç”¨multipart/form-dataæ ¼å¼
- æ–‡ä»¶å­—æ®µå: `file`
- æ”¯æŒçš„æ–‡ä»¶ç±»å‹: `.txt`

#### è¯·æ±‚ç¤ºä¾‹

```bash
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/upload-table-list \
  -F "file=@tables.txt"
```

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 200,
    "message": "è¡¨æ¸…å•æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
    "data": {
        "task_id": "task_20250627_143052",
        "file_name": "table_list.txt",
        "file_size": 1024,
        "file_path": "./data_pipeline/training_data/task_20250627_143052/table_list.txt",
        "table_count": 8,
        "tables": [
            "bss_business_day_data",
            "bss_car_day_count",
            "bss_company"
        ]
    }
}
```

### 4.4 è·å–è¡¨æ¸…å•ä¿¡æ¯

**ç«¯ç‚¹**: `GET /api/v0/data_pipeline/tasks/{task_id}/table-list-info`

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 200,
    "message": "è·å–è¡¨æ¸…å•ä¿¡æ¯æˆåŠŸ",
    "data": {
        "task_id": "task_20250627_143052",
        "has_table_list": true,
        "file_name": "table_list.txt",
        "file_size": 1024,
        "file_path": "./data_pipeline/training_data/task_20250627_143052/table_list.txt",
        "table_count": 8,
        "tables": [
            "bss_business_day_data",
            "bss_car_day_count"
        ],
        "uploaded_at": "2025-06-27T14:32:15"
    }
}
```

### 4.5 ç›´æ¥åˆ›å»ºè¡¨æ¸…å•

**ç«¯ç‚¹**: `POST /api/v0/data_pipeline/tasks/{task_id}/table-list`

#### è¯·æ±‚å‚æ•°

| å‚æ•° | å¿…éœ€ | ç±»å‹ | è¯´æ˜ |
|------|------|------|------|
| `table_names` | âœ… | array | è¡¨ååˆ—è¡¨ |
| `business_context` | âŒ | string | ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿° |

#### è¯·æ±‚ç¤ºä¾‹

```bash
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/table-list \
  -H "Content-Type: application/json" \
  -d '{
    "table_names": [
      "bss_business_day_data",
      "bss_car_day_count",
      "bss_company"
    ],
    "business_context": "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ"
  }'
```

## 5. æ—¥å¿—ç®¡ç†API

### 5.1 è·å–ä»»åŠ¡æ—¥å¿—

**ç«¯ç‚¹**: `GET /api/v0/data_pipeline/tasks/{task_id}/logs`

#### æŸ¥è¯¢å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `limit` | int | `100` | æ—¥å¿—è¡Œæ•°é™åˆ¶ |
| `level` | string | - | æ—¥å¿—çº§åˆ«è¿‡æ»¤ |

#### è¯·æ±‚ç¤ºä¾‹

```bash
curl "http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/logs?limit=50&level=ERROR"
```

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 200,
    "message": "è·å–ä»»åŠ¡æ—¥å¿—æˆåŠŸ",
    "data": {
        "task_id": "task_20250627_143052",
        "logs": [
            {
                "timestamp": "2025-06-27 14:30:52",
                "level": "INFO",
                "logger": "SchemaWorkflowOrchestrator",
                "message": "ğŸš€ å¼€å§‹æ‰§è¡ŒSchemaå·¥ä½œæµç¼–æ’"
            },
            {
                "timestamp": "2025-06-27 14:30:53",
                "level": "INFO",
                "logger": "DDLMDGenerator",
                "message": "å¼€å§‹å¤„ç†è¡¨: bss_business_day_data"
            },
            {
                "timestamp": "2025-06-27 14:31:25",
                "level": "ERROR",
                "logger": "SQLValidator",
                "message": "SQLéªŒè¯å¤±è´¥ï¼Œå°è¯•LLMä¿®å¤"
            }
        ],
        "total": 3,
        "source": "file",
        "log_file": "/path/to/data_pipeline/training_data/task_20250627_143052/data_pipeline.log"
    }
}
```

### 5.2 é«˜çº§æ—¥å¿—æŸ¥è¯¢

**ç«¯ç‚¹**: `POST /api/v0/data_pipeline/tasks/{task_id}/logs/query`

#### è¯·æ±‚å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `page` | int | `1` | é¡µç  |
| `page_size` | int | `50` | æ¯é¡µå¤§å° |
| `level` | string | - | æ—¥å¿—çº§åˆ«è¿‡æ»¤ |
| `start_time` | string | - | å¼€å§‹æ—¶é—´ |
| `end_time` | string | - | ç»“æŸæ—¶é—´ |
| `keyword` | string | - | å…³é”®è¯æœç´¢ |
| `logger_name` | string | - | è®°å½•å™¨åç§°è¿‡æ»¤ |
| `step_name` | string | - | æ­¥éª¤åç§°è¿‡æ»¤ |
| `sort_by` | string | `timestamp` | æ’åºå­—æ®µ |
| `sort_order` | string | `desc` | æ’åºé¡ºåº |

#### è¯·æ±‚ç¤ºä¾‹

```bash
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/logs/query \
  -H "Content-Type: application/json" \
  -d '{
    "page": 1,
    "page_size": 20,
    "level": "ERROR",
    "keyword": "SQLéªŒè¯",
    "start_time": "2025-06-27T14:30:00",
    "end_time": "2025-06-27T14:45:00"
  }'
```

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 200,
    "message": "é«˜çº§æ—¥å¿—æŸ¥è¯¢æˆåŠŸ",
    "data": {
        "logs": [
            {
                "timestamp": "2025-06-27 14:31:25",
                "level": "ERROR",
                "logger": "SQLValidator",
                "step": "sql_validation",
                "message": "SQLéªŒè¯å¤±è´¥ï¼Œå°è¯•LLMä¿®å¤",
                "line_number": 125
            }
        ],
        "pagination": {
            "page": 1,
            "page_size": 20,
            "total": 1,
            "total_pages": 1,
            "has_next": false,
            "has_prev": false
        },
        "log_file_info": {
            "exists": true,
            "file_path": "/path/to/data_pipeline/training_data/task_20250627_143052/data_pipeline.log",
            "file_size": 51200,
            "last_modified": "2025-06-27T14:45:30"
        },
        "query_time": "0.023s"
    }
}
```

## 6. æ•°æ®åº“å·¥å…·API

### 6.1 è·å–æ•°æ®åº“è¡¨åˆ—è¡¨

**ç«¯ç‚¹**: `POST /api/v0/database/tables`

#### è¯·æ±‚å‚æ•°

| å‚æ•° | å¿…éœ€ | ç±»å‹ | è¯´æ˜ |
|------|------|------|------|
| `db_connection` | âŒ | string | æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ï¼Œä¸ä¼ åˆ™ä½¿ç”¨é»˜è®¤é…ç½® |
| `schema` | âŒ | string | Schemaåç§°ï¼Œæ”¯æŒå¤šä¸ªç”¨é€—å·åˆ†éš” |
| `table_name_pattern` | âŒ | string | è¡¨åæ¨¡å¼åŒ¹é…ï¼Œæ”¯æŒé€šé…ç¬¦ |

#### è¯·æ±‚ç¤ºä¾‹

```bash
curl -X POST http://localhost:8084/api/v0/database/tables \
  -H "Content-Type: application/json" \
  -d '{
    "db_connection": "postgresql://postgres:postgres@192.168.67.1:5432/highway_db",
    "schema": "public,ods",
    "table_name_pattern": "bss_*"
  }'
```

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 200,
    "message": "è·å–è¡¨åˆ—è¡¨æˆåŠŸ",
    "data": {
        "tables": [
            "public.bss_business_day_data",
            "public.bss_car_day_count",
            "ods.bss_company"
        ],
        "total": 3,
        "schemas": ["public", "ods"],
        "table_name_pattern": "bss_*",
        "db_connection_info": {
            "database": "highway_db"
        }
    }
}
```

### 6.2 è·å–è¡¨DDL/MDæ–‡æ¡£

**ç«¯ç‚¹**: `POST /api/v0/database/table/ddl`

#### è¯·æ±‚å‚æ•°

| å‚æ•° | å¿…éœ€ | ç±»å‹ | è¯´æ˜ |
|------|------|------|------|
| `table` | âœ… | string | è¡¨åï¼ˆå¯åŒ…å«schemaï¼‰ |
| `db_connection` | âŒ | string | æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸² |
| `business_context` | âŒ | string | ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿° |
| `type` | âŒ | enum | è¾“å‡ºç±»å‹ï¼š`ddl`/`md`/`both` |

#### è¯·æ±‚ç¤ºä¾‹

```bash
curl -X POST http://localhost:8084/api/v0/database/table/ddl \
  -H "Content-Type: application/json" \
  -d '{
    "table": "public.bss_company",
    "business_context": "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ",
    "type": "both"
  }'
```

#### å“åº”ç¤ºä¾‹

```json
{
    "success": true,
    "code": 200,
    "message": "è·å–è¡¨DDLæˆåŠŸ",
    "data": {
        "ddl": "-- ä¸­æ–‡å: å­˜å‚¨é«˜é€Ÿå…¬è·¯ç®¡ç†å…¬å¸ä¿¡æ¯\ncreate table public.bss_company (\n  id varchar(32) not null,\n  company_name varchar(255)\n);",
        "md": "## bss_companyï¼ˆå­˜å‚¨é«˜é€Ÿå…¬è·¯ç®¡ç†å…¬å¸ä¿¡æ¯ï¼‰\n\nå­—æ®µåˆ—è¡¨ï¼š\n- id (varchar(32)) - ä¸»é”®ID\n- company_name (varchar(255)) - å…¬å¸åç§°",
        "table_info": {
            "table_name": "bss_company",
            "schema_name": "public",
            "full_name": "public.bss_company",
            "comment": "å­˜å‚¨é«˜é€Ÿå…¬è·¯ç®¡ç†å…¬å¸ä¿¡æ¯",
            "field_count": 2,
            "row_count": 15
        },
        "fields": [
            {
                "column_name": "id",
                "data_type": "varchar",
                "max_length": 32,
                "is_nullable": false,
                "is_primary_key": true,
                "comment": "ä¸»é”®ID"
            }
        ],
        "generation_info": {
            "business_context": "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ",
            "output_type": "both",
            "has_llm_comments": true,
            "database": "highway_db"
        }
    }
}
```

## 7. æ—¥å¿—é…ç½®

### 7.1 APIæ—¥å¿—æ¶æ„

APIæ¨¡å¼ä¸‹çš„æ—¥å¿—ç³»ç»Ÿé‡‡ç”¨åŒæ—¥å¿—æ¶æ„ï¼š

1. **ä»»åŠ¡ç›®å½•æ—¥å¿—**: æ¯ä¸ªä»»åŠ¡åœ¨ç‹¬ç«‹ç›®å½•ä¸‹ç”Ÿæˆè¯¦ç»†æ—¥å¿—
2. **APIç³»ç»Ÿæ—¥å¿—**: è®°å½•APIè°ƒç”¨å’Œç³»ç»Ÿäº‹ä»¶

### 7.2 æ—¥å¿—æ–‡ä»¶ä½ç½®

#### ä»»åŠ¡æ—¥å¿—
```
data_pipeline/training_data/task_20250627_143052/
â””â”€â”€ data_pipeline.log                    # ä»»åŠ¡è¯¦ç»†æ‰§è¡Œæ—¥å¿—
```

#### APIç³»ç»Ÿæ—¥å¿—
```
logs/
â”œâ”€â”€ app.log                              # Flaskåº”ç”¨æ—¥å¿—
â”œâ”€â”€ agent.log                            # Agentç›¸å…³æ—¥å¿—
â”œâ”€â”€ vanna.log                            # Vannaç³»ç»Ÿæ—¥å¿—
â””â”€â”€ data_pipeline.log                    # data_pipelineæ¨¡å—æ—¥å¿—ï¼ˆå·²å¼ƒç”¨ï¼‰
```

### 7.3 æ—¥å¿—çº§åˆ«é…ç½®

#### 7.3.1 ç¯å¢ƒå˜é‡é…ç½®

```bash
# è®¾ç½®æ—¥å¿—çº§åˆ«
export DATA_PIPELINE_LOG_LEVEL=DEBUG
export FLASK_LOG_LEVEL=INFO

# è®¾ç½®æ—¥å¿—ç›®å½•
export DATA_PIPELINE_LOG_DIR=./logs/data_pipeline/
```

#### 7.3.2 ä»»åŠ¡æ—¥å¿—é…ç½®

ä»»åŠ¡æ‰§è¡Œæ—¶è‡ªåŠ¨é…ç½®ï¼š

```python
# åœ¨SimpleWorkflowExecutorä¸­è‡ªåŠ¨è®¾ç½®
def _setup_task_directory_logger(self):
    task_dir = self.file_manager.get_task_directory(self.task_id)
    log_file = task_dir / "data_pipeline.log"
    
    # åˆ›å»ºä»»åŠ¡ä¸“ç”¨logger
    self.task_dir_logger = logging.getLogger(f"TaskDir_{self.task_id}")
    # ... é…ç½®è¯¦ç»†æ ¼å¼å’Œå¤„ç†å™¨
```

### 7.4 æ—¥å¿—æ ¼å¼

#### ä»»åŠ¡æ—¥å¿—æ ¼å¼
```
2025-07-01 14:30:52 [INFO] TaskDir_task_20250627_143052: ä»»åŠ¡ç›®å½•æ—¥å¿—åˆå§‹åŒ–å®Œæˆ
2025-07-01 14:30:53 [INFO] SchemaWorkflowOrchestrator: ğŸš€ å¼€å§‹æ‰§è¡ŒSchemaå·¥ä½œæµç¼–æ’
2025-07-01 14:30:54 [INFO] DDLMDGenerator: å¼€å§‹å¤„ç†è¡¨: bss_business_day_data
```

#### APIæ—¥å¿—æ ¼å¼
```
2025-07-01 14:30:50 [INFO] unified_api: POST /api/v0/data_pipeline/tasks - ä»»åŠ¡åˆ›å»ºæˆåŠŸ
2025-07-01 14:30:51 [INFO] SimpleWorkflowManager: åˆ›å»ºä»»åŠ¡: task_20250627_143052
2025-07-01 14:30:52 [INFO] unified_api: POST /api/v0/data_pipeline/tasks/task_20250627_143052/execute - ä»»åŠ¡æ‰§è¡Œå¯åŠ¨
```

### 7.5 æ—¥å¿—ç›‘æ§å’ŒæŸ¥è¯¢

#### é€šè¿‡APIæŸ¥è¯¢æ—¥å¿—
```bash
# è·å–æœ€æ–°æ—¥å¿—
curl "http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/logs?limit=100"

# æŸ¥è¯¢é”™è¯¯æ—¥å¿—
curl "http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/logs?level=ERROR"

# é«˜çº§æ—¥å¿—æŸ¥è¯¢
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/logs/query \
  -H "Content-Type: application/json" \
  -d '{
    "page": 1,
    "page_size": 50,
    "level": "ERROR",
    "keyword": "SQLéªŒè¯"
  }'
```

#### ç›´æ¥æŸ¥çœ‹æ—¥å¿—æ–‡ä»¶
```bash
# æŸ¥çœ‹æœ€æ–°ä»»åŠ¡æ—¥å¿—
tail -f data_pipeline/training_data/task_*/data_pipeline.log

# æœç´¢é”™è¯¯æ—¥å¿—
grep -i "error" data_pipeline/training_data/task_*/data_pipeline.log
```

## 8. ä½¿ç”¨ç¤ºä¾‹

### 8.1 å®Œæ•´å·¥ä½œæµç¤ºä¾‹

#### æ­¥éª¤1: åˆ›å»ºä»»åŠ¡
```bash
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "table_list_file": "tables.txt",
    "business_context": "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ",
    "db_name": "highway_db",
    "task_name": "é«˜é€Ÿå…¬è·¯æ•°æ®å¤„ç†",
    "backup_vector_tables": false,
    "truncate_vector_tables": false
  }'
```

#### æ­¥éª¤2: æ‰§è¡Œå®Œæ•´å·¥ä½œæµ
```bash
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/execute \
  -H "Content-Type: application/json" \
  -d '{
    "execution_mode": "complete",
    "backup_vector_tables": false,
    "truncate_vector_tables": false
  }'
```

#### æ­¥éª¤3: è½®è¯¢ä»»åŠ¡çŠ¶æ€
```bash
#!/bin/bash
TASK_ID="task_20250627_143052"

while true; do
    STATUS=$(curl -s "http://localhost:8084/api/v0/data_pipeline/tasks/$TASK_ID" | jq -r '.data.status')
    echo "ä»»åŠ¡çŠ¶æ€: $STATUS"
    
    if [[ "$STATUS" == "completed" || "$STATUS" == "failed" ]]; then
        break
    fi
    
    sleep 10
done
```

#### æ­¥éª¤4: è·å–ç»“æœæ–‡ä»¶
```bash
# è·å–æ–‡ä»¶åˆ—è¡¨
curl "http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/files"

# ä¸‹è½½ä¸»è¦è¾“å‡ºæ–‡ä»¶
curl -O "http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/files/qs_highway_db_20250627_143052_pair.json"
```

### 8.2 Vectorè¡¨ç®¡ç†ç¤ºä¾‹

#### å¸¦Vectorè¡¨ç®¡ç†çš„å®Œæ•´å·¥ä½œæµ
```bash
# åˆ›å»ºä»»åŠ¡å¹¶å¯ç”¨vectorè¡¨æ¸…ç©º
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "table_list_file": "tables.txt",
    "business_context": "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ",
    "db_name": "highway_db",
    "task_name": "é«˜é€Ÿå…¬è·¯æ•°æ®å¤„ç†_æ¸…ç©ºvector",
    "truncate_vector_tables": true
  }'

# æ‰§è¡Œå·¥ä½œæµï¼ˆtruncate_vector_tablesä¼šè‡ªåŠ¨å¯ç”¨backup_vector_tablesï¼‰
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/execute \
  -H "Content-Type: application/json" \
  -d '{
    "execution_mode": "complete",
    "truncate_vector_tables": true
  }'

# æ£€æŸ¥vectorè¡¨ç®¡ç†ç»“æœ
curl "http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052"

# ä¸‹è½½å¤‡ä»½æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
curl "http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/files" | \
  jq -r '.data.files[] | select(.file_name | contains("langchain_")) | .download_url'
```

#### ä»…å¤‡ä»½Vectorè¡¨
```bash
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/execute \
  -H "Content-Type: application/json" \
  -d '{
    "execution_mode": "complete",
    "backup_vector_tables": true,
    "truncate_vector_tables": false
  }'
```

### 8.3 åˆ†æ­¥æ‰§è¡Œç¤ºä¾‹

#### æ­¥éª¤1: åˆ›å»ºä»»åŠ¡ï¼ˆæ— è¡¨æ¸…å•ï¼‰
```bash
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks \
  -H "Content-Type: application/json" \
  -d '{
    "business_context": "æµ‹è¯•ç³»ç»Ÿ",
    "db_name": "test_db",
    "task_name": "æµ‹è¯•ä»»åŠ¡"
  }'
```

#### æ­¥éª¤2: ä¸Šä¼ è¡¨æ¸…å•
```bash
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/upload-table-list \
  -F "file=@tables.txt"
```

#### æ­¥éª¤3: åˆ†æ­¥æ‰§è¡Œ
```bash
# æ‰§è¡ŒDDLç”Ÿæˆ
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/execute \
  -H "Content-Type: application/json" \
  -d '{
    "execution_mode": "step",
    "step_name": "ddl_generation"
  }'

# ç­‰å¾…å®Œæˆåæ‰§è¡ŒQ&Aç”Ÿæˆ
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/execute \
  -H "Content-Type: application/json" \
  -d '{
    "execution_mode": "step",
    "step_name": "qa_generation"
  }'
```

### 8.4 æ•°æ®åº“å·¥å…·ä½¿ç”¨ç¤ºä¾‹

#### è·å–æ•°æ®åº“è¡¨åˆ—è¡¨
```bash
curl -X POST http://localhost:8084/api/v0/database/tables \
  -H "Content-Type: application/json" \
  -d '{
    "schema": "public",
    "table_name_pattern": "bss_*"
  }'
```

#### è·å–å•è¡¨DDL
```bash
curl -X POST http://localhost:8084/api/v0/database/table/ddl \
  -H "Content-Type: application/json" \
  -d '{
    "table": "public.bss_company",
    "business_context": "é«˜é€Ÿå…¬è·¯ç®¡ç†ç³»ç»Ÿ",
    "type": "ddl"
  }'
```

### 8.5 JavaScriptå®¢æˆ·ç«¯ç¤ºä¾‹

```javascript
class DataPipelineAPI {
    constructor(baseUrl = 'http://localhost:8084') {
        this.baseUrl = baseUrl;
    }

    async createTask(params) {
        const response = await fetch(`${this.baseUrl}/api/v0/data_pipeline/tasks`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(params)
        });
        return await response.json();
    }

    async executeTask(taskId, executionMode = 'complete', stepName = null) {
        const params = { execution_mode: executionMode };
        if (stepName) params.step_name = stepName;

        const response = await fetch(`${this.baseUrl}/api/v0/data_pipeline/tasks/${taskId}/execute`, {
            method: 'POST',
            headers: {
                'Content-Type': 'application/json'
            },
            body: JSON.stringify(params)
        });
        return await response.json();
    }

    async getTaskStatus(taskId) {
        const response = await fetch(`${this.baseUrl}/api/v0/data_pipeline/tasks/${taskId}`);
        return await response.json();
    }

    async pollTaskStatus(taskId, interval = 5000) {
        return new Promise((resolve, reject) => {
            const poll = async () => {
                try {
                    const result = await this.getTaskStatus(taskId);
                    const status = result.data?.status;
                    
                    console.log(`ä»»åŠ¡çŠ¶æ€: ${status}`);
                    
                    if (status === 'completed') {
                        resolve(result);
                    } else if (status === 'failed') {
                        reject(new Error('ä»»åŠ¡æ‰§è¡Œå¤±è´¥'));
                    } else {
                        setTimeout(poll, interval);
                    }
                } catch (error) {
                    reject(error);
                }
            };
            poll();
        });
    }

    async getTaskFiles(taskId) {
        const response = await fetch(`${this.baseUrl}/api/v0/data_pipeline/tasks/${taskId}/files`);
        return await response.json();
    }

    async getTaskLogs(taskId, limit = 100, level = null) {
        const params = new URLSearchParams({ limit });
        if (level) params.append('level', level);
        
        const response = await fetch(`${this.baseUrl}/api/v0/data_pipeline/tasks/${taskId}/logs?${params}`);
        return await response.json();
    }
}

// ä½¿ç”¨ç¤ºä¾‹
async function runDataPipelineWorkflow() {
    const api = new DataPipelineAPI();
    
    try {
        // 1. åˆ›å»ºä»»åŠ¡
        const createResult = await api.createTask({
            table_list_file: 'tables.txt',
            business_context: 'é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ',
            db_name: 'highway_db',
            task_name: 'é«˜é€Ÿå…¬è·¯æ•°æ®å¤„ç†',
            backup_vector_tables: false,
            truncate_vector_tables: false
        });
        
        const taskId = createResult.data.task_id;
        console.log(`ä»»åŠ¡åˆ›å»ºæˆåŠŸ: ${taskId}`);
        
        // 2. æ‰§è¡Œä»»åŠ¡
        await api.executeTask(taskId, 'complete');
        console.log('ä»»åŠ¡æ‰§è¡Œå·²å¯åŠ¨');
        
        // 3. è½®è¯¢çŠ¶æ€
        const finalResult = await api.pollTaskStatus(taskId);
        console.log('ä»»åŠ¡æ‰§è¡Œå®Œæˆ:', finalResult);
        
        // 4. è·å–ç»“æœæ–‡ä»¶
        const files = await api.getTaskFiles(taskId);
        console.log('ç”Ÿæˆçš„æ–‡ä»¶:', files.data.files);
        
    } catch (error) {
        console.error('å·¥ä½œæµæ‰§è¡Œå¤±è´¥:', error);
    }
}
```

### 8.6 Pythonå®¢æˆ·ç«¯ç¤ºä¾‹

```python
import requests
import time
import json

class DataPipelineAPI:
    def __init__(self, base_url='http://localhost:8084'):
        self.base_url = base_url
        self.session = requests.Session()
        self.session.headers.update({
            'Content-Type': 'application/json'
        })

    def create_task(self, **params):
        """åˆ›å»ºä»»åŠ¡"""
        response = self.session.post(
            f'{self.base_url}/api/v0/data_pipeline/tasks',
            json=params
        )
        response.raise_for_status()
        return response.json()

    def execute_task(self, task_id, execution_mode='complete', step_name=None):
        """æ‰§è¡Œä»»åŠ¡"""
        params = {'execution_mode': execution_mode}
        if step_name:
            params['step_name'] = step_name
        
        response = self.session.post(
            f'{self.base_url}/api/v0/data_pipeline/tasks/{task_id}/execute',
            json=params
        )
        response.raise_for_status()
        return response.json()

    def get_task_status(self, task_id):
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        response = self.session.get(
            f'{self.base_url}/api/v0/data_pipeline/tasks/{task_id}'
        )
        response.raise_for_status()
        return response.json()

    def poll_task_status(self, task_id, interval=5):
        """è½®è¯¢ä»»åŠ¡çŠ¶æ€ç›´åˆ°å®Œæˆ"""
        while True:
            result = self.get_task_status(task_id)
            status = result['data']['status']
            
            print(f"ä»»åŠ¡çŠ¶æ€: {status}")
            
            if status == 'completed':
                return result
            elif status == 'failed':
                raise Exception(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {result['data'].get('error_message', 'æœªçŸ¥é”™è¯¯')}")
            
            time.sleep(interval)

    def get_task_files(self, task_id):
        """è·å–ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨"""
        response = self.session.get(
            f'{self.base_url}/api/v0/data_pipeline/tasks/{task_id}/files'
        )
        response.raise_for_status()
        return response.json()

    def download_file(self, task_id, filename, save_path=None):
        """ä¸‹è½½æ–‡ä»¶"""
        response = self.session.get(
            f'{self.base_url}/api/v0/data_pipeline/tasks/{task_id}/files/{filename}'
        )
        response.raise_for_status()
        
        if save_path is None:
            save_path = filename
        
        with open(save_path, 'wb') as f:
            f.write(response.content)
        
        return save_path

    def get_task_logs(self, task_id, limit=100, level=None):
        """è·å–ä»»åŠ¡æ—¥å¿—"""
        params = {'limit': limit}
        if level:
            params['level'] = level
        
        response = self.session.get(
            f'{self.base_url}/api/v0/data_pipeline/tasks/{task_id}/logs',
            params=params
        )
        response.raise_for_status()
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
def run_data_pipeline_workflow():
    api = DataPipelineAPI()
    
    try:
        # 1. åˆ›å»ºä»»åŠ¡
        create_result = api.create_task(
            table_list_file='tables.txt',
            business_context='é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ',
            db_name='highway_db',
            task_name='é«˜é€Ÿå…¬è·¯æ•°æ®å¤„ç†',
            backup_vector_tables=False,
            truncate_vector_tables=False
        )
        
        task_id = create_result['data']['task_id']
        print(f"ä»»åŠ¡åˆ›å»ºæˆåŠŸ: {task_id}")
        
        # 2. æ‰§è¡Œä»»åŠ¡
        api.execute_task(task_id, 'complete')
        print("ä»»åŠ¡æ‰§è¡Œå·²å¯åŠ¨")
        
        # 3. è½®è¯¢çŠ¶æ€
        final_result = api.poll_task_status(task_id)
        print("ä»»åŠ¡æ‰§è¡Œå®Œæˆ:", json.dumps(final_result, indent=2, ensure_ascii=False))
        
        # 4. è·å–ç»“æœæ–‡ä»¶
        files = api.get_task_files(task_id)
        print("ç”Ÿæˆçš„æ–‡ä»¶:")
        for file_info in files['data']['files']:
            print(f"  - {file_info['file_name']} ({file_info['file_size_human']})")
        
        # 5. ä¸‹è½½ä¸»è¦è¾“å‡ºæ–‡ä»¶
        primary_files = [f for f in files['data']['files'] if f.get('is_primary')]
        if primary_files:
            filename = primary_files[0]['file_name']
            save_path = api.download_file(task_id, filename)
            print(f"ä¸»è¦è¾“å‡ºæ–‡ä»¶å·²ä¸‹è½½: {save_path}")
        
    except Exception as error:
        print(f"å·¥ä½œæµæ‰§è¡Œå¤±è´¥: {error}")

if __name__ == '__main__':
    run_data_pipeline_workflow()
```

## 9. é”™è¯¯å¤„ç†

### 9.1 å¸¸è§é”™è¯¯ç 

| é”™è¯¯ç  | HTTPçŠ¶æ€ | è¯´æ˜ | è§£å†³æ–¹æ¡ˆ |
|--------|----------|------|----------|
| `TASK_NOT_FOUND` | 404 | ä»»åŠ¡ä¸å­˜åœ¨ | æ£€æŸ¥task_idæ˜¯å¦æ­£ç¡® |
| `TASK_ALREADY_RUNNING` | 409 | ä»»åŠ¡å·²åœ¨æ‰§è¡Œ | ç­‰å¾…å½“å‰ä»»åŠ¡å®Œæˆ |
| `INVALID_EXECUTION_MODE` | 400 | æ— æ•ˆçš„æ‰§è¡Œæ¨¡å¼ | ä½¿ç”¨`complete`æˆ–`step` |
| `MISSING_STEP_NAME` | 400 | ç¼ºå°‘æ­¥éª¤åç§° | æ­¥éª¤æ¨¡å¼éœ€è¦æŒ‡å®šstep_name |
| `INVALID_STEP_NAME` | 400 | æ— æ•ˆçš„æ­¥éª¤åç§° | ä½¿ç”¨æœ‰æ•ˆçš„æ­¥éª¤åç§° |
| `FILE_NOT_FOUND` | 404 | æ–‡ä»¶ä¸å­˜åœ¨ | æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦æ­£ç¡® |
| `DATABASE_CONNECTION_ERROR` | 500 | æ•°æ®åº“è¿æ¥å¤±è´¥ | æ£€æŸ¥æ•°æ®åº“é…ç½® |
| `VECTOR_BACKUP_FAILED` | 500 | Vectorè¡¨å¤‡ä»½å¤±è´¥ | æ£€æŸ¥æ•°æ®åº“è¿æ¥å’Œç£ç›˜ç©ºé—´ |
| `VECTOR_TRUNCATE_FAILED` | 500 | Vectorè¡¨æ¸…ç©ºå¤±è´¥ | æ£€æŸ¥æ•°æ®åº“æƒé™ |

### 9.2 é”™è¯¯å“åº”æ ¼å¼

```json
{
    "success": false,
    "code": 400,
    "message": "è¯·æ±‚å‚æ•°é”™è¯¯",
    "error": {
        "type": "ValidationError",
        "details": "æ— æ•ˆçš„æ‰§è¡Œæ¨¡å¼ï¼Œå¿…é¡»æ˜¯ 'complete' æˆ– 'step'",
        "invalid_params": ["execution_mode"],
        "timestamp": "2025-06-27T14:30:52"
    }
}
```

### 9.3 é”™è¯¯å¤„ç†æœ€ä½³å®è·µ

#### å®¢æˆ·ç«¯é”™è¯¯å¤„ç†
```javascript
async function apiRequest(url, options = {}) {
    try {
        const response = await fetch(url, {
            headers: {
                'Content-Type': 'application/json',
                ...options.headers
            },
            ...options
        });
        
        const data = await response.json();
        
        if (!data.success) {
            throw new Error(`APIé”™è¯¯: ${data.message} (${data.code})`);
        }
        
        return data;
        
    } catch (error) {
        if (error instanceof TypeError && error.message.includes('fetch')) {
            throw new Error('ç½‘ç»œè¿æ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æœåŠ¡å™¨æ˜¯å¦æ­£å¸¸è¿è¡Œ');
        }
        throw error;
    }
}

// ä½¿ç”¨ç¤ºä¾‹
try {
    const result = await apiRequest('/api/v0/data_pipeline/tasks/invalid_id');
} catch (error) {
    if (error.message.includes('404')) {
        console.log('ä»»åŠ¡ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ä»»åŠ¡ID');
    } else {
        console.error('APIè°ƒç”¨å¤±è´¥:', error.message);
    }
}
```

#### é‡è¯•æœºåˆ¶
```python
import time
import requests
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

def create_session_with_retry():
    session = requests.Session()
    
    # é…ç½®é‡è¯•ç­–ç•¥
    retry_strategy = Retry(
        total=3,
        status_forcelist=[429, 500, 502, 503, 504],
        method_whitelist=["HEAD", "GET", "OPTIONS"],
        backoff_factor=1
    )
    
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    
    return session

# ä½¿ç”¨ç¤ºä¾‹
session = create_session_with_retry()
response = session.get('http://localhost:8084/api/v0/data_pipeline/tasks/task_id')
```

### 9.4 æ—¥å¿—é”™è¯¯æ’æŸ¥

#### æŸ¥çœ‹ä»»åŠ¡é”™è¯¯æ—¥å¿—
```bash
# è·å–ä»»åŠ¡é”™è¯¯æ—¥å¿—
curl "http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/logs?level=ERROR"

# æœç´¢ç‰¹å®šé”™è¯¯
curl -X POST http://localhost:8084/api/v0/data_pipeline/tasks/task_20250627_143052/logs/query \
  -H "Content-Type: application/json" \
  -d '{
    "keyword": "è¿æ¥å¤±è´¥",
    "level": "ERROR"
  }'
```

#### åˆ†æé”™è¯¯æ¨¡å¼
```python
def analyze_error_logs(api, task_id):
    """åˆ†æä»»åŠ¡é”™è¯¯æ—¥å¿—"""
    logs_result = api.get_task_logs(task_id, level='ERROR')
    error_logs = logs_result['data']['logs']
    
    # ç»Ÿè®¡é”™è¯¯ç±»å‹
    error_types = {}
    for log in error_logs:
        message = log['message']
        if 'SQLéªŒè¯å¤±è´¥' in message:
            error_types['SQL_VALIDATION'] = error_types.get('SQL_VALIDATION', 0) + 1
        elif 'æ•°æ®åº“è¿æ¥' in message:
            error_types['DATABASE_CONNECTION'] = error_types.get('DATABASE_CONNECTION', 0) + 1
        elif 'LLMè°ƒç”¨' in message:
            error_types['LLM_CALL'] = error_types.get('LLM_CALL', 0) + 1
        else:
            error_types['OTHER'] = error_types.get('OTHER', 0) + 1
    
    return error_types

# ä½¿ç”¨ç¤ºä¾‹
error_analysis = analyze_error_logs(api, 'task_20250627_143052')
print("é”™è¯¯ç±»å‹ç»Ÿè®¡:", error_analysis)
```

## 10. æœ€ä½³å®è·µ

### 10.1 ä»»åŠ¡ç®¡ç†æœ€ä½³å®è·µ

#### 1. ä»»åŠ¡å‘½åè§„èŒƒ
```python
# æ¨èçš„ä»»åŠ¡å‘½åæ ¼å¼
task_name_patterns = {
    "ç¯å¢ƒ_æ•°æ®åº“_ç”¨é€”_æ—¶é—´": "prod_highway_training_20250627",
    "é¡¹ç›®_æ¨¡å—_ç‰ˆæœ¬": "crm_customer_v1.2",
    "ä¸šåŠ¡_åœºæ™¯_æ‰¹æ¬¡": "ecommerce_recommendation_batch01"
}
```

#### 2. å‚æ•°é…ç½®å»ºè®®
```json
{
    "æ¨èé…ç½®": {
        "enable_sql_validation": true,
        "enable_llm_repair": true,
        "modify_original_file": true,
        "enable_training_data_load": true,
        "backup_vector_tables": false,
        "truncate_vector_tables": false
    },
    "è°ƒè¯•é…ç½®": {
        "enable_sql_validation": false,
        "enable_llm_repair": false,
        "modify_original_file": false,
        "enable_training_data_load": false,
        "backup_vector_tables": false,
        "truncate_vector_tables": false
    },
    "å¿«é€Ÿé…ç½®": {
        "enable_sql_validation": true,
        "enable_llm_repair": false,
        "modify_original_file": false,
        "enable_training_data_load": true,
        "backup_vector_tables": false,
        "truncate_vector_tables": false
    },
    "Vectoræ¸…ç†é…ç½®": {
        "enable_sql_validation": true,
        "enable_llm_repair": true,
        "modify_original_file": true,
        "enable_training_data_load": true,
        "backup_vector_tables": true,
        "truncate_vector_tables": true
    }
}
```

#### 3. ä»»åŠ¡ç›‘æ§ç­–ç•¥
```javascript
class TaskMonitor {
    constructor(api, taskId) {
        this.api = api;
        this.taskId = taskId;
        this.callbacks = {};
    }

    on(event, callback) {
        this.callbacks[event] = callback;
    }

    async start(interval = 5000) {
        while (true) {
            try {
                const result = await this.api.getTaskStatus(this.taskId);
                const status = result.data.status;
                
                // è§¦å‘çŠ¶æ€å˜åŒ–å›è°ƒ
                if (this.callbacks.statusChange) {
                    this.callbacks.statusChange(status, result.data);
                }
                
                // æ£€æŸ¥æ˜¯å¦å®Œæˆ
                if (status === 'completed') {
                    if (this.callbacks.completed) {
                        this.callbacks.completed(result.data);
                    }
                    break;
                } else if (status === 'failed') {
                    if (this.callbacks.failed) {
                        this.callbacks.failed(result.data);
                    }
                    break;
                }
                
                await new Promise(resolve => setTimeout(resolve, interval));
                
            } catch (error) {
                if (this.callbacks.error) {
                    this.callbacks.error(error);
                }
                break;
            }
        }
    }
}

// ä½¿ç”¨ç¤ºä¾‹
const monitor = new TaskMonitor(api, taskId);
monitor.on('statusChange', (status, data) => {
    console.log(`ä»»åŠ¡çŠ¶æ€æ›´æ–°: ${status}`);
    updateProgressBar(data.step_status);
});
monitor.on('completed', (data) => {
    console.log('ä»»åŠ¡å®Œæˆï¼Œå¼€å§‹ä¸‹è½½ç»“æœæ–‡ä»¶');
    downloadResultFiles(data);
});
monitor.on('failed', (data) => {
    console.error('ä»»åŠ¡å¤±è´¥:', data.error_message);
    showErrorDialog(data.error_message);
});
monitor.start();
```

### 10.2 æ€§èƒ½ä¼˜åŒ–å»ºè®®

#### 1. å¹¶å‘æ§åˆ¶
```python
import asyncio
import aiohttp

class AsyncDataPipelineAPI:
    def __init__(self, base_url='http://localhost:8084'):
        self.base_url = base_url
        self.session = None

    async def __aenter__(self):
        self.session = aiohttp.ClientSession()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def create_multiple_tasks(self, task_configs, max_concurrent=3):
        """å¹¶å‘åˆ›å»ºå¤šä¸ªä»»åŠ¡"""
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def create_single_task(config):
            async with semaphore:
                async with self.session.post(
                    f'{self.base_url}/api/v0/data_pipeline/tasks',
                    json=config
                ) as response:
                    return await response.json()
        
        tasks = [create_single_task(config) for config in task_configs]
        return await asyncio.gather(*tasks)

# ä½¿ç”¨ç¤ºä¾‹
async def batch_create_tasks():
    task_configs = [
        {
            'table_list_file': f'tables_{i}.txt',
            'business_context': f'ä¸šåŠ¡ç³»ç»Ÿ{i}',
            'db_name': f'db_{i}'
        }
        for i in range(1, 6)
    ]
    
    async with AsyncDataPipelineAPI() as api:
        results = await api.create_multiple_tasks(task_configs)
        print(f"åˆ›å»ºäº† {len(results)} ä¸ªä»»åŠ¡")
```

#### 2. ç¼“å­˜ç­–ç•¥
```python
import functools
import time

def cache_with_ttl(ttl_seconds=300):
    """å¸¦TTLçš„ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        cache = {}
        
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            key = str(args) + str(sorted(kwargs.items()))
            current_time = time.time()
            
            if key in cache:
                result, timestamp = cache[key]
                if current_time - timestamp < ttl_seconds:
                    return result
            
            result = func(*args, **kwargs)
            cache[key] = (result, current_time)
            return result
        
        return wrapper
    return decorator

class CachedDataPipelineAPI(DataPipelineAPI):
    @cache_with_ttl(60)  # ç¼“å­˜1åˆ†é’Ÿ
    def get_task_status(self, task_id):
        return super().get_task_status(task_id)
    
    @cache_with_ttl(300)  # ç¼“å­˜5åˆ†é’Ÿ
    def get_task_files(self, task_id):
        return super().get_task_files(task_id)
```

### 10.3 å®‰å…¨æœ€ä½³å®è·µ

#### 1. APIè®¤è¯
```python
class SecureDataPipelineAPI(DataPipelineAPI):
    def __init__(self, base_url, api_key=None):
        super().__init__(base_url)
        if api_key:
            self.session.headers.update({
                'Authorization': f'Bearer {api_key}'
            })

    def set_api_key(self, api_key):
        self.session.headers.update({
            'Authorization': f'Bearer {api_key}'
        })
```

#### 2. å‚æ•°éªŒè¯
```python
def validate_task_params(params):
    """éªŒè¯ä»»åŠ¡å‚æ•°"""
    required_fields = ['business_context', 'db_name']
    
    for field in required_fields:
        if field not in params or not params[field]:
            raise ValueError(f"ç¼ºå°‘å¿…éœ€å‚æ•°: {field}")
    
    # éªŒè¯æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²æ ¼å¼
    if 'db_connection' in params:
        db_conn = params['db_connection']
        if not db_conn.startswith('postgresql://'):
            raise ValueError("æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²å¿…é¡»ä»¥postgresql://å¼€å¤´")
    
    # éªŒè¯ä»»åŠ¡åç§°é•¿åº¦
    if 'task_name' in params and len(params['task_name']) > 100:
        raise ValueError("ä»»åŠ¡åç§°ä¸èƒ½è¶…è¿‡100ä¸ªå­—ç¬¦")
    
    return True

# ä½¿ç”¨ç¤ºä¾‹
try:
    validate_task_params({
        'business_context': 'é«˜é€Ÿå…¬è·¯ç®¡ç†ç³»ç»Ÿ',
        'db_name': 'highway_db'
    })
except ValueError as e:
    print(f"å‚æ•°éªŒè¯å¤±è´¥: {e}")
```

### 10.4 ç›‘æ§å’Œå‘Šè­¦

#### 1. ä»»åŠ¡çŠ¶æ€ç›‘æ§
```python
import logging
from datetime import datetime, timedelta

class TaskStatusMonitor:
    def __init__(self, api, alert_callback=None):
        self.api = api
        self.alert_callback = alert_callback
        self.logger = logging.getLogger(__name__)

    async def monitor_all_tasks(self):
        """ç›‘æ§æ‰€æœ‰æ´»è·ƒä»»åŠ¡"""
        tasks_result = self.api.get_tasks_list(status_filter='in_progress')
        active_tasks = tasks_result['data']['tasks']
        
        for task in active_tasks:
            await self.check_task_health(task)

    async def check_task_health(self, task):
        """æ£€æŸ¥å•ä¸ªä»»åŠ¡å¥åº·çŠ¶æ€"""
        task_id = task['task_id']
        started_at = datetime.fromisoformat(task['started_at'])
        current_time = datetime.now()
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦è¶…æ—¶ï¼ˆè¶…è¿‡2å°æ—¶ï¼‰
        if current_time - started_at > timedelta(hours=2):
            self.logger.warning(f"ä»»åŠ¡ {task_id} å¯èƒ½è¶…æ—¶")
            if self.alert_callback:
                self.alert_callback('TASK_TIMEOUT', task)
        
        # æ£€æŸ¥ä»»åŠ¡æ—¥å¿—æ˜¯å¦æœ‰é”™è¯¯
        logs_result = self.api.get_task_logs(task_id, level='ERROR')
        if logs_result['data']['total'] > 0:
            self.logger.error(f"ä»»åŠ¡ {task_id} å‘ç°é”™è¯¯æ—¥å¿—")
            if self.alert_callback:
                self.alert_callback('TASK_ERROR', task)

def alert_handler(alert_type, task):
    """å‘Šè­¦å¤„ç†å‡½æ•°"""
    if alert_type == 'TASK_TIMEOUT':
        print(f"âš ï¸ ä»»åŠ¡è¶…æ—¶å‘Šè­¦: {task['task_id']}")
        # å¯ä»¥é›†æˆé‚®ä»¶ã€é’‰é’‰ã€å¾®ä¿¡ç­‰é€šçŸ¥æ–¹å¼
    elif alert_type == 'TASK_ERROR':
        print(f"âŒ ä»»åŠ¡é”™è¯¯å‘Šè­¦: {task['task_id']}")

# ä½¿ç”¨ç¤ºä¾‹
monitor = TaskStatusMonitor(api, alert_handler)
asyncio.run(monitor.monitor_all_tasks())
```

#### 2. æ€§èƒ½æŒ‡æ ‡æ”¶é›†
```python
class PerformanceMetrics:
    def __init__(self):
        self.metrics = {}

    def record_api_call(self, endpoint, duration, status_code):
        """è®°å½•APIè°ƒç”¨æŒ‡æ ‡"""
        key = f"{endpoint}_{status_code}"
        if key not in self.metrics:
            self.metrics[key] = {
                'count': 0,
                'total_duration': 0,
                'avg_duration': 0,
                'max_duration': 0,
                'min_duration': float('inf')
            }
        
        metric = self.metrics[key]
        metric['count'] += 1
        metric['total_duration'] += duration
        metric['avg_duration'] = metric['total_duration'] / metric['count']
        metric['max_duration'] = max(metric['max_duration'], duration)
        metric['min_duration'] = min(metric['min_duration'], duration)

    def get_report(self):
        """è·å–æ€§èƒ½æŠ¥å‘Š"""
        return {
            'api_metrics': self.metrics,
            'summary': {
                'total_calls': sum(m['count'] for m in self.metrics.values()),
                'avg_response_time': sum(m['avg_duration'] for m in self.metrics.values()) / len(self.metrics) if self.metrics else 0
            }
        }

# é›†æˆåˆ°APIå®¢æˆ·ç«¯
class InstrumentedDataPipelineAPI(DataPipelineAPI):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.metrics = PerformanceMetrics()

    def _make_request(self, method, url, **kwargs):
        """å¸¦æ€§èƒ½ç›‘æ§çš„è¯·æ±‚æ–¹æ³•"""
        start_time = time.time()
        try:
            response = getattr(self.session, method)(url, **kwargs)
            duration = time.time() - start_time
            self.metrics.record_api_call(url, duration, response.status_code)
            return response
        except Exception as e:
            duration = time.time() - start_time
            self.metrics.record_api_call(url, duration, 0)
            raise
```

## æ€»ç»“

data_pipeline APIç³»ç»Ÿæä¾›äº†å®Œæ•´çš„RESTfulæ¥å£æ”¯æŒï¼Œèƒ½å¤Ÿæ»¡è¶³å„ç§æ•°æ®å¤„ç†éœ€æ±‚ã€‚é€šè¿‡åˆç†ä½¿ç”¨ä»»åŠ¡ç®¡ç†ã€æ–‡ä»¶ç®¡ç†ã€æ—¥å¿—ç®¡ç†å’Œæ•°æ®åº“å·¥å…·APIï¼Œå¯ä»¥æ„å»ºé«˜æ•ˆçš„æ•°æ®å¤„ç†å·¥ä½œæµã€‚å»ºè®®åœ¨ç”Ÿäº§ç¯å¢ƒä¸­å®æ–½é€‚å½“çš„ç›‘æ§ã€å‘Šè­¦å’Œæ€§èƒ½ä¼˜åŒ–æªæ–½ï¼Œç¡®ä¿ç³»ç»Ÿçš„ç¨³å®šæ€§å’Œå¯é æ€§ã€‚ 