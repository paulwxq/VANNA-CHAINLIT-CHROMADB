# Vectorè¡¨ç®¡ç†åŠŸèƒ½è®¾è®¡æ–‡æ¡£

## æ¦‚è¿°

ä¸º data_pipeline æ·»åŠ ä¸¤ä¸ªæ–°å‚æ•°æ¥ç®¡ç† vector è¡¨æ•°æ®ï¼š
- `--backup-vector-tables`: å¤‡ä»½vectorè¡¨æ•°æ®
- `--truncate-vector-tables`: æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰

## éœ€æ±‚åˆ†æ

### 1. å‚æ•°ä¾èµ–å…³ç³»
- å¯ä»¥å•ç‹¬ä½¿ç”¨ `--backup-vector-tables`
- ä¸å¯ä»¥å•ç‹¬ä½¿ç”¨ `--truncate-vector-tables`
- ä½¿ç”¨ `--truncate-vector-tables` æ—¶è‡ªåŠ¨å¯ç”¨ `--backup-vector-tables`

### 2. æ”¯æŒçš„æ‰§è¡Œå…¥å£
1. `python -m data_pipeline.schema_workflow`ï¼ˆåŒ…æ‹¬ä½¿ç”¨ `--skip-training-load` çš„æƒ…å†µï¼‰
2. `python -m data_pipeline.trainer.run_training`
3. API æ¥å£ï¼š`POST /api/v0/data_pipeline/tasks/{task_id}/execute`

### 3. ç‰¹æ®Šæ‰§è¡Œåœºæ™¯
- **è·³è¿‡è®­ç»ƒåŠ è½½åœºæ™¯**: å³ä½¿ `schema_workflow` ä½¿ç”¨äº† `--skip-training-load` å‚æ•°ï¼Œä»ç„¶è¦æ”¯æŒ `--backup-vector-tables` å’Œ `--truncate-vector-tables` å‚æ•°çš„æ‰§è¡Œ
- **é‡å¤æ‰§è¡Œé¿å…**: ç”±äº `schema_workflow` çš„å®Œæ•´æµç¨‹åŒ…å«äº† `run_training` çš„è°ƒç”¨ï¼Œéœ€è¦é¿å…é‡å¤æ‰§è¡Œvectorè¡¨ç®¡ç†æ“ä½œ

### 4. å½±å“çš„è¡¨
- `langchain_pg_collection`ï¼šåªå¤‡ä»½ï¼Œä¸æ¸…ç©º
- `langchain_pg_embedding`ï¼šå¤‡ä»½å¹¶æ¸…ç©º

## è¯¦ç»†è®¾è®¡

### 1. å‚æ•°å®šä¹‰å’Œä¼ é€’

#### 1.1 å‘½ä»¤è¡Œå‚æ•°
```bash
# schema_workflow.py æ–°å¢å‚æ•°
--backup-vector-tables      # å¤‡ä»½vectorè¡¨æ•°æ®
--truncate-vector-tables    # æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰

# run_training.py æ–°å¢å‚æ•°
--backup-vector-tables      # å¤‡ä»½vectorè¡¨æ•°æ®  
--truncate-vector-tables    # æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰
```

#### 1.2 å‚æ•°ä¼ é€’é“¾
```
CLIå‚æ•° -> SchemaWorkflowOrchestrator -> process_training_files -> VectorTableManager
```

### 2. æ ¸å¿ƒç»„ä»¶è®¾è®¡

#### 2.1 æ–°å¢ VectorTableManager ç±»
**ä½ç½®**: `data_pipeline/trainer/vector_table_manager.py`

```python
class VectorTableManager:
    """Vectorè¡¨ç®¡ç†å™¨ï¼Œè´Ÿè´£å¤‡ä»½å’Œæ¸…ç©ºæ“ä½œ"""
    
    def __init__(self, task_output_dir: str, task_id: str = None):
        """
        Args:
            task_output_dir: ä»»åŠ¡è¾“å‡ºç›®å½•ï¼ˆç”¨äºå­˜æ”¾å¤‡ä»½æ–‡ä»¶ï¼‰
            task_id: ä»»åŠ¡IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        Note:
            æ•°æ®åº“è¿æ¥å°†ä»data_pipeline.config.SCHEMA_TOOLS_CONFIGè‡ªåŠ¨è·å–
        """
        self.task_output_dir = task_output_dir
        self.task_id = task_id
        
        # ä»data_pipeline.configè·å–é…ç½®
        from data_pipeline.config import SCHEMA_TOOLS_CONFIG
        self.config = SCHEMA_TOOLS_CONFIG.get("vector_table_management", {})
        
        # åˆå§‹åŒ–æ—¥å¿—
        if task_id:
            from data_pipeline.dp_logging import get_logger
            self.logger = get_logger("VectorTableManager", task_id)
        else:
            import logging
            self.logger = logging.getLogger("VectorTableManager")
    
    async def backup_vector_tables(self) -> Dict[str, Any]:
        """å¤‡ä»½vectorè¡¨æ•°æ®"""
    
    async def truncate_vector_tables(self) -> Dict[str, Any]: 
        """æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆåªæ¸…ç©ºlangchain_pg_embeddingï¼‰"""
        
    async def execute_vector_management(self, backup: bool, truncate: bool) -> Dict[str, Any]:
        """æ‰§è¡Œvectorè¡¨ç®¡ç†æ“ä½œ"""
        
    def get_connection(self):
    """è·å–pgvectoræ•°æ®åº“è¿æ¥ï¼ˆä»data_pipeline.configè·å–é…ç½®ï¼‰"""
    
def _format_file_size(self, size_bytes: int) -> str:
    """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º"""
```

#### 2.2 ä¸»è¦æ‰§è¡Œæµç¨‹

```python
async def execute_vector_management(self, backup: bool, truncate: bool) -> Dict[str, Any]:
    """æ‰§è¡Œvectorè¡¨ç®¡ç†æ“ä½œçš„ä¸»æµç¨‹"""
    
    start_time = time.time()
    
    # 1. å‚æ•°éªŒè¯å’Œè‡ªåŠ¨å¯ç”¨é€»è¾‘
    if truncate and not backup:
        backup = True
        self.logger.info("ğŸ”„ å¯ç”¨truncateæ—¶è‡ªåŠ¨å¯ç”¨backup")
    
    if not backup and not truncate:
        self.logger.info("â­ï¸ æœªå¯ç”¨vectorè¡¨ç®¡ç†ï¼Œè·³è¿‡æ“ä½œ")
        return {"backup_performed": False, "truncate_performed": False}
    
    # 2. åˆå§‹åŒ–ç»“æœç»Ÿè®¡
    result = {
        "backup_performed": backup,
        "truncate_performed": truncate,
        "tables_backed_up": {},
        "truncate_results": {},
        "errors": [],
        "backup_directory": None,
        "duration": 0
    }
    
    try:
        # 3. åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_dir = Path(self.task_output_dir) / self.config["backup_directory"]
        if backup:
            backup_dir.mkdir(parents=True, exist_ok=True)
            result["backup_directory"] = str(backup_dir)
            self.logger.info(f"ğŸ“ å¤‡ä»½ç›®å½•: {backup_dir}")
        
        # 4. æ‰§è¡Œå¤‡ä»½æ“ä½œ
        if backup:
            self.logger.info("ğŸ—‚ï¸ å¼€å§‹å¤‡ä»½vectorè¡¨...")
            backup_results = await self.backup_vector_tables()
            result["tables_backed_up"] = backup_results
            
            # æ£€æŸ¥å¤‡ä»½æ˜¯å¦å…¨éƒ¨æˆåŠŸ
            backup_failed = any(not r.get("success", False) for r in backup_results.values())
            if backup_failed:
                result["errors"].append("éƒ¨åˆ†è¡¨å¤‡ä»½å¤±è´¥")
                if truncate:
                    self.logger.error("âŒ å¤‡ä»½å¤±è´¥ï¼Œå–æ¶ˆæ¸…ç©ºæ“ä½œ")
                    result["truncate_performed"] = False
                    truncate = False
        
        # 5. æ‰§è¡Œæ¸…ç©ºæ“ä½œï¼ˆä»…åœ¨å¤‡ä»½æˆåŠŸæ—¶ï¼‰
        if truncate:
            self.logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºvectorè¡¨...")
            truncate_results = await self.truncate_vector_tables()
            result["truncate_results"] = truncate_results
            
            # æ£€æŸ¥æ¸…ç©ºæ˜¯å¦æˆåŠŸ
            truncate_failed = any(not r.get("success", False) for r in truncate_results.values())
            if truncate_failed:
                result["errors"].append("éƒ¨åˆ†è¡¨æ¸…ç©ºå¤±è´¥")
        
        # 6. ç”Ÿæˆå¤‡ä»½æ—¥å¿—æ–‡ä»¶
        if backup and backup_dir.exists():
            self._write_backup_log(backup_dir, result)
        
        # 7. è®¡ç®—æ€»è€—æ—¶
        result["duration"] = time.time() - start_time
        
        # 8. è®°å½•æœ€ç»ˆçŠ¶æ€
        if result["errors"]:
            self.logger.warning(f"âš ï¸ Vectorè¡¨ç®¡ç†å®Œæˆï¼Œä½†æœ‰é”™è¯¯: {'; '.join(result['errors'])}")
        else:
            self.logger.info(f"âœ… Vectorè¡¨ç®¡ç†å®Œæˆï¼Œè€—æ—¶: {result['duration']:.2f}ç§’")
        
        return result
        
    except Exception as e:
        result["duration"] = time.time() - start_time
        result["errors"].append(f"æ‰§è¡Œå¤±è´¥: {str(e)}")
        self.logger.error(f"âŒ Vectorè¡¨ç®¡ç†å¤±è´¥: {e}")
        raise
```

#### 2.3 æ•°æ®åº“è¿æ¥ç®¡ç†

**é…ç½®è·å–å±‚æ¬¡**ï¼š
```
VectorTableManager
  â†“
data_pipeline.config.SCHEMA_TOOLS_CONFIG["default_db_connection"]
  â†“
app_config.PGVECTOR_CONFIG (åœ¨config.pyä¸­è‡ªåŠ¨ç»§æ‰¿)
```

```python
def get_connection(self):
    """è·å–pgvectoræ•°æ®åº“è¿æ¥"""
    import psycopg2
    
    try:
        # æ–¹æ³•1ï¼šå¦‚æœSCHEMA_TOOLS_CONFIGä¸­æœ‰è¿æ¥å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
        connection_string = self.config.get("default_db_connection")
        if connection_string:
            conn = psycopg2.connect(connection_string)
        else:
            # æ–¹æ³•2ï¼šä»app_configè·å–pgvectoræ•°æ®åº“é…ç½®
            import app_config
            pgvector_config = app_config.PGVECTOR_CONFIG
            conn = psycopg2.connect(
                host=pgvector_config.get('host'),
                port=pgvector_config.get('port'),
                database=pgvector_config.get('dbname'),
                user=pgvector_config.get('user'),
                password=pgvector_config.get('password')
            )
        
        # è®¾ç½®è‡ªåŠ¨æäº¤ï¼Œé¿å…äº‹åŠ¡é—®é¢˜
        conn.autocommit = True
        return conn
        
    except Exception as e:
        self.logger.error(f"pgvectoræ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        raise

def _write_backup_log(self, backup_dir: Path, result: Dict[str, Any]):
    """å†™å…¥è¯¦ç»†çš„å¤‡ä»½æ—¥å¿—"""
    log_file = backup_dir / "vector_backup_log.txt"
    
    try:
        with open(log_file, 'w', encoding='utf-8') as f:
            f.write("=== Vector Table Backup Log ===\n")
            f.write(f"Backup Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Task ID: {self.task_id or 'Unknown'}\n")
            f.write(f"Duration: {result.get('duration', 0):.2f}s\n\n")
            
            # å¤‡ä»½çŠ¶æ€
            f.write("Tables Backup Status:\n")
            for table_name, info in result.get("tables_backed_up", {}).items():
                if info.get("success", False):
                    f.write(f"âœ“ {table_name}: {info['row_count']} rows -> {info['backup_file']} ({info['file_size']})\n")
                else:
                    f.write(f"âœ— {table_name}: FAILED - {info.get('error', 'Unknown error')}\n")
            
            # æ¸…ç©ºçŠ¶æ€
            if result.get("truncate_performed", False):
                f.write("\nTruncate Status:\n")
                for table_name, info in result.get("truncate_results", {}).items():
                    if info.get("success", False):
                        f.write(f"âœ“ {table_name}: TRUNCATED ({info['rows_before']} rows removed)\n")
                    else:
                        f.write(f"âœ— {table_name}: FAILED - {info.get('error', 'Unknown error')}\n")
            else:
                f.write("\nTruncate Status:\n- Not performed\n")
            
            # é”™è¯¯æ±‡æ€»
            if result.get("errors"):
                f.write(f"\nErrors: {'; '.join(result['errors'])}\n")
                
    except Exception as e:
        self.logger.warning(f"å†™å…¥å¤‡ä»½æ—¥å¿—å¤±è´¥: {e}")
```

#### 2.4 å¤‡ä»½æ–‡ä»¶å‘½åè§„åˆ™
```
{task_output_dir}/vector_bak/langchain_pg_collection_{timestamp}.csv
{task_output_dir}/vector_bak/langchain_pg_embedding_{timestamp}.csv
```

æ—¶é—´æˆ³æ ¼å¼ï¼š`YYYYMMDD_HHMMSS`

### 3. æ‰§è¡Œæµç¨‹è®¾è®¡

#### 3.1 å®Œæ•´å·¥ä½œæµ (schema_workflow.py)
```
æ­¥éª¤1: DDL/MDç”Ÿæˆ
æ­¥éª¤2: Question-SQLç”Ÿæˆ  
æ­¥éª¤3: SQLéªŒè¯ï¼ˆå¯é€‰ï¼‰
æ­¥éª¤4: è®­ç»ƒæ•°æ®åŠ è½½
  â”œâ”€â”€ 4.1 Vectorè¡¨ç®¡ç†ï¼ˆæ–°å¢ï¼‰
  â”‚   â”œâ”€â”€ å¤‡ä»½vectorè¡¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
  â”‚   â””â”€â”€ æ¸…ç©ºvectorè¡¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
  â””â”€â”€ 4.2 åŠ è½½è®­ç»ƒæ•°æ®
```

#### 3.2 ç‹¬ç«‹è®­ç»ƒåŠ è½½ (run_training.py)
```
å‰ç½®æ­¥éª¤: Vectorè¡¨ç®¡ç†ï¼ˆæ–°å¢ï¼‰
â”œâ”€â”€ å¤‡ä»½vectorè¡¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
â””â”€â”€ æ¸…ç©ºvectorè¡¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
ä¸»è¦æ­¥éª¤: è®­ç»ƒæ•°æ®åŠ è½½
```

### 4. æ–‡ä»¶ç»“æ„è®¾è®¡

#### 4.1 ç›®å½•ç»“æ„
```
data_pipeline/training_data/manual_20250720_121007/
â”œâ”€â”€ *.ddl                           # DDLæ–‡ä»¶
â”œâ”€â”€ *.md                            # æ–‡æ¡£æ–‡ä»¶
â”œâ”€â”€ *.json                          # QSæ–‡ä»¶
â”œâ”€â”€ data_pipeline.log               # ä»»åŠ¡æ—¥å¿—ï¼ˆç›´æ¥åœ¨æ ¹ç›®å½•ï¼‰
â”œâ”€â”€ vector_bak/                     # æ–°å¢ï¼švectorå¤‡ä»½ç›®å½•
â”‚   â”œâ”€â”€ langchain_pg_collection_20250720_121007.csv
â”‚   â”œâ”€â”€ langchain_pg_embedding_20250720_121007.csv
â”‚   â””â”€â”€ vector_backup_log.txt       # å¤‡ä»½æ“ä½œæ—¥å¿—
â””â”€â”€ task_config.json                # ä»»åŠ¡é…ç½®æ–‡ä»¶
```

#### 4.2 å¤‡ä»½æ“ä½œæ—¥å¿—æ ¼å¼
```
=== Vector Table Backup Log ===
Backup Time: 2025-01-20 12:10:07
Task ID: manual_20250720_121007
Database: highway_db

Tables Backup Status:
âœ“ langchain_pg_collection: 1,234 rows -> langchain_pg_collection_20250720_121007.csv (45.6 KB)
âœ“ langchain_pg_embedding: 12,345 rows -> langchain_pg_embedding_20250720_121007.csv (2.1 MB)

Truncate Status:
âœ“ langchain_pg_embedding: TRUNCATED
- langchain_pg_collection: SKIPPED (collection table preserved)
```

### 5. è„šæœ¬æ€»ç»“æŠ¥å‘Šè®¾è®¡

**è¦æ±‚**ï¼šåœ¨è„šæœ¬ä½œä¸šçš„æ—¥å¿—æœ€åçš„summaryé˜¶æ®µï¼Œå¿…é¡»æ€»ç»“æ˜¯å¦æ‰§è¡Œäº†å¤‡ä»½å’Œtruncateã€‚

#### 5.1 schema_workflow.py æ€»ç»“ä¿®æ”¹
åœ¨ `print_final_summary()` æ–¹æ³•ä¸­æ·»åŠ vectorç®¡ç†æ€»ç»“ï¼š

```python
def print_final_summary(self, report: Dict[str, Any]):
    # ç°æœ‰æ€»ç»“é€»è¾‘...
    
    # æ–°å¢ï¼šVectorè¡¨ç®¡ç†æ€»ç»“
    vector_stats = report.get("vector_management_stats")
    if vector_stats:
        self.logger.info("ğŸ“Š Vectorè¡¨ç®¡ç†:")
        if vector_stats.get("backup_performed", False):
            tables_count = len(vector_stats.get("tables_backed_up", {}))
            total_size = sum(info.get("file_size", 0) for info in vector_stats.get("tables_backed_up", {}).values())
            self.logger.info(f"   âœ… å¤‡ä»½æ‰§è¡Œ: {tables_count}ä¸ªè¡¨ï¼Œæ€»å¤§å°: {self._format_size(total_size)}")
        else:
            self.logger.info("   - å¤‡ä»½æ‰§è¡Œ: æœªæ‰§è¡Œ")
            
        if vector_stats.get("truncate_performed", False):
            self.logger.info("   âœ… æ¸…ç©ºæ‰§è¡Œ: langchain_pg_embeddingè¡¨å·²æ¸…ç©º")
        else:
            self.logger.info("   - æ¸…ç©ºæ‰§è¡Œ: æœªæ‰§è¡Œ")
            
        duration = vector_stats.get("duration", 0)
        self.logger.info(f"   â±ï¸  æ‰§è¡Œè€—æ—¶: {duration:.1f}ç§’")
    else:
        self.logger.info("ğŸ“Š Vectorè¡¨ç®¡ç†: æœªæ‰§è¡Œï¼ˆæœªå¯ç”¨ç›¸å…³å‚æ•°ï¼‰")
```

#### 5.2 run_training.py æ€»ç»“ä¿®æ”¹
åœ¨ `main()` å‡½æ•°çš„æœ€ç»ˆç»Ÿè®¡éƒ¨åˆ†æ·»åŠ vectorç®¡ç†æŠ¥å‘Šï¼š

```python
def main():
    # ç°æœ‰é€»è¾‘...
    
    # æ‰§è¡Œè®­ç»ƒå¤„ç†
    process_successful, vector_stats = process_training_files(data_path, task_id, 
                                                             backup_vector_tables, 
                                                             truncate_vector_tables)
    
    # åŸæœ‰æˆåŠŸç»Ÿè®¡...
    
    # æ–°å¢ï¼šVectorè¡¨ç®¡ç†æ€»ç»“
    print("\n===== Vectorè¡¨ç®¡ç†ç»Ÿè®¡ =====")
    if vector_stats:
        if vector_stats.get("backup_performed", False):
            tables_info = vector_stats.get("tables_backed_up", {})
            print(f"âœ“ å¤‡ä»½æ‰§è¡Œ: æˆåŠŸå¤‡ä»½ {len(tables_info)} ä¸ªè¡¨")
            for table_name, info in tables_info.items():
                print(f"  - {table_name}: {info['row_count']}è¡Œ -> {info['backup_file']} ({info['file_size']})")
        if vector_stats.get("truncate_performed", False):
            print("âœ“ æ¸…ç©ºæ‰§è¡Œ: langchain_pg_embeddingè¡¨å·²æ¸…ç©º")
        print(f"âœ“ æ€»è€—æ—¶: {vector_stats.get('duration', 0):.1f}ç§’")
    else:
        print("- æœªæ‰§è¡Œvectorè¡¨ç®¡ç†æ“ä½œ")
    
    print("===========================")
```

### 6. å…·ä½“ä¿®æ”¹æ–¹æ¡ˆ

#### 6.1 ä¿®æ”¹ schema_workflow.py

**æ–°å¢å‚æ•°**:
```python
parser.add_argument(
    "--backup-vector-tables",
    action="store_true",
    help="å¤‡ä»½vectorè¡¨æ•°æ®åˆ°ä»»åŠ¡ç›®å½•"
)

parser.add_argument(
    "--truncate-vector-tables", 
    action="store_true",
    help="æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰"
)
```

**ä¿®æ”¹ SchemaWorkflowOrchestrator æ„é€ å‡½æ•°**:
```python
def __init__(self, ..., backup_vector_tables: bool = False, truncate_vector_tables: bool = False):
    # å‚æ•°éªŒè¯å’Œè‡ªåŠ¨å¯ç”¨é€»è¾‘
    if truncate_vector_tables:
        backup_vector_tables = True
```

**ä¿®æ”¹ _execute_step_4_training_data_load**:
```python
async def _execute_step_4_training_data_load(self):
    # æ–°å¢ï¼šVectorè¡¨ç®¡ç†
    if self.backup_vector_tables or self.truncate_vector_tables:
        await self._execute_vector_table_management()
    
    # åŸæœ‰ï¼šè®­ç»ƒæ•°æ®åŠ è½½
    load_successful = process_training_files(
        training_data_dir, 
        self.task_id,
        backup_vector_tables=False,  # é¿å…é‡å¤æ‰§è¡Œ
        truncate_vector_tables=False  # é¿å…é‡å¤æ‰§è¡Œ
    )
```

**æ–°å¢ç‹¬ç«‹çš„Vectorè¡¨ç®¡ç†æ–¹æ³•**:
```python
async def _execute_vector_table_management(self):
    """ç‹¬ç«‹æ‰§è¡ŒVectorè¡¨ç®¡ç†ï¼ˆæ”¯æŒ--skip-training-loadåœºæ™¯ï¼‰"""
    if not (self.backup_vector_tables or self.truncate_vector_tables):
        return
        
    self.logger.info("ğŸ—‚ï¸ å¼€å§‹æ‰§è¡ŒVectorè¡¨ç®¡ç†...")
    
    try:
        from data_pipeline.trainer.vector_table_manager import VectorTableManager
        
        vector_manager = VectorTableManager(
            task_output_dir=str(self.output_dir),
            task_id=self.task_id
        )
        
        # æ‰§è¡Œvectorè¡¨ç®¡ç†
        vector_stats = await vector_manager.execute_vector_management(
            backup=self.backup_vector_tables,
            truncate=self.truncate_vector_tables
        )
        
        # è®°å½•ç»“æœåˆ°å·¥ä½œæµçŠ¶æ€
        self.workflow_state["artifacts"]["vector_management"] = vector_stats
        
        self.logger.info("âœ… Vectorè¡¨ç®¡ç†å®Œæˆ")
        
    except Exception as e:
        self.logger.error(f"âŒ Vectorè¡¨ç®¡ç†å¤±è´¥: {e}")
        raise
```

**ä¿®æ”¹ä¸»å·¥ä½œæµä»¥æ”¯æŒ--skip-training-loadåœºæ™¯**:
```python
async def execute_complete_workflow(self) -> Dict[str, Any]:
    # ç°æœ‰æ­¥éª¤1-3...
    
    # æ–°å¢ï¼šç‹¬ç«‹çš„Vectorè¡¨ç®¡ç†ï¼ˆåœ¨è®­ç»ƒåŠ è½½ä¹‹å‰æˆ–æ›¿ä»£è®­ç»ƒåŠ è½½ï¼‰
    if self.backup_vector_tables or self.truncate_vector_tables:
        await self._execute_vector_table_management()
    
    # æ­¥éª¤4: è®­ç»ƒæ•°æ®åŠ è½½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if self.enable_training_data_load:
        await self._execute_step_4_training_data_load()
    else:
        self.logger.info("â­ï¸ è·³è¿‡è®­ç»ƒæ•°æ®åŠ è½½æ­¥éª¤")
```

#### 6.2 ä¿®æ”¹ run_training.py

**æ–°å¢å‚æ•°å¤„ç†**:
```python
parser.add_argument('--backup-vector-tables', action='store_true', help='å¤‡ä»½vectorè¡¨æ•°æ®')
parser.add_argument('--truncate-vector-tables', action='store_true', help='æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰')
```

**ä¿®æ”¹ process_training_files å‡½æ•°**:
```python
def process_training_files(data_path, task_id=None, backup_vector_tables=False, truncate_vector_tables=False):
    # å‚æ•°éªŒè¯å’Œè‡ªåŠ¨å¯ç”¨é€»è¾‘
    if truncate_vector_tables:
        backup_vector_tables = True
    
    # Vectorè¡¨ç®¡ç†ï¼ˆå‰ç½®æ­¥éª¤ï¼‰
    vector_stats = None
    if backup_vector_tables or truncate_vector_tables:
        vector_manager = VectorTableManager(data_path, task_id)
        vector_stats = asyncio.run(vector_manager.execute_vector_management(backup_vector_tables, truncate_vector_tables))
    
    # åŸæœ‰è®­ç»ƒæ•°æ®å¤„ç†é€»è¾‘...
    
    # åœ¨æœ€ç»ˆç»Ÿè®¡ä¸­åŒ…å«vectorç®¡ç†ä¿¡æ¯
    return process_successful, vector_stats
```

#### 6.3 ä¿®æ”¹ API ç›¸å…³æ–‡ä»¶

**SimpleWorkflowExecutor ä¿®æ”¹**:
```python
def __init__(self, task_id: str, backup_vector_tables: bool = False, truncate_vector_tables: bool = False):
    # ä¼ é€’å‚æ•°ç»™ orchestrator
```

**API è·¯ç”±å¤„ç†**ï¼ˆåç»­æ­¥éª¤ï¼Œå½“å‰ä¸å®ç°ï¼‰:
```json
{
    "execution_mode": "complete",
    "step_name": null,
    "backup_vector_tables": false,
    "truncate_vector_tables": false
}
```

### 7. SQLæ“ä½œè®¾è®¡

#### 7.1 å¤‡ä»½æ“ä½œ

**SQLå‘½ä»¤è®¾è®¡**ï¼š
```sql
-- è®¾ç½®ç¼–ç 
SET client_encoding TO 'UTF8';

-- å¯¼å‡ºæ•°æ®ï¼ˆå…ˆå¯¼å‡ºä¸º.tmpæ–‡ä»¶ï¼‰
COPY langchain_pg_collection TO '{backup_path}/langchain_pg_collection_{timestamp}.csv.tmp' WITH CSV HEADER;
COPY langchain_pg_embedding TO '{backup_path}/langchain_pg_embedding_{timestamp}.csv.tmp' WITH CSV HEADER;
```

**Pythonå®ç°æ–¹å¼**ï¼š
```python
async def backup_vector_tables(self) -> Dict[str, Any]:
    """å¤‡ä»½vectorè¡¨æ•°æ®"""
    
    # 1. åˆ›å»ºå¤‡ä»½ç›®å½•
    backup_dir = Path(self.task_output_dir) / "vector_bak"
    backup_dir.mkdir(parents=True, exist_ok=True)
    
    # 2. ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime(self.config["timestamp_format"])
    
    # 3. æ‰§è¡Œå¤‡ä»½ï¼ˆæ¯ä¸ªè¡¨åˆ†åˆ«å¤„ç†ï¼‰
    results = {}
    
    for table_name in self.config["supported_tables"]:
        try:
            # 3.1 å®šä¹‰æ–‡ä»¶è·¯å¾„ï¼ˆ.tmpä¸´æ—¶æ–‡ä»¶ï¼‰
            temp_file = backup_dir / f"{table_name}_{timestamp}.csv.tmp"
            final_file = backup_dir / f"{table_name}_{timestamp}.csv"
            
            # 3.2 æ‰§è¡ŒCOPYå‘½ä»¤å¯¼å‡ºåˆ°.tmpæ–‡ä»¶
            copy_sql = f"""
                SET client_encoding TO 'UTF8';
                COPY {table_name} TO '{temp_file}' WITH CSV HEADER;
            """
            
            # 3.3 é€šè¿‡psycopg2æ‰§è¡ŒSQL
            start_time = time.time()
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    # æ‰§è¡Œç¼–ç è®¾ç½®
                    cursor.execute("SET client_encoding TO 'UTF8'")
                    
                    # æ‰§è¡ŒCOPYå‘½ä»¤
                    cursor.execute(f"COPY {table_name} TO '{temp_file}' WITH CSV HEADER")
                    
                    # è·å–å¯¼å‡ºè¡Œæ•°
                    row_count = cursor.rowcount
            
            # 3.4 å¯¼å‡ºå®Œæˆåï¼Œé‡å‘½åæ–‡ä»¶ (.tmp -> .csv)
            if temp_file.exists():
                temp_file.rename(final_file)
                
                # 3.5 è·å–æ–‡ä»¶ä¿¡æ¯
                file_stat = final_file.stat()
                duration = time.time() - start_time
                
                results[table_name] = {
                    "success": True,
                    "row_count": row_count,
                    "file_size": self._format_file_size(file_stat.st_size),
                    "backup_file": final_file.name,
                    "duration": duration
                }
                
                self.logger.info(f"âœ… {table_name} å¤‡ä»½æˆåŠŸ: {row_count}è¡Œ -> {final_file.name}")
            else:
                raise Exception(f"ä¸´æ—¶æ–‡ä»¶ {temp_file} æœªç”Ÿæˆ")
                
        except Exception as e:
            results[table_name] = {
                "success": False,
                "error": str(e)
            }
            self.logger.error(f"âŒ {table_name} å¤‡ä»½å¤±è´¥: {e}")
            
            # æ¸…ç†å¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶
            if temp_file.exists():
                temp_file.unlink()
    
    return results

# æ³¨æ„ï¼šget_connection()æ–¹æ³•åœ¨ç±»çš„å…¶ä»–åœ°æ–¹å·²å®šä¹‰ï¼Œè¿™é‡Œä¸éœ€è¦é‡å¤
```

**å…³é”®è®¾è®¡ç‚¹**ï¼š
1. **ä¸´æ—¶æ–‡ä»¶æœºåˆ¶**: å…ˆå¯¼å‡ºä¸º `.csv.tmp` æ–‡ä»¶ï¼Œå®Œæˆåé‡å‘½åä¸º `.csv`
2. **åŸå­æ€§æ“ä½œ**: ç¡®ä¿æ–‡ä»¶é‡å‘½åæ˜¯åŸå­æ“ä½œï¼Œé¿å…ä¸‹è½½åˆ°æœªå®Œæˆçš„æ–‡ä»¶
3. **é”™è¯¯å¤„ç†**: å¦‚æœå¯¼å‡ºå¤±è´¥ï¼Œè‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶
4. **é€è¡¨å¤„ç†**: æ¯ä¸ªè¡¨å•ç‹¬å¤‡ä»½ï¼Œä¸€ä¸ªå¤±è´¥ä¸å½±å“å…¶ä»–è¡¨

#### 7.2 æ¸…ç©ºæ“ä½œ

**SQLå‘½ä»¤è®¾è®¡**ï¼š
```sql
-- åªæ¸…ç©º embedding è¡¨ï¼Œä¿ç•™ collection è¡¨
TRUNCATE TABLE langchain_pg_embedding;
```

**Pythonå®ç°æ–¹å¼**ï¼š
```python
async def truncate_vector_tables(self) -> Dict[str, Any]:
    """æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆåªæ¸…ç©ºlangchain_pg_embeddingï¼‰"""
    
    results = {}
    
    # åªæ¸…ç©ºé…ç½®ä¸­æŒ‡å®šçš„è¡¨ï¼ˆé€šå¸¸åªæœ‰langchain_pg_embeddingï¼‰
    truncate_tables = self.config["truncate_tables"]
    
    for table_name in truncate_tables:
        try:
            # è®°å½•æ¸…ç©ºå‰çš„è¡Œæ•°ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
            count_sql = f"SELECT COUNT(*) FROM {table_name}"
            
            start_time = time.time()
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    # 1. è·å–æ¸…ç©ºå‰çš„è¡Œæ•°
                    cursor.execute(count_sql)
                    rows_before = cursor.fetchone()[0]
                    
                    # 2. æ‰§è¡ŒTRUNCATE
                    cursor.execute(f"TRUNCATE TABLE {table_name}")
                    
                    # 3. éªŒè¯æ¸…ç©ºç»“æœ
                    cursor.execute(count_sql)
                    rows_after = cursor.fetchone()[0]
            
            duration = time.time() - start_time
            
            if rows_after == 0:
                results[table_name] = {
                    "success": True,
                    "rows_before": rows_before,
                    "rows_after": rows_after,
                    "duration": duration
                }
                self.logger.info(f"âœ… {table_name} æ¸…ç©ºæˆåŠŸ: {rows_before}è¡Œ -> 0è¡Œ")
            else:
                raise Exception(f"æ¸…ç©ºå¤±è´¥ï¼Œè¡¨ä¸­ä»æœ‰ {rows_after} è¡Œæ•°æ®")
                
        except Exception as e:
            results[table_name] = {
                "success": False,
                "error": str(e)
            }
            self.logger.error(f"âŒ {table_name} æ¸…ç©ºå¤±è´¥: {e}")
    
    return results
```

**å…³é”®è®¾è®¡ç‚¹**ï¼š
1. **é€‰æ‹©æ€§æ¸…ç©º**: åªæ¸…ç©º `langchain_pg_embedding` è¡¨ï¼Œä¿ç•™ `langchain_pg_collection` è¡¨
2. **ç»Ÿè®¡ä¿¡æ¯**: è®°å½•æ¸…ç©ºå‰åçš„è¡Œæ•°ï¼Œä¾¿äºç»Ÿè®¡æŠ¥å‘Š
3. **éªŒè¯æœºåˆ¶**: æ¸…ç©ºåéªŒè¯è¡¨ç¡®å®ä¸ºç©º
4. **äº‹åŠ¡å®‰å…¨**: æ¯ä¸ªè¡¨çš„æ“ä½œåœ¨ç‹¬ç«‹çš„è¿æ¥ä¸­æ‰§è¡Œ

#### 7.3 æ¢å¤æ“ä½œï¼ˆå¤‡ç”¨ï¼Œä¸åœ¨å½“å‰éœ€æ±‚ä¸­ï¼‰
```sql
SET client_encoding TO 'UTF8';
COPY langchain_pg_collection FROM '{backup_path}/langchain_pg_collection_{timestamp}.csv' WITH CSV HEADER;
COPY langchain_pg_embedding FROM '{backup_path}/langchain_pg_embedding_{timestamp}.csv' WITH CSV HEADER;
```

### 8. é”™è¯¯å¤„ç†å’Œå›æ»š

#### 8.1 é”™è¯¯åœºæ™¯
1. æ•°æ®åº“è¿æ¥å¤±è´¥
2. æƒé™ä¸è¶³ï¼ˆæ— æ³•æ‰§è¡Œ COPY æˆ– TRUNCATEï¼‰
3. ç£ç›˜ç©ºé—´ä¸è¶³
4. å¤‡ä»½æ–‡ä»¶å†™å…¥å¤±è´¥
5. æ¸…ç©ºæ“ä½œå¤±è´¥

#### 8.2 å›æ»šç­–ç•¥
- å¦‚æœå¤‡ä»½å¤±è´¥ï¼Œä¸æ‰§è¡Œæ¸…ç©ºæ“ä½œ
- å¦‚æœæ¸…ç©ºå¤±è´¥ï¼Œä¿ç•™å¤‡ä»½æ–‡ä»¶ï¼Œè®°å½•é”™è¯¯çŠ¶æ€
- æä¾›è¯¦ç»†çš„é”™è¯¯æ—¥å¿—å’ŒçŠ¶æ€æŠ¥å‘Š

### 9. é…ç½®ç®¡ç†

#### 9.1 æ–°å¢é…ç½®é¡¹
```python
# data_pipeline/config.py æ–°å¢é…ç½®é¡¹
SCHEMA_TOOLS_CONFIG = {
    # ç°æœ‰é…ç½®...
    
    # æ–°å¢ï¼šVectorè¡¨ç®¡ç†é…ç½®
    "vector_table_management": {
        "backup_enabled": True,
        "backup_directory": "vector_bak",
        "supported_tables": [
            "langchain_pg_collection",
            "langchain_pg_embedding"
        ],
        "truncate_tables": [
            "langchain_pg_embedding"  # åªæ¸…ç©ºembeddingè¡¨
        ],
        "timestamp_format": "%Y%m%d_%H%M%S",
        "backup_temp_suffix": ".tmp"
    }
}
```

### 10. æ—¥å¿—å’Œç›‘æ§

#### 10.1 æ—¥å¿—çº§åˆ«
- INFO: æ­£å¸¸æ“ä½œï¼ˆå¼€å§‹å¤‡ä»½ã€å®Œæˆå¤‡ä»½ç­‰ï¼‰
- WARNING: éè‡´å‘½é—®é¢˜ï¼ˆæƒé™é™åˆ¶ã€æ–‡ä»¶å·²å­˜åœ¨ç­‰ï¼‰
- ERROR: æ“ä½œå¤±è´¥ï¼ˆè¿æ¥å¤±è´¥ã€ç£ç›˜æ»¡ç­‰ï¼‰

#### 10.2 ç»Ÿè®¡ä¿¡æ¯

**ç»Ÿè®¡ä¿¡æ¯å°†å‡ºç°åœ¨ä»¥ä¸‹ä½ç½®**ï¼š
1. **API è¿”å›ç»“æœ**ï¼šä»»åŠ¡æ‰§è¡Œå®Œæˆåçš„JSONå“åº”ä¸­
2. **è„šæœ¬æ—¥å¿—æ‘˜è¦**ï¼šå‘½ä»¤è¡Œè„šæœ¬çš„æœ€ç»ˆæ€»ç»“é˜¶æ®µ
3. **ä»»åŠ¡ç›®å½•æ—¥å¿—æ–‡ä»¶**ï¼šè¯¦ç»†çš„æ“ä½œæ—¥å¿—

**ç»Ÿè®¡ä¿¡æ¯æ ¼å¼**ï¼š
```python
{
    "vector_management_stats": {
        "backup_performed": True,
        "truncate_performed": True,
        "tables_backed_up": {
            "langchain_pg_collection": {
                "row_count": 1234,
                "file_size": "45.6 KB",
                "backup_file": "langchain_pg_collection_20250720_121007.csv"
            },
            "langchain_pg_embedding": {
                "row_count": 12345,
                "file_size": "2.1 MB", 
                "backup_file": "langchain_pg_embedding_20250720_121007.csv"
            }
        },
        "truncate_results": {
            "langchain_pg_embedding": "SUCCESS"
        },
        "duration": 12.5,
        "backup_directory": "/path/to/task/vector_bak"
    }
}
```

**è„šæœ¬æ€»ç»“ç¤ºä¾‹**ï¼š
```
ğŸ“Š å·¥ä½œæµç¨‹æ‰§è¡Œç»Ÿè®¡
===================
âœ… Vectorè¡¨ç®¡ç†:
   - å¤‡ä»½æ‰§è¡Œ: æ˜¯
   - æ¸…ç©ºæ‰§è¡Œ: æ˜¯  
   - å¤‡ä»½æ–‡ä»¶: 2ä¸ª (å…±2.15MB)
   - æ‰§è¡Œè€—æ—¶: 12.5ç§’

æˆ–è€…ï¼ˆå¦‚æœæœªæ‰§è¡Œvectorç®¡ç†ï¼‰ï¼š
ğŸ“Š å·¥ä½œæµç¨‹æ‰§è¡Œç»Ÿè®¡
===================
- Vectorè¡¨ç®¡ç†: æœªæ‰§è¡Œï¼ˆæœªå¯ç”¨ç›¸å…³å‚æ•°ï¼‰
```

### 11. API æ”¯æŒè®¾è®¡è€ƒè™‘

#### 11.1 å½“å‰ API ç»“æ„åˆ†æ
å½“å‰æ‰§è¡Œ APIï¼š`POST /api/v0/data_pipeline/tasks/{task_id}/execute`

è¯·æ±‚ä½“æ ¼å¼ï¼š
```json
{
    "execution_mode": "complete|step",
    "step_name": "ddl_generation|qa_generation|sql_validation|training_load"
}
```

#### 11.2 API æ‰©å±•æ–¹æ¡ˆ
**æ–¹æ¡ˆ1**: åœ¨è¯·æ±‚ä½“ä¸­æ·»åŠ  vector ç®¡ç†å‚æ•°
```json
{
    "execution_mode": "complete",
    "step_name": null,
    "vector_options": {
        "backup_vector_tables": false,
        "truncate_vector_tables": false
    }
}
```

**æ–¹æ¡ˆ2**: æ‰å¹³åŒ–å‚æ•°ç»“æ„
```json
{
    "execution_mode": "complete",
    "step_name": null,
    "backup_vector_tables": false,
    "truncate_vector_tables": false
}
```

#### 11.3 API å“åº”æ‰©å±•
å“åº”ä¸­åŒ…å« vector ç®¡ç†æ“ä½œçš„ç»“æœï¼š
```json
{
    "success": true,
    "task_id": "manual_20250720_121007",
    "execution_mode": "complete", 
    "result": {
        "workflow_state": {...},
        "vector_management": {
            "backup_performed": true,
            "truncate_performed": true,
            "backup_files": [...],
            "statistics": {...}
        }
    }
}
```

### 12. æµ‹è¯•ç­–ç•¥

#### 12.1 å•å…ƒæµ‹è¯•
- VectorTableManager ç±»çš„å„ä¸ªæ–¹æ³•
- å‚æ•°éªŒè¯é€»è¾‘
- SQL æ“ä½œå°è£…

#### 12.2 é›†æˆæµ‹è¯•  
- å®Œæ•´å·¥ä½œæµä¸­çš„ vector ç®¡ç†
- ç‹¬ç«‹è®­ç»ƒåŠ è½½ä¸­çš„ vector ç®¡ç†
- API è°ƒç”¨åœºæ™¯

#### 12.3 è¾¹ç•Œæµ‹è¯•
- å¤§æ•°æ®é‡å¤‡ä»½
- ç£ç›˜ç©ºé—´ä¸è¶³åœºæ™¯
- æ•°æ®åº“æƒé™é™åˆ¶åœºæ™¯

### 13. å®æ–½è®¡åˆ’

#### é˜¶æ®µ1: æ ¸å¿ƒåŠŸèƒ½å®ç°
1. åˆ›å»º VectorTableManager ç±»
2. ä¿®æ”¹ schema_workflow.py å‚æ•°å¤„ç†
3. ä¿®æ”¹ run_training.py å‚æ•°å¤„ç†
4. å®ç°å¤‡ä»½å’Œæ¸…ç©ºé€»è¾‘

#### é˜¶æ®µ2: é›†æˆæµ‹è¯•
1. å®Œæ•´å·¥ä½œæµæµ‹è¯•
2. ç‹¬ç«‹è®­ç»ƒåŠ è½½æµ‹è¯•
3. é”™è¯¯åœºæ™¯æµ‹è¯•

#### é˜¶æ®µ3: API æ”¯æŒï¼ˆåç»­ï¼‰
1. ä¿®æ”¹ SimpleWorkflowExecutor
2. æ‰©å±• API æ¥å£
3. API æµ‹è¯•

### 14. é£é™©è¯„ä¼°

#### 14.1 ä¸»è¦é£é™©
1. **æ•°æ®ä¸¢å¤±é£é™©**: æ¸…ç©ºæ“ä½œä¸å¯é€†ï¼Œå¿…é¡»ç¡®ä¿å¤‡ä»½æˆåŠŸ
2. **ç£ç›˜ç©ºé—´é£é™©**: å¤‡ä»½å¤§é‡æ•°æ®å¯èƒ½å¡«æ»¡ç£ç›˜
3. **æƒé™é£é™©**: COPY å‘½ä»¤éœ€è¦è¶³å¤Ÿçš„æ–‡ä»¶ç³»ç»Ÿæƒé™
4. **å¹¶å‘é£é™©**: è®­ç»ƒè¿‡ç¨‹ä¸­å…¶ä»–è¿›ç¨‹å¯èƒ½åœ¨è®¿é—® vector è¡¨

#### 14.2 é£é™©ç¼“è§£
1. å¤‡ä»½å¤±è´¥æ—¶ä¸æ‰§è¡Œæ¸…ç©ºæ“ä½œ
2. é¢„å…ˆæ£€æŸ¥ç£ç›˜ç©ºé—´
3. æƒé™æ£€æŸ¥å’Œå‹å¥½çš„é”™è¯¯æç¤º
4. æ¸…æ™°çš„æ“ä½œæ—¥å¿—å’ŒçŠ¶æ€æŠ¥å‘Š

### 15. æ–‡æ¡£å’Œç”¨æˆ·æŒ‡å—

#### 15.1 ç”¨æˆ·æ–‡æ¡£
- å‚æ•°ä½¿ç”¨è¯´æ˜
- å¤‡ä»½æ–‡ä»¶ä½ç½®å’Œå‘½åè§„åˆ™
- å¸¸è§é”™è¯¯åŠè§£å†³æ–¹æ¡ˆ

#### 15.2 å¼€å‘æ–‡æ¡£
- VectorTableManager API æ–‡æ¡£
- é…ç½®é¡¹è¯´æ˜
- æ‰©å±•æŒ‡å—

## æ€»ç»“

è¿™ä¸ªè®¾è®¡æä¾›äº†ä¸€ä¸ªå®Œæ•´çš„ vector è¡¨ç®¡ç†åŠŸèƒ½ï¼ŒåŒ…æ‹¬ï¼š

1. **æ¸…æ™°çš„å‚æ•°ä¾èµ–å…³ç³»**: ç¡®ä¿æ•°æ®å®‰å…¨
2. **çµæ´»çš„æ‰§è¡Œæ–¹å¼**: æ”¯æŒå¤šç§å…¥å£
3. **å®Œå–„çš„é”™è¯¯å¤„ç†**: ç¡®ä¿æ“ä½œå¯é æ€§
4. **è¯¦ç»†çš„æ—¥å¿—è®°å½•**: ä¾¿äºé—®é¢˜è¯Šæ–­
5. **API æ‰©å±•è€ƒè™‘**: ä¸ºåç»­åŠŸèƒ½åšå‡†å¤‡

### å¯¹ç”¨æˆ·åé¦ˆçš„ä¿®æ­£

æ ¹æ®ç”¨æˆ·åé¦ˆï¼Œå·²å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œäº†ä¿®æ­£ï¼š

#### ç¬¬ä¸€è½®ä¿®æ­£ï¼š
1. **æ•°æ®åº“è¿æ¥é…ç½®**: ä¿®æ­£ä¸ºä» `data_pipeline.config.SCHEMA_TOOLS_CONFIG` è·å–ï¼ˆè¯¥é…ç½®ä» `app_config.PGVECTOR_CONFIG` ç»§æ‰¿ï¼‰ï¼Œè€Œä¸æ˜¯é€šè¿‡å‚æ•°ä¼ é€’
2. **ç›®å½•ç»“æ„**: ä¿®æ­£äº†ä»»åŠ¡ç›®å½•ç»“æ„ï¼Œæ—¥å¿—æ–‡ä»¶ç›´æ¥å­˜å‚¨åœ¨ä»»åŠ¡æ ¹ç›®å½•ï¼Œè€Œä¸æ˜¯ `logs/` å­ç›®å½•
3. **å‚æ•°é»˜è®¤å€¼**: æ˜ç¡®è¯´æ˜ `--backup-vector-tables` å’Œ `--truncate-vector-tables` éƒ½æ˜¯å¯é€‰å‚æ•°ï¼Œæ²¡æœ‰é»˜è®¤å€¼
4. **é…ç½®æ ¼å¼**: ä¿®æ­£äº†é…ç½®é¡¹æ ¼å¼ï¼Œä½¿ç”¨æ­£ç¡®çš„ Python å­—å…¸æ ¼å¼è€Œä¸æ˜¯ YAML æ ¼å¼
5. **ç»Ÿè®¡ä¿¡æ¯ä½ç½®**: æ˜ç¡®äº†ç»Ÿè®¡ä¿¡æ¯å°†å‡ºç°åœ¨ API è¿”å›ç»“æœã€è„šæœ¬æ—¥å¿—æ‘˜è¦å’Œä»»åŠ¡ç›®å½•æ—¥å¿—æ–‡ä»¶ä¸­
6. **è„šæœ¬æ€»ç»“**: æ·»åŠ äº†è¯¦ç»†çš„è„šæœ¬æ€»ç»“æŠ¥å‘Šè®¾è®¡ï¼Œç¡®ä¿åœ¨è„šæœ¬ä½œä¸šçš„æ—¥å¿—æœ€åæ€»ç»“æ˜¯å¦æ‰§è¡Œäº†å¤‡ä»½å’Œtruncate

#### ç¬¬äºŒè½®ä¿®æ­£ï¼š
7. **ä¸´æ—¶æ–‡ä»¶æœºåˆ¶**: è¡¥å……äº†å®Œæ•´çš„ `.csv.tmp` ä¸´æ—¶æ–‡ä»¶è®¾è®¡ï¼Œç¡®ä¿å¯¼å‡ºè¿‡ç¨‹çš„åŸå­æ€§
   - å…ˆå¯¼å‡ºä¸º `.csv.tmp` æ–‡ä»¶
   - å¯¼å‡ºå®Œæˆåé‡å‘½åä¸º `.csv` æ–‡ä»¶
   - å¦‚æœå¯¼å‡ºå¤±è´¥ï¼Œè‡ªåŠ¨æ¸…ç†ä¸´æ—¶æ–‡ä»¶
8. **SQLæ‰§è¡Œæ–¹å¼**: è¡¥å……äº†è¯¦ç»†çš„PostgreSQLå‘½ä»¤æ‰§è¡Œè®¾è®¡
   - ä½¿ç”¨ `psycopg2` è¿æ¥pgvectoræ•°æ®åº“ï¼ˆé…ç½®ä» `data_pipeline.config` è·å–ï¼‰
   - è¯¦ç»†çš„è¿æ¥ç®¡ç†å’Œé”™è¯¯å¤„ç†
   - å®Œæ•´çš„å¤‡ä»½å’Œæ¸…ç©ºæ“ä½œå®ç°ä»£ç 
9. **ä¸»è¦æ‰§è¡Œæµç¨‹**: æ·»åŠ äº†å®Œæ•´çš„ `execute_vector_management()` æ–¹æ³•è®¾è®¡
   - å‚æ•°éªŒè¯å’Œè‡ªåŠ¨å¯ç”¨é€»è¾‘
   - å¤‡ä»½æˆåŠŸéªŒè¯åå†æ‰§è¡Œæ¸…ç©º
   - è¯¦ç»†çš„é”™è¯¯å¤„ç†å’ŒçŠ¶æ€è·Ÿè¸ª
10. **å¤‡ä»½æ—¥å¿—**: è¡¥å……äº†è¯¦ç»†çš„å¤‡ä»½æ“ä½œæ—¥å¿—å†™å…¥æœºåˆ¶

#### ç¬¬ä¸‰è½®ä¿®æ­£ï¼š
11. **--skip-training-loadåœºæ™¯**: è¡¥å……äº†å½“ `schema_workflow` ä½¿ç”¨ `--skip-training-load` æ—¶ä»æ”¯æŒvectorè¡¨ç®¡ç†çš„è®¾è®¡
    - ç‹¬ç«‹çš„ `_execute_vector_table_management()` æ–¹æ³•
    - åœ¨ä¸»å·¥ä½œæµä¸­ç‹¬ç«‹æ‰§è¡Œï¼Œä¸ä¾èµ–è®­ç»ƒåŠ è½½æ­¥éª¤
12. **é‡å¤æ‰§è¡Œé¿å…æœºåˆ¶**: è®¾è®¡äº†é˜²æ­¢vectorè¡¨ç®¡ç†æ“ä½œé‡å¤æ‰§è¡Œçš„æœºåˆ¶
    - `schema_workflow` ä¸­ç‹¬ç«‹æ‰§è¡Œvectorç®¡ç†
    - ä¼ é€’ç»™ `run_training` æ—¶ç¦ç”¨vectorç®¡ç†å‚æ•°ï¼ˆè®¾ä¸ºFalseï¼‰
    - ç¡®ä¿æ“ä½œåªæ‰§è¡Œä¸€æ¬¡

#### ç¬¬å››è½®ä¿®æ­£ï¼š
13. **å‚æ•°å‘½åä¸€è‡´æ€§**: æ ¹æ®å®é™…ä»£ç ä¿®æ­£äº†æ–‡æ¡£ä¸­çš„å‘½ä»¤è¡Œå‚æ•°å†™æ³•
    - ç»Ÿä¸€ä½¿ç”¨è¿å­—ç¬¦æ ¼å¼ï¼š`--backup-vector-tables` å’Œ `--truncate-vector-tables`
    - ä¿®æ­£äº†æ¦‚è¿°å’Œéœ€æ±‚åˆ†æéƒ¨åˆ†çš„å‚æ•°åç§°
    - ç¡®ä¿æ–‡æ¡£ä¸å®é™…ä»£ç å®ç°çš„ä¸€è‡´æ€§

## ğŸ¯ **æ­£ç¡®çš„ä½¿ç”¨ç¤ºä¾‹**

### å‘½ä»¤è¡Œä½¿ç”¨ (æ³¨æ„ä½¿ç”¨è¿å­—ç¬¦)ï¼š

```bash
# 1. å®Œæ•´å·¥ä½œæµ + å¤‡ä»½å’Œæ¸…ç©ºvectorè¡¨
python -m data_pipeline.schema_workflow --db-connection "postgresql://postgres:postgres@localhost:6432/highway_db" --table-list ./data_pipeline/tables.txt --business-context "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ" --truncate-vector-tables

# 2. è·³è¿‡è®­ç»ƒä½†æ‰§è¡Œvectorè¡¨ç®¡ç†
python -m data_pipeline.schema_workflow --db-connection "postgresql://postgres:postgres@localhost:6432/highway_db" --table-list ./data_pipeline/tables.txt --business-context "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ" --skip-training-load --backup-vector-tables

# 3. è·³è¿‡è®­ç»ƒå¹¶æ¸…ç©ºvectorè¡¨
python -m data_pipeline.schema_workflow --db-connection "postgresql://postgres:postgres@localhost:6432/highway_db" --table-list ./data_pipeline/tables.txt --business-context "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ" --skip-training-load --truncate-vector-tables

# 4. ç‹¬ç«‹è®­ç»ƒè„šæœ¬ + vectorè¡¨ç®¡ç†
python -m data_pipeline.trainer.run_training --data_path "./training_data/" --backup-vector-tables --truncate-vector-tables

# 5. åªå¤‡ä»½ä¸æ¸…ç©º
python -m data_pipeline.trainer.run_training --data_path "./training_data/" --backup-vector-tables
```

### å‚æ•°è¯´æ˜ï¼š
- `--backup-vector-tables`: å¤‡ä»½ langchain_pg_collection å’Œ langchain_pg_embedding è¡¨
- `--truncate-vector-tables`: æ¸…ç©º langchain_pg_embedding è¡¨ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰

æ ¸å¿ƒåŸåˆ™æ˜¯**å®‰å…¨ä¼˜å…ˆ**ï¼Œç¡®ä¿åœ¨ä»»ä½•æƒ…å†µä¸‹éƒ½ä¸ä¼šæ„å¤–ä¸¢å¤±æ•°æ®ã€‚ 