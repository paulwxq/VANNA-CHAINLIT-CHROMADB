# Schema Tools è¯¦ç»†è®¾è®¡æ–‡æ¡£

## 1. é¡¹ç›®ç»“æ„ä¸æ¨¡å—è®¾è®¡

### 1.1 å®Œæ•´ç›®å½•ç»“æ„

```
schema_tools/
â”œâ”€â”€ __init__.py                     # æ¨¡å—å…¥å£ï¼Œå¯¼å‡ºä¸»è¦æ¥å£
â”œâ”€â”€ __main__.py                     # å‘½ä»¤è¡Œå…¥å£
â”œâ”€â”€ config.py                       # é…ç½®ç®¡ç†
â”œâ”€â”€ training_data_agent.py          # ä¸»AI Agent
â”œâ”€â”€ tools/                          # Agentå·¥å…·é›†
â”‚   â”œâ”€â”€ __init__.py                 # å·¥å…·æ¨¡å—åˆå§‹åŒ–
â”‚   â”œâ”€â”€ base.py                     # åŸºç¡€å·¥å…·ç±»å’Œæ³¨å†Œæœºåˆ¶
â”‚   â”œâ”€â”€ database_inspector.py       # æ•°æ®åº“å…ƒæ•°æ®æ£€æŸ¥å·¥å…·
â”‚   â”œâ”€â”€ data_sampler.py             # æ•°æ®é‡‡æ ·å·¥å…·
â”‚   â”œâ”€â”€ comment_generator.py        # LLMæ³¨é‡Šç”Ÿæˆå·¥å…·
â”‚   â”œâ”€â”€ ddl_generator.py            # DDLæ ¼å¼ç”Ÿæˆå·¥å…·
â”‚   â””â”€â”€ doc_generator.py            # MDæ–‡æ¡£ç”Ÿæˆå·¥å…·
â”œâ”€â”€ utils/                          # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ data_structures.py          # æ•°æ®ç»“æ„å®šä¹‰
â”‚   â”œâ”€â”€ table_parser.py             # è¡¨æ¸…å•è§£æå™¨
â”‚   â”œâ”€â”€ file_manager.py             # æ–‡ä»¶ç®¡ç†å™¨
â”‚   â”œâ”€â”€ system_filter.py            # ç³»ç»Ÿè¡¨è¿‡æ»¤å™¨
â”‚   â”œâ”€â”€ permission_checker.py       # æƒé™æ£€æŸ¥å™¨
â”‚   â”œâ”€â”€ large_table_handler.py      # å¤§è¡¨å¤„ç†å™¨
â”‚   â””â”€â”€ logger.py                   # æ—¥å¿—ç®¡ç†
â”œâ”€â”€ prompts/                        # æç¤ºè¯æ¨¡æ¿
â”‚   â”œâ”€â”€ table_comment_template.txt
â”‚   â”œâ”€â”€ field_comment_template.txt
â”‚   â”œâ”€â”€ enum_detection_template.txt
â”‚   â”œâ”€â”€ business_context.txt
â”‚   â””â”€â”€ business_dictionary.txt
â””â”€â”€ tests/                          # æµ‹è¯•ç”¨ä¾‹
    â”œâ”€â”€ unit/
    â”œâ”€â”€ integration/
    â””â”€â”€ fixtures/
```

## 2. æ ¸å¿ƒæ•°æ®ç»“æ„è®¾è®¡

### 2.1 æ•°æ®ç»“æ„å®šä¹‰ (`utils/data_structures.py`)

```python
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Any, Union
from enum import Enum
import hashlib
import json

class FieldType(Enum):
    """å­—æ®µç±»å‹æšä¸¾"""
    INTEGER = "integer"
    VARCHAR = "varchar"
    TEXT = "text"
    TIMESTAMP = "timestamp"
    DATE = "date"
    BOOLEAN = "boolean"
    NUMERIC = "numeric"
    ENUM = "enum"
    JSON = "json"
    UUID = "uuid"
    OTHER = "other"

class ProcessingStatus(Enum):
    """å¤„ç†çŠ¶æ€æšä¸¾"""
    PENDING = "pending"
    RUNNING = "running"
    SUCCESS = "success"
    FAILED = "failed"
    SKIPPED = "skipped"

@dataclass
class FieldInfo:
    """å­—æ®µä¿¡æ¯æ ‡å‡†ç»“æ„"""
    name: str
    type: str
    nullable: bool
    default_value: Optional[str] = None
    comment: Optional[str] = None
    original_comment: Optional[str] = None  # åŸå§‹æ³¨é‡Š
    generated_comment: Optional[str] = None  # LLMç”Ÿæˆçš„æ³¨é‡Š
    is_primary_key: bool = False
    is_foreign_key: bool = False
    is_enum: bool = False
    enum_values: Optional[List[str]] = None
    enum_description: Optional[str] = None
    max_length: Optional[int] = None
    precision: Optional[int] = None
    scale: Optional[int] = None
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'name': self.name,
            'type': self.type,
            'nullable': self.nullable,
            'default_value': self.default_value,
            'comment': self.comment,
            'is_primary_key': self.is_primary_key,
            'is_foreign_key': self.is_foreign_key,
            'is_enum': self.is_enum,
            'enum_values': self.enum_values
        }

@dataclass
class TableMetadata:
    """è¡¨å…ƒæ•°æ®æ ‡å‡†ç»“æ„"""
    schema_name: str
    table_name: str
    full_name: str  # schema.table_name
    comment: Optional[str] = None
    original_comment: Optional[str] = None  # åŸå§‹æ³¨é‡Š
    generated_comment: Optional[str] = None  # LLMç”Ÿæˆçš„æ³¨é‡Š
    fields: List[FieldInfo] = field(default_factory=list)
    sample_data: List[Dict[str, Any]] = field(default_factory=list)
    row_count: Optional[int] = None
    table_size: Optional[str] = None  # è¡¨å¤§å°ï¼ˆå¦‚ "1.2 MB"ï¼‰
    created_date: Optional[str] = None
    
    @property
    def safe_file_name(self) -> str:
        """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
        if self.schema_name.lower() == 'public':
            return self.table_name
        return f"{self.schema_name}__{self.table_name}".replace('.', '__').replace('-', '_').replace(' ', '_')
    
    def get_metadata_hash(self) -> str:
        """è®¡ç®—å…ƒæ•°æ®å“ˆå¸Œå€¼ï¼Œç”¨äºå¢é‡æ›´æ–°åˆ¤æ–­"""
        hash_data = {
            'schema_name': self.schema_name,
            'table_name': self.table_name,
            'fields': [f.to_dict() for f in self.fields],
            'comment': self.original_comment
        }
        return hashlib.md5(json.dumps(hash_data, sort_keys=True).encode()).hexdigest()

@dataclass
class ProcessingResult:
    """å·¥å…·å¤„ç†ç»“æœæ ‡å‡†ç»“æ„"""
    success: bool
    data: Optional[Any] = None
    error_message: Optional[str] = None
    warnings: List[str] = field(default_factory=list)
    execution_time: Optional[float] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    
    def add_warning(self, warning: str):
        """æ·»åŠ è­¦å‘Šä¿¡æ¯"""
        self.warnings.append(warning)
    
    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸æ ¼å¼"""
        return {
            'success': self.success,
            'data': self.data,
            'error_message': self.error_message,
            'warnings': self.warnings,
            'execution_time': self.execution_time,
            'metadata': self.metadata
        }

@dataclass
class TableProcessingContext:
    """è¡¨å¤„ç†ä¸Šä¸‹æ–‡"""
    table_metadata: TableMetadata
    business_context: str
    output_dir: str
    pipeline: str
    vn: Any  # vannaå®ä¾‹
    file_manager: Any
    current_step: str = "initialized"
    step_results: Dict[str, ProcessingResult] = field(default_factory=dict)
    start_time: Optional[float] = None
    
    def update_step(self, step_name: str, result: ProcessingResult):
        """æ›´æ–°æ­¥éª¤ç»“æœ"""
        self.current_step = step_name
        self.step_results[step_name] = result
```

## 3. å·¥å…·æ³¨å†Œä¸ç®¡ç†ç³»ç»Ÿ

### 3.1 åŸºç¡€å·¥å…·ç±» (`tools/base.py`)

```python
import asyncio
import time
import logging
from abc import ABC, abstractmethod
from typing import Dict, Any, Optional, Type
from utils.data_structures import ProcessingResult, TableProcessingContext

class ToolRegistry:
    """å·¥å…·æ³¨å†Œç®¡ç†å™¨"""
    _tools: Dict[str, Type['BaseTool']] = {}
    _instances: Dict[str, 'BaseTool'] = {}
    
    @classmethod
    def register(cls, name: str):
        """è£…é¥°å™¨ï¼šæ³¨å†Œå·¥å…·"""
        def decorator(tool_class: Type['BaseTool']):
            cls._tools[name] = tool_class
            logging.debug(f"æ³¨å†Œå·¥å…·: {name} -> {tool_class.__name__}")
            return tool_class
        return decorator
    
    @classmethod
    def get_tool(cls, name: str, **kwargs) -> 'BaseTool':
        """è·å–å·¥å…·å®ä¾‹ï¼Œæ”¯æŒå•ä¾‹æ¨¡å¼"""
        if name not in cls._instances:
            if name not in cls._tools:
                raise ValueError(f"å·¥å…· '{name}' æœªæ³¨å†Œ")
            
            tool_class = cls._tools[name]
            
            # è‡ªåŠ¨æ³¨å…¥vannaå®ä¾‹åˆ°éœ€è¦LLMçš„å·¥å…·
            if hasattr(tool_class, 'needs_llm') and tool_class.needs_llm:
                from core.vanna_llm_factory import create_vanna_instance
                kwargs['vn'] = create_vanna_instance()
                logging.debug(f"ä¸ºå·¥å…· {name} æ³¨å…¥LLMå®ä¾‹")
            
            cls._instances[name] = tool_class(**kwargs)
        
        return cls._instances[name]
    
    @classmethod
    def list_tools(cls) -> List[str]:
        """åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„å·¥å…·"""
        return list(cls._tools.keys())
    
    @classmethod
    def clear_instances(cls):
        """æ¸…é™¤æ‰€æœ‰å·¥å…·å®ä¾‹ï¼ˆç”¨äºæµ‹è¯•ï¼‰"""
        cls._instances.clear()

class BaseTool(ABC):
    """å·¥å…·åŸºç±»"""
    
    needs_llm: bool = False  # æ˜¯å¦éœ€è¦LLMå®ä¾‹
    tool_name: str = ""      # å·¥å…·åç§°
    
    def __init__(self, **kwargs):
        self.logger = logging.getLogger(f"schema_tools.{self.__class__.__name__}")
        
        # å¦‚æœå·¥å…·éœ€è¦LLMï¼Œæ£€æŸ¥æ˜¯å¦å·²æ³¨å…¥
        if self.needs_llm and 'vn' not in kwargs:
            raise ValueError(f"å·¥å…· {self.__class__.__name__} éœ€è¦LLMå®ä¾‹ä½†æœªæä¾›")
        
        # å­˜å‚¨vannaå®ä¾‹
        if 'vn' in kwargs:
            self.vn = kwargs['vn']
    
    @abstractmethod
    async def execute(self, context: TableProcessingContext) -> ProcessingResult:
        """
        æ‰§è¡Œå·¥å…·é€»è¾‘
        Args:
            context: è¡¨å¤„ç†ä¸Šä¸‹æ–‡
        Returns:
            ProcessingResult: å¤„ç†ç»“æœ
        """
        pass
    
    async def _execute_with_timing(self, context: TableProcessingContext) -> ProcessingResult:
        """å¸¦è®¡æ—¶çš„æ‰§è¡ŒåŒ…è£…å™¨"""
        start_time = time.time()
        
        try:
            self.logger.info(f"å¼€å§‹æ‰§è¡Œå·¥å…·: {self.tool_name}")
            result = await self.execute(context)
            execution_time = time.time() - start_time
            result.execution_time = execution_time
            
            if result.success:
                self.logger.info(f"å·¥å…· {self.tool_name} æ‰§è¡ŒæˆåŠŸï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
            else:
                self.logger.error(f"å·¥å…· {self.tool_name} æ‰§è¡Œå¤±è´¥: {result.error_message}")
            
            return result
            
        except Exception as e:
            execution_time = time.time() - start_time
            self.logger.exception(f"å·¥å…· {self.tool_name} æ‰§è¡Œå¼‚å¸¸")
            
            return ProcessingResult(
                success=False,
                error_message=f"å·¥å…·æ‰§è¡Œå¼‚å¸¸: {str(e)}",
                execution_time=execution_time
            )
    
    def validate_input(self, context: TableProcessingContext) -> bool:
        """è¾“å…¥éªŒè¯ï¼ˆå­ç±»å¯é‡å†™ï¼‰"""
        return context.table_metadata is not None
```

### 3.2 Pipelineæ‰§è¡Œå™¨ (`training_data_agent.py` çš„ä¸€éƒ¨åˆ†)

```python
class PipelineExecutor:
    """å¤„ç†é“¾æ‰§è¡Œå™¨"""
    
    def __init__(self, pipeline_config: Dict[str, List[str]]):
        self.pipeline_config = pipeline_config
        self.logger = logging.getLogger("schema_tools.PipelineExecutor")
    
    async def execute_pipeline(self, pipeline_name: str, context: TableProcessingContext) -> Dict[str, ProcessingResult]:
        """æ‰§è¡ŒæŒ‡å®šçš„å¤„ç†é“¾"""
        if pipeline_name not in self.pipeline_config:
            raise ValueError(f"æœªçŸ¥çš„å¤„ç†é“¾: {pipeline_name}")
        
        steps = self.pipeline_config[pipeline_name]
        results = {}
        
        self.logger.info(f"å¼€å§‹æ‰§è¡Œå¤„ç†é“¾ '{pipeline_name}': {' -> '.join(steps)}")
        
        for step_name in steps:
            try:
                tool = ToolRegistry.get_tool(step_name)
                
                # éªŒè¯è¾“å…¥
                if not tool.validate_input(context):
                    result = ProcessingResult(
                        success=False,
                        error_message=f"å·¥å…· {step_name} è¾“å…¥éªŒè¯å¤±è´¥"
                    )
                else:
                    result = await tool._execute_with_timing(context)
                
                results[step_name] = result
                context.update_step(step_name, result)
                
                # å¦‚æœæ­¥éª¤å¤±è´¥ä¸”ä¸å…è®¸ç»§ç»­ï¼Œåˆ™åœæ­¢
                if not result.success:
                    from config import SCHEMA_TOOLS_CONFIG
                    if not SCHEMA_TOOLS_CONFIG["continue_on_error"]:
                        self.logger.error(f"æ­¥éª¤ {step_name} å¤±è´¥ï¼Œåœæ­¢å¤„ç†é“¾æ‰§è¡Œ")
                        break
                    else:
                        self.logger.warning(f"æ­¥éª¤ {step_name} å¤±è´¥ï¼Œç»§ç»­æ‰§è¡Œä¸‹ä¸€æ­¥")
                
            except Exception as e:
                self.logger.exception(f"æ‰§è¡Œæ­¥éª¤ {step_name} æ—¶å‘ç”Ÿå¼‚å¸¸")
                results[step_name] = ProcessingResult(
                    success=False,
                    error_message=f"æ­¥éª¤æ‰§è¡Œå¼‚å¸¸: {str(e)}"
                )
                break
        
        return results
```

## 4. æ ¸å¿ƒå·¥å…·å®ç°

### 4.1 æ•°æ®åº“æ£€æŸ¥å·¥å…· (`tools/database_inspector.py`)

```python
import asyncio
import asyncpg
from typing import List, Dict, Any, Optional
from tools.base import BaseTool, ToolRegistry
from utils.data_structures import ProcessingResult, TableProcessingContext, FieldInfo, TableMetadata

@ToolRegistry.register("database_inspector")
class DatabaseInspectorTool(BaseTool):
    """æ•°æ®åº“å…ƒæ•°æ®æ£€æŸ¥å·¥å…·"""
    
    needs_llm = False
    tool_name = "æ•°æ®åº“æ£€æŸ¥å™¨"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.db_connection = kwargs.get('db_connection')
        self.connection_pool = None
    
    async def execute(self, context: TableProcessingContext) -> ProcessingResult:
        """æ‰§è¡Œæ•°æ®åº“å…ƒæ•°æ®æ£€æŸ¥"""
        try:
            # å»ºç«‹æ•°æ®åº“è¿æ¥
            if not self.connection_pool:
                await self._create_connection_pool()
            
            table_name = context.table_metadata.table_name
            schema_name = context.table_metadata.schema_name
            
            # è·å–è¡¨çš„åŸºæœ¬ä¿¡æ¯
            table_info = await self._get_table_info(schema_name, table_name)
            if not table_info:
                return ProcessingResult(
                    success=False,
                    error_message=f"è¡¨ {schema_name}.{table_name} ä¸å­˜åœ¨æˆ–æ— æƒé™è®¿é—®"
                )
            
            # è·å–å­—æ®µä¿¡æ¯
            fields = await self._get_table_fields(schema_name, table_name)
            
            # è·å–è¡¨æ³¨é‡Š
            table_comment = await self._get_table_comment(schema_name, table_name)
            
            # è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯
            stats = await self._get_table_statistics(schema_name, table_name)
            
            # æ›´æ–°è¡¨å…ƒæ•°æ®
            context.table_metadata.original_comment = table_comment
            context.table_metadata.comment = table_comment
            context.table_metadata.fields = fields
            context.table_metadata.row_count = stats.get('row_count')
            context.table_metadata.table_size = stats.get('table_size')
            
            return ProcessingResult(
                success=True,
                data={
                    'fields_count': len(fields),
                    'table_comment': table_comment,
                    'row_count': stats.get('row_count'),
                    'table_size': stats.get('table_size')
                },
                metadata={'tool': self.tool_name}
            )
            
        except Exception as e:
            self.logger.exception(f"æ•°æ®åº“æ£€æŸ¥å¤±è´¥")
            return ProcessingResult(
                success=False,
                error_message=f"æ•°æ®åº“æ£€æŸ¥å¤±è´¥: {str(e)}"
            )
    
    async def _create_connection_pool(self):
        """åˆ›å»ºæ•°æ®åº“è¿æ¥æ± """
        try:
            self.connection_pool = await asyncpg.create_pool(
                self.db_connection,
                min_size=1,
                max_size=5,
                command_timeout=30
            )
            self.logger.info("æ•°æ®åº“è¿æ¥æ± åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            self.logger.error(f"åˆ›å»ºæ•°æ®åº“è¿æ¥æ± å¤±è´¥: {e}")
            raise
    
    async def _get_table_info(self, schema_name: str, table_name: str) -> Optional[Dict]:
        """è·å–è¡¨åŸºæœ¬ä¿¡æ¯"""
        query = """
        SELECT schemaname, tablename, tableowner, tablespace, hasindexes, hasrules, hastriggers
        FROM pg_tables 
        WHERE schemaname = $1 AND tablename = $2
        """
        async with self.connection_pool.acquire() as conn:
            result = await conn.fetchrow(query, schema_name, table_name)
            return dict(result) if result else None
    
    async def _get_table_fields(self, schema_name: str, table_name: str) -> List[FieldInfo]:
        """è·å–è¡¨å­—æ®µä¿¡æ¯"""
        query = """
        SELECT 
            c.column_name,
            c.data_type,
            c.is_nullable,
            c.column_default,
            c.character_maximum_length,
            c.numeric_precision,
            c.numeric_scale,
            pd.description as column_comment,
            CASE WHEN pk.column_name IS NOT NULL THEN true ELSE false END as is_primary_key,
            CASE WHEN fk.column_name IS NOT NULL THEN true ELSE false END as is_foreign_key
        FROM information_schema.columns c
        LEFT JOIN pg_description pd ON pd.objsubid = c.ordinal_position 
            AND pd.objoid = (
                SELECT oid FROM pg_class 
                WHERE relname = c.table_name 
                AND relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = c.table_schema)
            )
        LEFT JOIN (
            SELECT ku.column_name
            FROM information_schema.table_constraints tc
            JOIN information_schema.key_column_usage ku ON tc.constraint_name = ku.constraint_name
            WHERE tc.table_schema = $1 AND tc.table_name = $2 AND tc.constraint_type = 'PRIMARY KEY'
        ) pk ON pk.column_name = c.column_name
        LEFT JOIN (
            SELECT ku.column_name
            FROM information_schema.table_constraints tc
            JOIN information_schema.key_column_usage ku ON tc.constraint_name = ku.constraint_name
            WHERE tc.table_schema = $1 AND tc.table_name = $2 AND tc.constraint_type = 'FOREIGN KEY'
        ) fk ON fk.column_name = c.column_name
        WHERE c.table_schema = $1 AND c.table_name = $2
        ORDER BY c.ordinal_position
        """
        
        fields = []
        async with self.connection_pool.acquire() as conn:
            rows = await conn.fetch(query, schema_name, table_name)
            
            for row in rows:
                field = FieldInfo(
                    name=row['column_name'],
                    type=row['data_type'],
                    nullable=row['is_nullable'] == 'YES',
                    default_value=row['column_default'],
                    original_comment=row['column_comment'],
                    comment=row['column_comment'],
                    is_primary_key=row['is_primary_key'],
                    is_foreign_key=row['is_foreign_key'],
                    max_length=row['character_maximum_length'],
                    precision=row['numeric_precision'],
                    scale=row['numeric_scale']
                )
                fields.append(field)
        
        return fields
    
    async def _get_table_comment(self, schema_name: str, table_name: str) -> Optional[str]:
        """è·å–è¡¨æ³¨é‡Š"""
        query = """
        SELECT obj_description(oid) as table_comment
        FROM pg_class 
        WHERE relname = $2 
        AND relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = $1)
        """
        async with self.connection_pool.acquire() as conn:
            result = await conn.fetchval(query, schema_name, table_name)
            return result
    
    async def _get_table_statistics(self, schema_name: str, table_name: str) -> Dict[str, Any]:
        """è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯"""
        stats_query = """
        SELECT 
            schemaname,
            tablename,
            attname,
            n_distinct,
            most_common_vals,
            most_common_freqs,
            histogram_bounds
        FROM pg_stats 
        WHERE schemaname = $1 AND tablename = $2
        """
        
        size_query = """
        SELECT pg_size_pretty(pg_total_relation_size($1)) as table_size,
               pg_relation_size($1) as table_size_bytes
        """
        
        count_query = f"SELECT COUNT(*) as row_count FROM {schema_name}.{table_name}"
        
        stats = {}
        async with self.connection_pool.acquire() as conn:
            try:
                # è·å–è¡Œæ•°
                row_count = await conn.fetchval(count_query)
                stats['row_count'] = row_count
                
                # è·å–è¡¨å¤§å°
                table_oid = await conn.fetchval(
                    "SELECT oid FROM pg_class WHERE relname = $1 AND relnamespace = (SELECT oid FROM pg_namespace WHERE nspname = $2)",
                    table_name, schema_name
                )
                if table_oid:
                    size_result = await conn.fetchrow(size_query, table_oid)
                    stats['table_size'] = size_result['table_size']
                    stats['table_size_bytes'] = size_result['table_size_bytes']
                
            except Exception as e:
                self.logger.warning(f"è·å–è¡¨ç»Ÿè®¡ä¿¡æ¯å¤±è´¥: {e}")
        
        return stats
```

### 4.2 æ•°æ®é‡‡æ ·å·¥å…· (`tools/data_sampler.py`)

```python
import random
from typing import List, Dict, Any
from tools.base import BaseTool, ToolRegistry
from utils.data_structures import ProcessingResult, TableProcessingContext

@ToolRegistry.register("data_sampler")
class DataSamplerTool(BaseTool):
    """æ•°æ®é‡‡æ ·å·¥å…·"""
    
    needs_llm = False
    tool_name = "æ•°æ®é‡‡æ ·å™¨"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.db_connection = kwargs.get('db_connection')
    
    async def execute(self, context: TableProcessingContext) -> ProcessingResult:
        """æ‰§è¡Œæ•°æ®é‡‡æ ·"""
        try:
            from config import SCHEMA_TOOLS_CONFIG
            
            table_metadata = context.table_metadata
            sample_limit = SCHEMA_TOOLS_CONFIG["sample_data_limit"]
            large_table_threshold = SCHEMA_TOOLS_CONFIG["large_table_threshold"]
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºå¤§è¡¨ï¼Œä½¿ç”¨ä¸åŒçš„é‡‡æ ·ç­–ç•¥
            if table_metadata.row_count and table_metadata.row_count > large_table_threshold:
                sample_data = await self._smart_sample_large_table(table_metadata, sample_limit)
                self.logger.info(f"å¤§è¡¨ {table_metadata.full_name} ä½¿ç”¨æ™ºèƒ½é‡‡æ ·ç­–ç•¥")
            else:
                sample_data = await self._simple_sample(table_metadata, sample_limit)
            
            # æ›´æ–°ä¸Šä¸‹æ–‡ä¸­çš„é‡‡æ ·æ•°æ®
            context.table_metadata.sample_data = sample_data
            
            return ProcessingResult(
                success=True,
                data={
                    'sample_count': len(sample_data),
                    'sampling_strategy': 'smart' if table_metadata.row_count and table_metadata.row_count > large_table_threshold else 'simple'
                },
                metadata={'tool': self.tool_name}
            )
            
        except Exception as e:
            self.logger.exception(f"æ•°æ®é‡‡æ ·å¤±è´¥")
            return ProcessingResult(
                success=False,
                error_message=f"æ•°æ®é‡‡æ ·å¤±è´¥: {str(e)}"
            )
    
    async def _simple_sample(self, table_metadata: TableMetadata, limit: int) -> List[Dict[str, Any]]:
        """ç®€å•é‡‡æ ·ç­–ç•¥"""
        from tools.database_inspector import DatabaseInspectorTool
        
        # å¤ç”¨æ•°æ®åº“æ£€æŸ¥å·¥å…·çš„è¿æ¥
        inspector = ToolRegistry.get_tool("database_inspector")
        
        query = f"SELECT * FROM {table_metadata.full_name} LIMIT {limit}"
        
        async with inspector.connection_pool.acquire() as conn:
            rows = await conn.fetch(query)
            return [dict(row) for row in rows]
    
    async def _smart_sample_large_table(self, table_metadata: TableMetadata, limit: int) -> List[Dict[str, Any]]:
        """æ™ºèƒ½é‡‡æ ·ç­–ç•¥ï¼ˆç”¨äºå¤§è¡¨ï¼‰"""
        from tools.database_inspector import DatabaseInspectorTool
        
        inspector = ToolRegistry.get_tool("database_inspector")
        samples_per_section = max(1, limit // 3)
        
        samples = []
        
        async with inspector.connection_pool.acquire() as conn:
            # 1. å‰Nè¡Œé‡‡æ ·
            front_query = f"SELECT * FROM {table_metadata.full_name} LIMIT {samples_per_section}"
            front_rows = await conn.fetch(front_query)
            samples.extend([dict(row) for row in front_rows])
            
            # 2. éšæœºä¸­é—´é‡‡æ ·ï¼ˆä½¿ç”¨TABLESAMPLEï¼‰
            if table_metadata.row_count > samples_per_section * 2:
                try:
                    # è®¡ç®—é‡‡æ ·ç™¾åˆ†æ¯”
                    sample_percent = min(1.0, (samples_per_section * 100.0) / table_metadata.row_count)
                    middle_query = f"""
                    SELECT * FROM {table_metadata.full_name} 
                    TABLESAMPLE SYSTEM({sample_percent}) 
                    LIMIT {samples_per_section}
                    """
                    middle_rows = await conn.fetch(middle_query)
                    samples.extend([dict(row) for row in middle_rows])
                except Exception as e:
                    self.logger.warning(f"TABLESAMPLEé‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨OFFSETé‡‡æ ·: {e}")
                    # å›é€€åˆ°OFFSETé‡‡æ ·
                    offset = random.randint(samples_per_section, table_metadata.row_count - samples_per_section)
                    offset_query = f"SELECT * FROM {table_metadata.full_name} OFFSET {offset} LIMIT {samples_per_section}"
                    offset_rows = await conn.fetch(offset_query)
                    samples.extend([dict(row) for row in offset_rows])
            
            # 3. åNè¡Œé‡‡æ ·
            remaining = limit - len(samples)
            if remaining > 0:
                # ä½¿ç”¨ORDER BY ... DESCæ¥è·å–æœ€åçš„è¡Œ
                tail_query = f"""
                SELECT * FROM (
                    SELECT *, ROW_NUMBER() OVER() as rn 
                    FROM {table_metadata.full_name}
                ) sub 
                WHERE sub.rn > (SELECT COUNT(*) FROM {table_metadata.full_name}) - {remaining}
                ORDER BY sub.rn
                """
                try:
                    tail_rows = await conn.fetch(tail_query)
                    # ç§»é™¤ROW_NUMBERåˆ—
                    for row in tail_rows:
                        row_dict = dict(row)
                        row_dict.pop('rn', None)
                        samples.append(row_dict)
                except Exception as e:
                    self.logger.warning(f"å°¾éƒ¨é‡‡æ ·å¤±è´¥: {e}")
        
        return samples[:limit]  # ç¡®ä¿ä¸è¶…è¿‡é™åˆ¶
```

### 4.3 LLMæ³¨é‡Šç”Ÿæˆå·¥å…· (`tools/comment_generator.py`)

~~~python
import asyncio
from typing import List, Dict, Any, Tuple
from tools.base import BaseTool, ToolRegistry
from utils.data_structures import ProcessingResult, TableProcessingContext, FieldInfo

@ToolRegistry.register("comment_generator")
class CommentGeneratorTool(BaseTool):
    """LLMæ³¨é‡Šç”Ÿæˆå·¥å…·"""
    
    needs_llm = True
    tool_name = "æ³¨é‡Šç”Ÿæˆå™¨"
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        self.business_context = kwargs.get('business_context', '')
        self.business_dictionary = self._load_business_dictionary()
    
    async def execute(self, context: TableProcessingContext) -> ProcessingResult:
        """æ‰§è¡Œæ³¨é‡Šç”Ÿæˆ"""
        try:
            table_metadata = context.table_metadata
            
            # ç”Ÿæˆè¡¨æ³¨é‡Š
            table_comment_result = await self._generate_table_comment(table_metadata, context.business_context)
            
            # ç”Ÿæˆå­—æ®µæ³¨é‡Šå’Œæšä¸¾å»ºè®®
            field_results = await self._generate_field_comments_and_enums(table_metadata, context.business_context)
            
            # æ›´æ–°è¡¨å…ƒæ•°æ®
            if table_comment_result['success']:
                table_metadata.generated_comment = table_comment_result['comment']
                table_metadata.comment = table_comment_result['comment']
            
            # æ›´æ–°å­—æ®µä¿¡æ¯
            enum_suggestions = []
            for i, field in enumerate(table_metadata.fields):
                if i < len(field_results) and field_results[i]['success']:
                    field.generated_comment = field_results[i]['comment']
                    field.comment = field_results[i]['comment']
                    
                    # å¤„ç†æšä¸¾å»ºè®®
                    if field_results[i].get('is_enum'):
                        field.is_enum = True
                        enum_suggestions.append({
                            'field_name': field.name,
                            'suggested_values': field_results[i].get('enum_values', []),
                            'enum_description': field_results[i].get('enum_description', '')
                        })
            
            # éªŒè¯æšä¸¾å»ºè®®
            if enum_suggestions:
                validated_enums = await self._validate_enum_suggestions(table_metadata, enum_suggestions)
                
                # æ›´æ–°éªŒè¯åçš„æšä¸¾ä¿¡æ¯
                for enum_info in validated_enums:
                    field_name = enum_info['field_name']
                    for field in table_metadata.fields:
                        if field.name == field_name:
                            field.enum_values = enum_info['actual_values']
                            field.enum_description = enum_info['description']
                            break
            
            return ProcessingResult(
                success=True,
                data={
                    'table_comment_generated': table_comment_result['success'],
                    'field_comments_generated': sum(1 for r in field_results if r['success']),
                    'enum_fields_detected': len([f for f in table_metadata.fields if f.is_enum]),
                    'enum_suggestions': enum_suggestions
                },
                metadata={'tool': self.tool_name}
            )
            
        except Exception as e:
            self.logger.exception(f"æ³¨é‡Šç”Ÿæˆå¤±è´¥")
            return ProcessingResult(
                success=False,
                error_message=f"æ³¨é‡Šç”Ÿæˆå¤±è´¥: {str(e)}"
            )
    
    async def _generate_table_comment(self, table_metadata, business_context: str) -> Dict[str, Any]:
        """ç”Ÿæˆè¡¨æ³¨é‡Š"""
        try:
            prompt = self._build_table_comment_prompt(table_metadata, business_context)
            
            # è°ƒç”¨LLM
            response = await self._call_llm_with_retry(prompt)
            
            # è§£æå“åº”
            comment = self._extract_table_comment(response)
            
            return {
                'success': True,
                'comment': comment,
                'original_response': response
            }
            
        except Exception as e:
            self.logger.error(f"è¡¨æ³¨é‡Šç”Ÿæˆå¤±è´¥: {e}")
            return {
                'success': False,
                'comment': table_metadata.original_comment or f"{table_metadata.table_name}è¡¨",
                'error': str(e)
            }
    
    async def _generate_field_comments_and_enums(self, table_metadata, business_context: str) -> List[Dict[str, Any]]:
        """æ‰¹é‡ç”Ÿæˆå­—æ®µæ³¨é‡Šå’Œæšä¸¾å»ºè®®"""
        try:
            # æ„å»ºæ‰¹é‡å¤„ç†çš„æç¤ºè¯
            prompt = self._build_field_batch_prompt(table_metadata, business_context)
            
            # è°ƒç”¨LLM
            response = await self._call_llm_with_retry(prompt)
            
            # è§£ææ‰¹é‡å“åº”
            field_results = self._parse_field_batch_response(response, table_metadata.fields)
            
            return field_results
            
        except Exception as e:
            self.logger.error(f"å­—æ®µæ³¨é‡Šæ‰¹é‡ç”Ÿæˆå¤±è´¥: {e}")
            # è¿”å›é»˜è®¤ç»“æœ
            return [
                {
                    'success': False,
                    'comment': field.original_comment or field.name,
                    'is_enum': False,
                    'error': str(e)
                }
                for field in table_metadata.fields
            ]
    
    def _build_table_comment_prompt(self, table_metadata, business_context: str) -> str:
        """æ„å»ºè¡¨æ³¨é‡Šç”Ÿæˆæç¤ºè¯"""
        # å‡†å¤‡å­—æ®µä¿¡æ¯æ‘˜è¦
        fields_summary = []
        for field in table_metadata.fields[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ªå­—æ®µé¿å…è¿‡é•¿
            field_desc = f"- {field.name} ({field.type})"
            if field.comment:
                field_desc += f": {field.comment}"
            fields_summary.append(field_desc)
        
        # å‡†å¤‡æ ·ä¾‹æ•°æ®æ‘˜è¦
        sample_summary = ""
        if table_metadata.sample_data:
            sample_count = min(3, len(table_metadata.sample_data))
            sample_summary = f"\næ ·ä¾‹æ•°æ®({sample_count}æ¡):\n"
            for i, sample in enumerate(table_metadata.sample_data[:sample_count]):
                sample_str = ", ".join([f"{k}={v}" for k, v in list(sample.items())[:5]])
                sample_summary += f"{i+1}. {sample_str}\n"
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“æ–‡æ¡£ä¸“å®¶ã€‚è¯·æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ä¸ºæ•°æ®åº“è¡¨ç”Ÿæˆç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡æ³¨é‡Šã€‚

ä¸šåŠ¡èƒŒæ™¯: {business_context}
{self.business_dictionary}

è¡¨ä¿¡æ¯:
- è¡¨å: {table_metadata.table_name}
- Schema: {table_metadata.schema_name}
- ç°æœ‰æ³¨é‡Š: {table_metadata.original_comment or "æ— "}
- å­—æ®µæ•°é‡: {len(table_metadata.fields)}
- æ•°æ®è¡Œæ•°: {table_metadata.row_count or "æœªçŸ¥"}

ä¸»è¦å­—æ®µ:
{chr(10).join(fields_summary)}

{sample_summary}

è¯·ç”Ÿæˆä¸€ä¸ªç®€æ´ã€å‡†ç¡®çš„ä¸­æ–‡è¡¨æ³¨é‡Šï¼Œè¦æ±‚:
1. å¦‚æœç°æœ‰æ³¨é‡Šæ˜¯è‹±æ–‡ï¼Œè¯·ç¿»è¯‘ä¸ºä¸­æ–‡å¹¶æ”¹è¿›
2. æ ¹æ®å­—æ®µåç§°å’Œæ ·ä¾‹æ•°æ®æ¨æ–­è¡¨çš„ä¸šåŠ¡ç”¨é€”
3. æ³¨é‡Šé•¿åº¦æ§åˆ¶åœ¨50å­—ä»¥å†…
4. çªå‡ºè¡¨çš„æ ¸å¿ƒä¸šåŠ¡ä»·å€¼

è¡¨æ³¨é‡Š:"""
        
        return prompt
    
    def _build_field_batch_prompt(self, table_metadata, business_context: str) -> str:
        """æ„å»ºå­—æ®µæ‰¹é‡å¤„ç†æç¤ºè¯"""
        # å‡†å¤‡å­—æ®µä¿¡æ¯
        fields_info = []
        sample_values = {}
        
        # æ”¶é›†å­—æ®µçš„æ ·ä¾‹å€¼
        for sample in table_metadata.sample_data[:5]:
            for field_name, value in sample.items():
                if field_name not in sample_values:
                    sample_values[field_name] = []
                if value is not None and len(sample_values[field_name]) < 5:
                    sample_values[field_name].append(str(value))
        
        # æ„å»ºå­—æ®µä¿¡æ¯åˆ—è¡¨
        for field in table_metadata.fields:
            field_info = f"{field.name} ({field.type})"
            if field.original_comment:
                field_info += f" - åŸæ³¨é‡Š: {field.original_comment}"
            
            # æ·»åŠ æ ·ä¾‹å€¼
            if field.name in sample_values and sample_values[field.name]:
                values_str = ", ".join(sample_values[field.name][:3])
                field_info += f" - æ ·ä¾‹å€¼: {values_str}"
            
            fields_info.append(field_info)
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªæ•°æ®åº“æ–‡æ¡£ä¸“å®¶ã€‚è¯·ä¸ºä»¥ä¸‹è¡¨çš„æ‰€æœ‰å­—æ®µç”Ÿæˆä¸­æ–‡æ³¨é‡Šï¼Œå¹¶è¯†åˆ«å¯èƒ½çš„æšä¸¾å­—æ®µã€‚

ä¸šåŠ¡èƒŒæ™¯: {business_context}
{self.business_dictionary}

è¡¨å: {table_metadata.schema_name}.{table_metadata.table_name}
è¡¨æ³¨é‡Š: {table_metadata.comment or "æ— "}

å­—æ®µåˆ—è¡¨:
{chr(10).join([f"{i+1}. {info}" for i, info in enumerate(fields_info)])}

è¯·æŒ‰ä»¥ä¸‹JSONæ ¼å¼è¾“å‡ºæ¯ä¸ªå­—æ®µçš„åˆ†æç»“æœ:
```json
{{
  "fields": [
    {{
      "name": "å­—æ®µå",
      "comment": "ä¸­æ–‡æ³¨é‡Šï¼ˆç®€æ´æ˜ç¡®ï¼Œ15å­—ä»¥å†…ï¼‰",
      "is_enum": true/false,
      "enum_values": ["å€¼1", "å€¼2", "å€¼3"] (å¦‚æœæ˜¯æšä¸¾),
      "enum_description": "æšä¸¾å«ä¹‰è¯´æ˜" (å¦‚æœæ˜¯æšä¸¾)
    }}
  ]
}}
~~~

æ³¨é‡Šç”Ÿæˆè¦æ±‚:

1. å¦‚æœåŸæ³¨é‡Šæ˜¯è‹±æ–‡ï¼Œç¿»è¯‘ä¸ºä¸­æ–‡å¹¶æ”¹è¿›
2. æ ¹æ®å­—æ®µåã€ç±»å‹å’Œæ ·ä¾‹å€¼æ¨æ–­å­—æ®µå«ä¹‰
3. è¯†åˆ«å¯èƒ½çš„æšä¸¾å­—æ®µï¼ˆå¦‚çŠ¶æ€ã€ç±»å‹ã€çº§åˆ«ç­‰ï¼‰
4. æšä¸¾åˆ¤æ–­æ ‡å‡†: VARCHARç±»å‹ + æ ·ä¾‹å€¼é‡å¤åº¦é«˜ + å­—æ®µåæš—ç¤ºåˆ†ç±»
5. æ³¨é‡Šè¦è´´è¿‘{business_context}çš„ä¸šåŠ¡åœºæ™¯

è¯·è¾“å‡ºJSONæ ¼å¼çš„åˆ†æç»“æœ:"""

```
    return prompt

async def _call_llm_with_retry(self, prompt: str, max_retries: int = 3) -> str:
    """å¸¦é‡è¯•çš„LLMè°ƒç”¨"""
    from config import SCHEMA_TOOLS_CONFIG
    
    for attempt in range(max_retries):
        try:
            # ä½¿ç”¨vannaå®ä¾‹è°ƒç”¨LLM
            response = await asyncio.to_thread(self.vn.ask, prompt)
            
            if response and response.strip():
                return response.strip()
            else:
                raise ValueError("LLMè¿”å›ç©ºå“åº”")
                
        except Exception as e:
            self.logger.warning(f"LLMè°ƒç”¨å¤±è´¥ (å°è¯• {attempt + 1}/{max_retries}): {e}")
            if attempt == max_retries - 1:
                raise
            await asyncio.sleep(1)  # ç­‰å¾…1ç§’åé‡è¯•
    
    raise Exception("LLMè°ƒç”¨è¾¾åˆ°æœ€å¤§é‡è¯•æ¬¡æ•°")

def _extract_table_comment(self, llm_response: str) -> str:
    """ä»LLMå“åº”ä¸­æå–è¡¨æ³¨é‡Š"""
    # ç®€å•çš„æ–‡æœ¬æ¸…ç†å’Œæå–é€»è¾‘
    lines = llm_response.strip().split('\n')
    
    # æŸ¥æ‰¾åŒ…å«å®é™…æ³¨é‡Šçš„è¡Œ
    for line in lines:
        line = line.strip()
        if line and not line.startswith('#') and not line.startswith('*'):
            # ç§»é™¤å¯èƒ½çš„å‰ç¼€
            prefixes = ['è¡¨æ³¨é‡Š:', 'æ³¨é‡Š:', 'è¯´æ˜:', 'è¡¨è¯´æ˜:']
            for prefix in prefixes:
                if line.startswith(prefix):
                    line = line[len(prefix):].strip()
            
            if line:
                return line[:200]  # é™åˆ¶é•¿åº¦
    
    return llm_response.strip()[:200]

def _parse_field_batch_response(self, llm_response: str, fields: List[FieldInfo]) -> List[Dict[str, Any]]:
    """è§£æå­—æ®µæ‰¹é‡å¤„ç†å“åº”"""
    import json
    import re
    
    try:
        # å°è¯•æå–JSONéƒ¨åˆ†
        json_match = re.search(r'```json\s*(.*?)\s*```', llm_response, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            # å¦‚æœæ²¡æœ‰ä»£ç å—ï¼Œå°è¯•ç›´æ¥è§£æ
            json_str = llm_response
        
        # è§£æJSON
        parsed_data = json.loads(json_str)
        field_data = parsed_data.get('fields', [])
        
        # æ˜ å°„åˆ°å­—æ®µç»“æœ
        results = []
        for i, field in enumerate(fields):
            if i < len(field_data):
                data = field_data[i]
                results.append({
                    'success': True,
                    'comment': data.get('comment', field.name),
                    'is_enum': data.get('is_enum', False),
                    'enum_values': data.get('enum_values', []),
                    'enum_description': data.get('enum_description', '')
                })
            else:
                # é»˜è®¤ç»“æœ
                results.append({
                    'success': False,
                    'comment': field.original_comment or field.name,
                    'is_enum': False
                })
        
        return results
        
    except Exception as e:
        self.logger.error(f"è§£æå­—æ®µæ‰¹é‡å“åº”å¤±è´¥: {e}")
        # è¿”å›é»˜è®¤ç»“æœ
        return [
            {
                'success': False,
                'comment': field.original_comment or field.name,
                'is_enum': False,
                'error': str(e)
            }
            for field in fields
        ]

async def _validate_enum_suggestions(self, table_metadata, enum_suggestions: List[Dict]) -> List[Dict]:
    """éªŒè¯æšä¸¾å»ºè®®"""
    from tools.database_inspector import DatabaseInspectorTool
    from config import SCHEMA_TOOLS_CONFIG
    
    validated_enums = []
    inspector = ToolRegistry.get_tool("database_inspector")
    sample_limit = SCHEMA_TOOLS_CONFIG["enum_detection_sample_limit"]
    
    for enum_info in enum_suggestions:
        field_name = enum_info['field_name']
        
        try:
            # æŸ¥è¯¢å­—æ®µçš„æ‰€æœ‰ä¸åŒå€¼
            query = f"""
            SELECT DISTINCT {field_name} as value, COUNT(*) as count
            FROM {table_metadata.full_name}
            WHERE {field_name} IS NOT NULL
            GROUP BY {field_name}
            ORDER BY count DESC
            LIMIT {sample_limit}
            """
            
            async with inspector.connection_pool.acquire() as conn:
                rows = await conn.fetch(query)
                
                actual_values = [str(row['value']) for row in rows]
                
                # éªŒè¯æ˜¯å¦çœŸçš„æ˜¯æšä¸¾ï¼ˆä¸åŒå€¼æ•°é‡åˆç†ï¼‰
                max_enum_values = SCHEMA_TOOLS_CONFIG["enum_max_distinct_values"]
                if len(actual_values) <= max_enum_values:
                    validated_enums.append({
                        'field_name': field_name,
                        'actual_values': actual_values,
                        'suggested_values': enum_info['suggested_values'],
                        'description': enum_info['enum_description'],
                        'value_counts': [(row['value'], row['count']) for row in rows]
                    })
                    self.logger.info(f"ç¡®è®¤å­—æ®µ {field_name} ä¸ºæšä¸¾ç±»å‹ï¼ŒåŒ…å« {len(actual_values)} ä¸ªå€¼")
                else:
                    self.logger.info(f"å­—æ®µ {field_name} ä¸åŒå€¼è¿‡å¤š({len(actual_values)})ï¼Œä¸è®¤ä¸ºæ˜¯æšä¸¾")
                    
        except Exception as e:
            self.logger.warning(f"éªŒè¯å­—æ®µ {field_name} çš„æšä¸¾å»ºè®®å¤±è´¥: {e}")
    
    return validated_enums

def _load_business_dictionary(self) -> str:
    """åŠ è½½ä¸šåŠ¡è¯å…¸"""
    try:
        import os
        dict_file = os.path.join(os.path.dirname(__file__), '..', 'prompts', 'business_dictionary.txt')
        if os.path.exists(dict_file):
            with open(dict_file, 'r', encoding='utf-8') as f:
                content = f.read().strip()
                return f"\nä¸šåŠ¡è¯å…¸:\n{content}\n" if content else ""
        return ""
    except Exception as e:
        self.logger.warning(f"åŠ è½½ä¸šåŠ¡è¯å…¸å¤±è´¥: {e}")
        return ""
## 5. ä¸»AI Agentå®ç°

### 5.1 ä¸»Agentæ ¸å¿ƒä»£ç  (`training_data_agent.py`)

```python
import asyncio
import time
import logging
import os
from typing import List, Dict, Any, Optional
from pathlib import Path

from tools.base import ToolRegistry, PipelineExecutor
from utils.data_structures import TableMetadata, TableProcessingContext, ProcessingResult
from utils.file_manager import FileNameManager
from utils.system_filter import SystemTableFilter
from utils.permission_checker import DatabasePermissionChecker
from utils.table_parser import TableListParser
from utils.logger import setup_logging

class SchemaTrainingDataAgent:
    """Schemaè®­ç»ƒæ•°æ®ç”ŸæˆAI Agent"""
    
    def __init__(self, 
                 db_connection: str,
                 table_list_file: str,
                 business_context: str = None,
                 output_dir: str = None,
                 pipeline: str = "full"):
        
        self.db_connection = db_connection
        self.table_list_file = table_list_file
        self.business_context = business_context or "æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ"
        self.pipeline = pipeline
        
        # é…ç½®ç®¡ç†
        from config import SCHEMA_TOOLS_CONFIG
        self.config = SCHEMA_TOOLS_CONFIG
        self.output_dir = output_dir or self.config["output_directory"]
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.file_manager = FileNameManager(self.output_dir)
        self.system_filter = SystemTableFilter()
        self.table_parser = TableListParser()
        self.pipeline_executor = PipelineExecutor(self.config["available_pipelines"])
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tables': 0,
            'processed_tables': 0,
            'failed_tables': 0,
            'skipped_tables': 0,
            'start_time': None,
            'end_time': None
        }
        
        self.failed_tables = []
        self.logger = logging.getLogger("schema_tools.Agent")
    
    async def generate_training_data(self) -> Dict[str, Any]:
        """ä¸»å…¥å£ï¼šç”Ÿæˆè®­ç»ƒæ•°æ®"""
        try:
            self.stats['start_time'] = time.time()
            self.logger.info("ğŸš€ å¼€å§‹ç”ŸæˆSchemaè®­ç»ƒæ•°æ®")
            
            # 1. åˆå§‹åŒ–
            await self._initialize()
            
            # 2. æ£€æŸ¥æ•°æ®åº“æƒé™
            await self._check_database_permissions()
            
            # 3. è§£æè¡¨æ¸…å•
            tables = await self._parse_table_list()
            
            # 4. è¿‡æ»¤ç³»ç»Ÿè¡¨
            user_tables = self._filter_system_tables(tables)
            
            # 5. å¹¶å‘å¤„ç†è¡¨
            results = await self._process_tables_concurrently(user_tables)
            
            # 6. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            report = self._generate_summary_report(results)
            
            self.stats['end_time'] = time.time()
            self.logger.info("âœ… Schemaè®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ")
            
            return report
            
        except Exception as e:
            self.stats['end_time'] = time.time()
            self.logger.exception("âŒ Schemaè®­ç»ƒæ•°æ®ç”Ÿæˆå¤±è´¥")
            raise
    
    async def _initialize(self):
        """åˆå§‹åŒ–Agent"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        if self.config["create_subdirectories"]:
            os.makedirs(os.path.join(self.output_dir, "ddl"), exist_ok=True)
            os.makedirs(os.path.join(self.output_dir, "docs"), exist_ok=True)
            os.makedirs(os.path.join(self.output_dir, "logs"), exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®åº“å·¥å…·
        database_tool = ToolRegistry.get_tool("database_inspector", db_connection=self.db_connection)
        await database_tool._create_connection_pool()
        
        self.logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    async def _check_database_permissions(self):
        """æ£€æŸ¥æ•°æ®åº“æƒé™"""
        if not self.config["check_permissions"]:
            return
        
        inspector = ToolRegistry.get_tool("database_inspector")
        checker = DatabasePermissionChecker(inspector)
        
        permissions = await checker.check_permissions()
        
        if not permissions['connect']:
            raise Exception("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“")
        
        if self.config["require_select_permission"] and not permissions['select_data']:
            if not self.config["allow_readonly_database"]:
                raise Exception("æ•°æ®åº“æŸ¥è¯¢æƒé™ä¸è¶³")
            else:
                self.logger.warning("æ•°æ®åº“ä¸ºåªè¯»æˆ–æƒé™å—é™ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—å½±å“")
        
        self.logger.info(f"æ•°æ®åº“æƒé™æ£€æŸ¥å®Œæˆ: {permissions}")
    
    async def _parse_table_list(self) -> List[str]:
        """è§£æè¡¨æ¸…å•æ–‡ä»¶"""
        tables = self.table_parser.parse_file(self.table_list_file)
        self.stats['total_tables'] = len(tables)
        self.logger.info(f"ğŸ“‹ ä»æ¸…å•æ–‡ä»¶è¯»å–åˆ° {len(tables)} ä¸ªè¡¨")
        return tables
    
    def _filter_system_tables(self, tables: List[str]) -> List[str]:
        """è¿‡æ»¤ç³»ç»Ÿè¡¨"""
        if not self.config["filter_system_tables"]:
            return tables
        
        user_tables = self.system_filter.filter_user_tables(tables)
        filtered_count = len(tables) - len(user_tables)
        
        if filtered_count > 0:
            self.logger.info(f"ğŸ” è¿‡æ»¤äº† {filtered_count} ä¸ªç³»ç»Ÿè¡¨ï¼Œä¿ç•™ {len(user_tables)} ä¸ªç”¨æˆ·è¡¨")
            self.stats['skipped_tables'] += filtered_count
        
        return user_tables
    
    async def _process_tables_concurrently(self, tables: List[str]) -> List[Dict[str, Any]]:
        """å¹¶å‘å¤„ç†è¡¨"""
        max_concurrent = self.config["max_concurrent_tables"]
        semaphore = asyncio.Semaphore(max_concurrent)
        
        self.logger.info(f"ğŸ”„ å¼€å§‹å¹¶å‘å¤„ç† {len(tables)} ä¸ªè¡¨ (æœ€å¤§å¹¶å‘: {max_concurrent})")
        
        # åˆ›å»ºä»»åŠ¡
        tasks = [
            self._process_single_table_with_semaphore(semaphore, table_spec)
            for table_spec in tables
        ]
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if isinstance(r, dict) and r.get('success', False))
        failed = len(results) - successful
        
        self.stats['processed_tables'] = successful
        self.stats['failed_tables'] = failed
        
        self.logger.info(f"ğŸ“Š å¤„ç†å®Œæˆ: æˆåŠŸ {successful} ä¸ªï¼Œå¤±è´¥ {failed} ä¸ª")
        
        return [r for r in results if isinstance(r, dict)]
    
    async def _process_single_table_with_semaphore(self, semaphore: asyncio.Semaphore, table_spec: str) -> Dict[str, Any]:
        """å¸¦ä¿¡å·é‡çš„å•è¡¨å¤„ç†"""
        async with semaphore:
            return await self._process_single_table(table_spec)
    
    async def _process_single_table(self, table_spec: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªè¡¨"""
        start_time = time.time()
        
        try:
            # è§£æè¡¨å
            if '.' in table_spec:
                schema_name, table_name = table_spec.split('.', 1)
            else:
                schema_name, table_name = 'public', table_spec
            
            full_name = f"{schema_name}.{table_name}"
            self.logger.info(f"ğŸ” å¼€å§‹å¤„ç†è¡¨: {full_name}")
            
            # åˆ›å»ºè¡¨å…ƒæ•°æ®
            table_metadata = TableMetadata(
                schema_name=schema_name,
                table_name=table_name,
                full_name=full_name
            )
            
            # åˆ›å»ºå¤„ç†ä¸Šä¸‹æ–‡
            context = TableProcessingContext(
                table_metadata=table_metadata,
                business_context=self.business_context,
                output_dir=self.output_dir,
                pipeline=self.pipeline,
                vn=None,  # å°†åœ¨å·¥å…·ä¸­æ³¨å…¥
                file_manager=self.file_manager,
                start_time=start_time
            )
            
            # æ‰§è¡Œå¤„ç†é“¾
            step_results = await self.pipeline_executor.execute_pipeline(self.pipeline, context)
            
            # è®¡ç®—æ€»ä½“æˆåŠŸçŠ¶æ€
            success = all(result.success for result in step_results.values())
            
            execution_time = time.time() - start_time
            
            if success:
                self.logger.info(f"âœ… è¡¨ {full_name} å¤„ç†æˆåŠŸï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
            else:
                self.logger.error(f"âŒ è¡¨ {full_name} å¤„ç†å¤±è´¥ï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
                self.failed_tables.append(full_name)
            
            return {
                'success': success,
                'table_name': full_name,
                'execution_time': execution_time,
                'step_results': {k: v.to_dict() for k, v in step_results.items()},
                'metadata': {
                    'fields_count': len(table_metadata.fields),
                    'row_count': table_metadata.row_count,
                    'enum_fields': len([f for f in table_metadata.fields if f.is_enum])
                }
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"è¡¨ {table_spec} å¤„ç†å¼‚å¸¸: {str(e)}"
            self.logger.exception(error_msg)
            self.failed_tables.append(table_spec)
            
            return {
                'success': False,
                'table_name': table_spec,
                'execution_time': execution_time,
                'error_message': error_msg,
                'step_results': {}
            }
    
    def _generate_summary_report(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        total_time = self.stats['end_time'] - self.stats['start_time']
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_results = [r for r in results if r.get('success', False)]
        failed_results = [r for r in results if not r.get('success', False)]
        
        total_fields = sum(r.get('metadata', {}).get('fields_count', 0) for r in successful_results)
        total_enum_fields = sum(r.get('metadata', {}).get('enum_fields', 0) for r in successful_results)
        
        avg_execution_time = sum(r.get('execution_time', 0) for r in results) / len(results) if results else 0
        
        report = {
            'summary': {
                'total_tables': self.stats['total_tables'],
                'processed_successfully': len(successful_results),
                'failed': len(failed_results),
                'skipped_system_tables': self.stats['skipped_tables'],
                'total_execution_time': total_time,
                'average_table_time': avg_execution_time
            },
            'statistics': {
                'total_fields_processed': total_fields,
                'enum_fields_detected': total_enum_fields,
                'files_generated': len(successful_results) * (2 if self.pipeline == 'full' else 1)
            },
            'failed_tables': self.failed_tables,
            'detailed_results': results,
            'configuration': {
                'pipeline': self.pipeline,
                'business_context': self.business_context,
                'output_directory': self.output_dir,
                'max_concurrent_tables': self.config['max_concurrent_tables']
            }
        }
        
        # è¾“å‡ºæ€»ç»“
        self.logger.info(f"ğŸ“Š å¤„ç†æ€»ç»“:")
        self.logger.info(f"  âœ… æˆåŠŸ: {report['summary']['processed_successfully']} ä¸ªè¡¨")
        self.logger.info(f"  âŒ å¤±è´¥: {report['summary']['failed']} ä¸ªè¡¨")
        self.logger.info(f"  â­ï¸  è·³è¿‡: {report['summary']['skipped_system_tables']} ä¸ªç³»ç»Ÿè¡¨")
        self.logger.info(f"  ğŸ“ ç”Ÿæˆæ–‡ä»¶: {report['statistics']['files_generated']} ä¸ª")
        self.logger.info(f"  ğŸ• æ€»è€—æ—¶: {total_time:.2f} ç§’")
        
        if self.failed_tables:
            self.logger.warning(f"âŒ å¤±è´¥çš„è¡¨: {', '.join(self.failed_tables)}")
        
        return report
    
    async def check_database_permissions(self) -> Dict[str, bool]:
        """æ£€æŸ¥æ•°æ®åº“æƒé™ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰"""
        inspector = ToolRegistry.get_tool("database_inspector", db_connection=self.db_connection)
        checker = DatabasePermissionChecker(inspector)
        return await checker.check_permissions()
```

## 6. å‘½ä»¤è¡Œæ¥å£å®ç°

### 6.1 å‘½ä»¤è¡Œå…¥å£ (`__main__.py`)

```python
import argparse
import asyncio
import sys
import os
import logging
from pathlib import Path

def setup_argument_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    parser = argparse.ArgumentParser(
        description='Schema Tools - è‡ªåŠ¨ç”Ÿæˆæ•°æ®åº“è®­ç»ƒæ•°æ®',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # åŸºæœ¬ä½¿ç”¨
  python -m schema_tools --db-connection "postgresql://user:pass@host:5432/db" --table-list tables.txt
  
  # æŒ‡å®šä¸šåŠ¡ä¸Šä¸‹æ–‡å’Œè¾“å‡ºç›®å½•
  python -m schema_tools --db-connection "..." --table-list tables.txt --business-context "ç”µå•†ç³»ç»Ÿ" --output-dir output
  
  # ä»…ç”ŸæˆDDLæ–‡ä»¶
  python -m schema_tools --db-connection "..." --table-list tables.txt --pipeline ddl_only
  
  # æƒé™æ£€æŸ¥æ¨¡å¼
  python -m schema_tools --db-connection "..." --check-permissions-only
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument(
        '--db-connection',
        required=True,
        help='æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸² (ä¾‹å¦‚: postgresql://user:pass@localhost:5432/dbname)'
    )
    
    # å¯é€‰å‚æ•°
    parser.add_argument(
        '--table-list',
        help='è¡¨æ¸…å•æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--business-context',
        help='ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿°'
    )
    
    parser.add_argument(
        '--business-context-file',
        help='ä¸šåŠ¡ä¸Šä¸‹æ–‡æ–‡ä»¶è·¯å¾„'
    )
    
    parser.add_argument(
        '--output-dir',
        help='è¾“å‡ºç›®å½•è·¯å¾„'
    )
    
    parser.add_argument(
        '--pipeline',
        choices=['full', 'ddl_only', 'analysis_only'],
        help='å¤„ç†é“¾ç±»å‹'
    )
    
    parser.add_argument(
        '--max-concurrent',
        type=int,
        help='æœ€å¤§å¹¶å‘è¡¨æ•°é‡'
    )
    
    # åŠŸèƒ½å¼€å…³
    parser.add_argument(
        '--no-filter-system-tables',
        action='store_true',
        help='ç¦ç”¨ç³»ç»Ÿè¡¨è¿‡æ»¤'
    )
    
    parser.add_argument(
        '--check-permissions-only',
        action='store_true',
        help='ä»…æ£€æŸ¥æ•°æ®åº“æƒé™ï¼Œä¸å¤„ç†è¡¨'
    )
    
    parser.add_argument(
        '--verbose', '-v',
        action='store_true',
        help='å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º'
    )
    
    parser.add_argument(
        '--log-file',
        help='æ—¥å¿—æ–‡ä»¶è·¯å¾„'
    )
    
    return parser

def load_config_with_overrides(args):
    """åŠ è½½é…ç½®å¹¶åº”ç”¨å‘½ä»¤è¡Œè¦†ç›–"""
    from config import SCHEMA_TOOLS_CONFIG
    
    config = SCHEMA_TOOLS_CONFIG.copy()
    
    # å‘½ä»¤è¡Œå‚æ•°è¦†ç›–é…ç½®
    if args.output_dir:
        config["output_directory"] = args.output_dir
    
    if args.pipeline:
        config["default_pipeline"] = args.pipeline
    
    if args.max_concurrent:
        config["max_concurrent_tables"] = args.max_concurrent
    
    if args.no_filter_system_tables:
        config["filter_system_tables"] = False
    
    if args.log_file:
        config["log_file"] = args.log_file
    
    return config

def load_business_context(args):
    """åŠ è½½ä¸šåŠ¡ä¸Šä¸‹æ–‡"""
    if args.business_context_file:
        try:
            with open(args.business_context_file, 'r', encoding='utf-8') as f:
                return f.read().strip()
        except Exception as e:
            print(f"è­¦å‘Š: æ— æ³•è¯»å–ä¸šåŠ¡ä¸Šä¸‹æ–‡æ–‡ä»¶ {args.business_context_file}: {e}")
    
    if args.business_context:
        return args.business_context
    
    from config import SCHEMA_TOOLS_CONFIG
    return SCHEMA_TOOLS_CONFIG.get("default_business_context", "æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ")

async def check_permissions_only(db_connection: str):
    """ä»…æ£€æŸ¥æ•°æ®åº“æƒé™"""
    from training_data_agent import SchemaTrainingDataAgent
    
    print("ğŸ” æ£€æŸ¥æ•°æ®åº“æƒé™...")
    
    try:
        agent = SchemaTrainingDataAgent(
            db_connection=db_connection,
            table_list_file="",  # ä¸éœ€è¦è¡¨æ¸…å•
            business_context=""   # ä¸éœ€è¦ä¸šåŠ¡ä¸Šä¸‹æ–‡
        )
        
        # åˆå§‹åŒ–Agentä»¥å»ºç«‹æ•°æ®åº“è¿æ¥
        await agent._initialize()
        
        # æ£€æŸ¥æƒé™
        permissions = await agent.check_database_permissions()
        
        print("\nğŸ“‹ æƒé™æ£€æŸ¥ç»“æœ:")
        print(f"  âœ… æ•°æ®åº“è¿æ¥: {'å¯ç”¨' if permissions['connect'] else 'ä¸å¯ç”¨'}")
        print(f"  âœ… å…ƒæ•°æ®æŸ¥è¯¢: {'å¯ç”¨' if permissions['select_metadata'] else 'ä¸å¯ç”¨'}")
        print(f"  âœ… æ•°æ®æŸ¥è¯¢: {'å¯ç”¨' if permissions['select_data'] else 'ä¸å¯ç”¨'}")
        print(f"  â„¹ï¸  æ•°æ®åº“ç±»å‹: {'åªè¯»' if permissions['is_readonly'] else 'è¯»å†™'}")
        
        if all(permissions.values()):
            print("\nâœ… æ•°æ®åº“æƒé™æ£€æŸ¥é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹å¤„ç†")
            return True
        else:
            print("\nâŒ æ•°æ®åº“æƒé™ä¸è¶³ï¼Œè¯·æ£€æŸ¥é…ç½®")
            return False
            
    except Exception as e:
        print(f"\nâŒ æƒé™æ£€æŸ¥å¤±è´¥: {e}")
        return False

async def main():
    """ä¸»å…¥å£å‡½æ•°"""
    parser = setup_argument_parser()
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    from utils.logger import setup_logging
    setup_logging(
        verbose=args.verbose,
        log_file=args.log_file
    )
    
    # ä»…æƒé™æ£€æŸ¥æ¨¡å¼
    if args.check_permissions_only:
        success = await check_permissions_only(args.db_connection)
        sys.exit(0 if success else 1)
    
    # éªŒè¯å¿…éœ€å‚æ•°
    if not args.table_list:
        print("é”™è¯¯: éœ€è¦æŒ‡å®š --table-list å‚æ•°")
        parser.print_help()
        sys.exit(1)
    
    if not os.path.exists(args.table_list):
        print(f"é”™è¯¯: è¡¨æ¸…å•æ–‡ä»¶ä¸å­˜åœ¨: {args.table_list}")
        sys.exit(1)
    
    try:
        # åŠ è½½é…ç½®å’Œä¸šåŠ¡ä¸Šä¸‹æ–‡
        config = load_config_with_overrides(args)
        business_context = load_business_context(args)
        
        # åˆ›å»ºAgent
        from training_data_agent import SchemaTrainingDataAgent
        
        agent = SchemaTrainingDataAgent(
            db_connection=args.db_connection,
            table_list_file=args.table_list,
            business_context=business_context,
            output_dir=config["output_directory"],
            pipeline=config["default_pipeline"]
        )
        
        # æ‰§è¡Œç”Ÿæˆ
        print("ğŸš€ å¼€å§‹ç”ŸæˆSchemaè®­ç»ƒæ•°æ®...")
        report = await agent.generate_training_data()
        
        # è¾“å‡ºç»“æœ
        if report['summary']['failed'] == 0:
            print("\nğŸ‰ æ‰€æœ‰è¡¨å¤„ç†æˆåŠŸ!")
        else:
            print(f"\nâš ï¸  å¤„ç†å®Œæˆï¼Œä½†æœ‰ {report['summary']['failed']} ä¸ªè¡¨å¤±è´¥")
        
        print(f"ğŸ“ è¾“å‡ºç›®å½•: {config['output_directory']}")
        
        # å¦‚æœæœ‰å¤±è´¥çš„è¡¨ï¼Œè¿”å›éé›¶é€€å‡ºç 
        sys.exit(1 if report['summary']['failed'] > 0 else 0)
        
    except KeyboardInterrupt:
        print("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
        sys.exit(130)
    except Exception as e:
        print(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main())
```

### 6.2 å®é™…è¾“å‡ºæ ·ä¾‹ï¼ˆåŸºäºé«˜é€Ÿå…¬è·¯æœåŠ¡åŒºä¸šåŠ¡ï¼‰

#### 6.2.1 DDLæ–‡ä»¶è¾“å‡ºæ ·ä¾‹ (`bss_service_area.ddl`)

sql

```sql
-- ä¸­æ–‡å: æœåŠ¡åŒºåŸºç¡€ä¿¡æ¯è¡¨
-- æè¿°: è®°å½•é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºçš„åŸºç¡€å±æ€§ï¼ŒåŒ…æ‹¬æœåŠ¡åŒºç¼–ç ã€åç§°ã€æ–¹å‘ã€å…¬å¸å½’å±ã€åœ°ç†ä½ç½®ã€æœåŠ¡ç±»å‹å’ŒçŠ¶æ€ï¼Œæ˜¯ä¸šåŠ¡åˆ†æä¸æœåŠ¡åŒºå®šä½çš„æ ¸å¿ƒè¡¨ã€‚
create table bss_service_area (
  id varchar(32) not null,              -- æœåŠ¡åŒºå”¯ä¸€æ ‡è¯†ï¼ˆä¸»é”®ï¼ŒUUIDæ ¼å¼ï¼‰
  version integer not null,             -- æ•°æ®ç‰ˆæœ¬å·
  create_ts timestamp(3),               -- åˆ›å»ºæ—¶é—´
  created_by varchar(50),               -- åˆ›å»ºäºº
  update_ts timestamp(3),               -- æ›´æ–°æ—¶é—´
  updated_by varchar(50),               -- æ›´æ–°äºº
  delete_ts timestamp(3),               -- åˆ é™¤æ—¶é—´
  deleted_by varchar(50),               -- åˆ é™¤äºº
  service_area_name varchar(255),       -- æœåŠ¡åŒºåç§°
  service_area_no varchar(255),         -- æœåŠ¡åŒºç¼–ç ï¼ˆä¸šåŠ¡å”¯ä¸€æ ‡è¯†ï¼‰
  company_id varchar(32),               -- å…¬å¸IDï¼ˆå¤–é”®å…³è”bss_company.idï¼‰
  service_position varchar(255),        -- ç»çº¬åº¦åæ ‡
  service_area_type varchar(50),        -- æœåŠ¡åŒºç±»å‹ï¼ˆæšä¸¾ï¼šä¿¡æ¯åŒ–æœåŠ¡åŒºã€æ™ºèƒ½åŒ–æœåŠ¡åŒºï¼‰
  service_state varchar(50),            -- æœåŠ¡åŒºçŠ¶æ€ï¼ˆæšä¸¾ï¼šå¼€æ”¾ã€å…³é—­ã€ä¸Šä¼ æ•°æ®ï¼‰
  primary key (id)
);
```

#### 6.2.2 MDæ–‡æ¡£è¾“å‡ºæ ·ä¾‹ (`bss_service_area_detail.md`)

markdown

```markdown
## bss_service_areaï¼ˆæœåŠ¡åŒºåŸºç¡€ä¿¡æ¯è¡¨ï¼‰
bss_service_area è¡¨è®°å½•é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºçš„åŸºç¡€å±æ€§ï¼ŒåŒ…æ‹¬æœåŠ¡åŒºç¼–ç ã€åç§°ã€æ–¹å‘ã€å…¬å¸å½’å±ã€åœ°ç†ä½ç½®ã€æœåŠ¡ç±»å‹å’ŒçŠ¶æ€ï¼Œæ˜¯ä¸šåŠ¡åˆ†æä¸æœåŠ¡åŒºå®šä½çš„æ ¸å¿ƒè¡¨ã€‚

å­—æ®µåˆ—è¡¨ï¼š
- id (varchar(32)) - æœåŠ¡åŒºå”¯ä¸€æ ‡è¯†ï¼ˆä¸»é”®ï¼ŒUUIDæ ¼å¼ï¼‰[ç¤ºä¾‹: 0271d68ef93de9684b7ad8c7aae600b6]
- version (integer) - æ•°æ®ç‰ˆæœ¬å· [ç¤ºä¾‹: 3]
- create_ts (timestamp(3)) - åˆ›å»ºæ—¶é—´ [ç¤ºä¾‹: 2021-05-21 13:26:40.589]
- created_by (varchar(50)) - åˆ›å»ºäºº [ç¤ºä¾‹: admin]
- update_ts (timestamp(3)) - æ›´æ–°æ—¶é—´ [ç¤ºä¾‹: 2021-07-10 15:41:28.795]
- updated_by (varchar(50)) - æ›´æ–°äºº [ç¤ºä¾‹: admin]
- delete_ts (timestamp(3)) - åˆ é™¤æ—¶é—´
- deleted_by (varchar(50)) - åˆ é™¤äºº
- service_area_name (varchar(255)) - æœåŠ¡åŒºåç§° [ç¤ºä¾‹: é„±é˜³æ¹–æœåŠ¡åŒº]
- service_area_no (varchar(255)) - æœåŠ¡åŒºç¼–ç ï¼ˆä¸šåŠ¡å”¯ä¸€æ ‡è¯†ï¼‰[ç¤ºä¾‹: H0509]
- company_id (varchar(32)) - å…¬å¸IDï¼ˆå¤–é”®å…³è”bss_company.idï¼‰[ç¤ºä¾‹: b1629f07c8d9ac81494fbc1de61f1ea5]
- service_position (varchar(255)) - ç»çº¬åº¦åæ ‡ [ç¤ºä¾‹: 114.574721,26.825584]
- service_area_type (varchar(50)) - æœåŠ¡åŒºç±»å‹ï¼ˆæšä¸¾ï¼šä¿¡æ¯åŒ–æœåŠ¡åŒºã€æ™ºèƒ½åŒ–æœåŠ¡åŒºï¼‰[ç¤ºä¾‹: ä¿¡æ¯åŒ–æœåŠ¡åŒº]
- service_state (varchar(50)) - æœåŠ¡åŒºçŠ¶æ€ï¼ˆæšä¸¾ï¼šå¼€æ”¾ã€å…³é—­ã€ä¸Šä¼ æ•°æ®ï¼‰[ç¤ºä¾‹: å¼€æ”¾]

å­—æ®µè¡¥å……è¯´æ˜ï¼š
- id ä¸ºä¸»é”®ï¼Œä½¿ç”¨ UUID ç¼–ç ï¼Œå”¯ä¸€æ ‡è¯†æ¯ä¸ªæœåŠ¡åŒº
- company_id å¤–é”®å…³è”æœåŠ¡åŒºç®¡ç†å…¬å¸è¡¨(bss_company.id)
- service_position ç»çº¬åº¦æ ¼å¼ä¸º"ç»åº¦,çº¬åº¦"
- service_area_type ä¸ºæšä¸¾å­—æ®µï¼ŒåŒ…å«ä¸¤ä¸ªå–å€¼ï¼šä¿¡æ¯åŒ–æœåŠ¡åŒºã€æ™ºèƒ½åŒ–æœåŠ¡åŒº
- service_state ä¸ºæšä¸¾å­—æ®µï¼ŒåŒ…å«ä¸‰ä¸ªå–å€¼ï¼šå¼€æ”¾ã€å…³é—­ã€ä¸Šä¼ æ•°æ®
- æœ¬è¡¨æ˜¯å¤šä¸ªè¡¨(bss_branch, bss_car_day_countç­‰)çš„æ ¸å¿ƒå…³è”å®ä½“
```

#### 6.2.3 å¤æ‚è¡¨æ ·ä¾‹ (`bss_business_day_data.ddl`)

sql

```sql
-- ä¸­æ–‡å: æ¡£å£æ—¥è¥ä¸šæ•°æ®è¡¨
-- æè¿°: è®°å½•æ¯å¤©æ¯ä¸ªæ¡£å£çš„è¥ä¸šæƒ…å†µï¼ŒåŒ…å«å¾®ä¿¡ã€æ”¯ä»˜å®ã€ç°é‡‘ã€é‡‘è±†ç­‰æ”¯ä»˜æ–¹å¼çš„é‡‘é¢ä¸è®¢å•æ•°ï¼Œæ˜¯æ ¸å¿ƒäº¤æ˜“æ•°æ®è¡¨ã€‚
create table bss_business_day_data (
  id varchar(32) not null,        -- ä¸»é”®ID
  version integer not null,       -- æ•°æ®ç‰ˆæœ¬å·
  create_ts timestamp(3),         -- åˆ›å»ºæ—¶é—´
  created_by varchar(50),         -- åˆ›å»ºäºº
  update_ts timestamp(3),         -- æ›´æ–°æ—¶é—´
  updated_by varchar(50),         -- æ›´æ–°äºº
  delete_ts timestamp(3),         -- åˆ é™¤æ—¶é—´
  deleted_by varchar(50),         -- åˆ é™¤äºº
  oper_date date,                 -- ç»Ÿè®¡æ—¥æœŸ
  service_no varchar(255),        -- æœåŠ¡åŒºç¼–ç 
  service_name varchar(255),      -- æœåŠ¡åŒºåç§°
  branch_no varchar(255),         -- æ¡£å£ç¼–ç 
  branch_name varchar(255),       -- æ¡£å£åç§°
  wx numeric(19,4),               -- å¾®ä¿¡æ”¯ä»˜é‡‘é¢
  wx_order integer,               -- å¾®ä¿¡æ”¯ä»˜è®¢å•æ•°é‡
  zfb numeric(19,4),              -- æ”¯ä»˜å®æ”¯ä»˜é‡‘é¢
  zf_order integer,               -- æ”¯ä»˜å®æ”¯ä»˜è®¢å•æ•°é‡
  rmb numeric(19,4),              -- ç°é‡‘æ”¯ä»˜é‡‘é¢
  rmb_order integer,              -- ç°é‡‘æ”¯ä»˜è®¢å•æ•°é‡
  xs numeric(19,4),               -- è¡Œå§æ”¯ä»˜é‡‘é¢
  xs_order integer,               -- è¡Œå§æ”¯ä»˜è®¢å•æ•°é‡
  jd numeric(19,4),               -- é‡‘è±†æ”¯ä»˜é‡‘é¢
  jd_order integer,               -- é‡‘è±†æ”¯ä»˜è®¢å•æ•°é‡
  order_sum integer,              -- è®¢å•æ€»æ•°
  pay_sum numeric(19,4),          -- æ”¯ä»˜æ€»é‡‘é¢
  source_type integer,            -- æ•°æ®æ¥æºç±»å‹ID
  primary key (id)
);
```

### 6.3 è¾“å‡ºæ ¼å¼å…³é”®ç‰¹å¾

#### 6.3.1 DDLæ ¼å¼ç‰¹å¾

- **ä¸­æ–‡è¡¨å¤´æ³¨é‡Š**: åŒ…å«è¡¨ä¸­æ–‡åå’Œä¸šåŠ¡æè¿°
- **å­—æ®µæ³¨é‡Š**: æ¯ä¸ªå­—æ®µéƒ½æœ‰ä¸­æ–‡æ³¨é‡Šè¯´æ˜
- **æšä¸¾æ ‡è¯†**: å¯¹äºæšä¸¾å­—æ®µï¼Œåœ¨æ³¨é‡Šä¸­æ˜ç¡®æ ‡å‡ºå¯é€‰å€¼
- **å¤–é”®å…³ç³»**: æ˜ç¡®æ ‡å‡ºå¤–é”®å…³è”å…³ç³»
- **ä¸šåŠ¡æ ‡è¯†**: ç‰¹æ®Šä¸šåŠ¡å­—æ®µï¼ˆå¦‚ç¼–ç ã€IDï¼‰æœ‰è¯¦ç»†è¯´æ˜

#### 6.3.2 MDæ ¼å¼ç‰¹å¾

- **è¡¨çº§æè¿°**: è¯¦ç»†çš„è¡¨ä¸šåŠ¡ç”¨é€”è¯´æ˜
- **å­—æ®µç¤ºä¾‹å€¼**: æ¯ä¸ªå­—æ®µéƒ½æä¾›çœŸå®çš„ç¤ºä¾‹æ•°æ®
- **æšä¸¾å€¼è¯¦è§£**: æšä¸¾å­—æ®µçš„æ‰€æœ‰å¯èƒ½å–å€¼å®Œæ•´åˆ—å‡º
- **è¡¥å……è¯´æ˜**: é‡è¦å­—æ®µçš„é¢å¤–ä¸šåŠ¡é€»è¾‘è¯´æ˜
- **å…³è”å…³ç³»**: ä¸å…¶ä»–è¡¨çš„å…³è”å…³ç³»è¯´æ˜



## 7. é…ç½®æ–‡ä»¶å®Œæ•´å®ç°

### 7.1 é…ç½®æ–‡ä»¶ (`config.py`)

```python
import os
import sys

# å¯¼å…¥app_configè·å–æ•°æ®åº“ç­‰é…ç½®
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
try:
    import app_config
except ImportError:
    app_config = None

# Schema Toolsä¸“ç”¨é…ç½®
SCHEMA_TOOLS_CONFIG = {
    # æ ¸å¿ƒé…ç½®
    "default_db_connection": None,  # ä»å‘½ä»¤è¡ŒæŒ‡å®š
    "default_business_context": "æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ", 
    "output_directory": "training/generated_data",
    
    # å¤„ç†é“¾é…ç½®
    "default_pipeline": "full",
    "available_pipelines": {
        "full": [
            "database_inspector", 
            "data_sampler", 
            "comment_generator", 
            "ddl_generator", 
            "doc_generator"
        ],
        "ddl_only": [
            "database_inspector", 
            "data_sampler", 
            "comment_generator", 
            "ddl_generator"
        ],
        "analysis_only": [
            "database_inspector", 
            "data_sampler", 
            "comment_generator"
        ]
    },
    
    # æ•°æ®å¤„ç†é…ç½®
    "sample_data_limit": 20,                    # ç”¨äºLLMåˆ†æçš„é‡‡æ ·æ•°æ®é‡
    "enum_detection_sample_limit": 5000,        # æšä¸¾æ£€æµ‹æ—¶çš„é‡‡æ ·é™åˆ¶
    "enum_max_distinct_values": 20,             # æšä¸¾å­—æ®µæœ€å¤§ä¸åŒå€¼æ•°é‡
    "enum_varchar_keywords": [                  # VARCHARæšä¸¾å…³é”®è¯
        "æ€§åˆ«", "gender", "çŠ¶æ€", "status", "ç±»å‹", "type", 
        "çº§åˆ«", "level", "æ–¹å‘", "direction", "å“ç±»", "classify",
        "æ¨¡å¼", "mode", "æ ¼å¼", "format"
    ],
    "large_table_threshold": 1000000,           # å¤§è¡¨é˜ˆå€¼ï¼ˆè¡Œæ•°ï¼‰
    
    # å¹¶å‘é…ç½®
    "max_concurrent_tables": 3,                 # æœ€å¤§å¹¶å‘å¤„ç†è¡¨æ•°
    
    # LLMé…ç½®
    "use_app_config_llm": True,                # æ˜¯å¦ä½¿ç”¨app_configä¸­çš„LLMé…ç½®
    "comment_generation_timeout": 30,          # LLMè°ƒç”¨è¶…æ—¶æ—¶é—´(ç§’)
    "max_llm_retries": 3,                      # LLMè°ƒç”¨æœ€å¤§é‡è¯•æ¬¡æ•°
    
    # ç³»ç»Ÿè¡¨è¿‡æ»¤é…ç½®
    "filter_system_tables": True,              # æ˜¯å¦è¿‡æ»¤ç³»ç»Ÿè¡¨
    "custom_system_prefixes": [],              # ç”¨æˆ·è‡ªå®šä¹‰ç³»ç»Ÿè¡¨å‰ç¼€
    "custom_system_schemas": [],               # ç”¨æˆ·è‡ªå®šä¹‰ç³»ç»Ÿschema
    
    # æƒé™ä¸å®‰å…¨é…ç½®
    "check_permissions": True,                 # æ˜¯å¦æ£€æŸ¥æ•°æ®åº“æƒé™
    "require_select_permission": True,         # æ˜¯å¦è¦æ±‚SELECTæƒé™
    "allow_readonly_database": True,           # æ˜¯å¦å…è®¸åªè¯»æ•°æ®åº“
    
    # é”™è¯¯å¤„ç†é…ç½®
    "continue_on_error": True,                 # é‡åˆ°é”™è¯¯æ˜¯å¦ç»§ç»­
    "max_table_failures": 5,                  # æœ€å¤§å…è®¸å¤±è´¥è¡¨æ•°
    "skip_large_tables": False,               # æ˜¯å¦è·³è¿‡è¶…å¤§è¡¨
    "max_table_size": 10000000,               # æœ€å¤§è¡¨è¡Œæ•°é™åˆ¶
    
    # æ–‡ä»¶é…ç½®
    "ddl_file_suffix": ".ddl",
    "doc_file_suffix": "_detail.md",
    "log_file": "schema_tools.log",
    "create_subdirectories": True,            # æ˜¯å¦åˆ›å»ºddl/docså­ç›®å½•
    
    # è¾“å‡ºæ ¼å¼é…ç½®
    "include_sample_data_in_comments": True,  # æ³¨é‡Šä¸­æ˜¯å¦åŒ…å«ç¤ºä¾‹æ•°æ®
    "max_comment_length": 500,                # æœ€å¤§æ³¨é‡Šé•¿åº¦
    "include_field_statistics": True,         # æ˜¯å¦åŒ…å«å­—æ®µç»Ÿè®¡ä¿¡æ¯
    
    # è°ƒè¯•é…ç½®
    "debug_mode": False,                      # è°ƒè¯•æ¨¡å¼
    "save_llm_prompts": False,               # æ˜¯å¦ä¿å­˜LLMæç¤ºè¯
    "save_llm_responses": False,             # æ˜¯å¦ä¿å­˜LLMå“åº”
}

# ä»app_configè·å–ç›¸å…³é…ç½®ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if app_config:
    # ç»§æ‰¿æ•°æ®åº“é…ç½®
    if hasattr(app_config, 'PGVECTOR_CONFIG'):
        pgvector_config = app_config.PGVECTOR_CONFIG
        if not SCHEMA_TOOLS_CONFIG["default_db_connection"]:
            SCHEMA_TOOLS_CONFIG["default_db_connection"] = (
                f"postgresql://{pgvector_config['user']}:{pgvector_config['password']}"
                f"@{pgvector_config['host']}:{pgvector_config['port']}/{pgvector_config['dbname']}"
            )

def get_config():
    """è·å–å½“å‰é…ç½®"""
    return SCHEMA_TOOLS_CONFIG

def update_config(**kwargs):
    """æ›´æ–°é…ç½®"""
    SCHEMA_TOOLS_CONFIG.update(kwargs)

def validate_config():
    """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
    errors = []
    
    # æ£€æŸ¥å¿…è¦é…ç½®
    if SCHEMA_TOOLS_CONFIG["max_concurrent_tables"] <= 0:
        errors.append("max_concurrent_tables å¿…é¡»å¤§äº0")
    
    if SCHEMA_TOOLS_CONFIG["sample_data_limit"] <= 0:
        errors.append("sample_data_limit å¿…é¡»å¤§äº0")
    
    # æ£€æŸ¥å¤„ç†é“¾é…ç½®
    default_pipeline = SCHEMA_TOOLS_CONFIG["default_pipeline"]
    available_pipelines = SCHEMA_TOOLS_CONFIG["available_pipelines"]
    
    if default_pipeline not in available_pipelines:
        errors.append(f"default_pipeline '{default_pipeline}' ä¸åœ¨ available_pipelines ä¸­")
    
    if errors:
        raise ValueError("é…ç½®éªŒè¯å¤±è´¥:\n" + "\n".join(f"  - {error}" for error in errors))
    
    return True

# å¯åŠ¨æ—¶éªŒè¯é…ç½®
try:
    validate_config()
except ValueError as e:
    print(f"è­¦å‘Š: {e}")
```

è¿™ä¸ªè¯¦ç»†è®¾è®¡æ–‡æ¡£æ¶µç›–äº†Schema Toolsçš„å®Œæ•´å®ç°ï¼ŒåŒ…æ‹¬ï¼š

## æ ¸å¿ƒç‰¹æ€§

1. **å®Œæ•´çš„æ•°æ®ç»“æ„è®¾è®¡** - æ ‡å‡†åŒ–çš„æ•°æ®æ¨¡å‹
2. **å·¥å…·æ³¨å†Œæœºåˆ¶** - è£…é¥°å™¨æ³¨å†Œå’Œè‡ªåŠ¨ä¾èµ–æ³¨å…¥
3. **Pipelineå¤„ç†é“¾** - å¯é…ç½®çš„å¤„ç†æµç¨‹
4. **å¹¶å‘å¤„ç†** - è¡¨çº§å¹¶å‘å’Œé”™è¯¯å¤„ç†
5. **LLMé›†æˆ** - æ™ºèƒ½æ³¨é‡Šç”Ÿæˆå’Œæšä¸¾æ£€æµ‹
6. **æƒé™ç®¡ç†** - æ•°æ®åº“æƒé™æ£€æŸ¥å’Œåªè¯»é€‚é…
7. **å‘½ä»¤è¡Œæ¥å£** - å®Œæ•´çš„CLIæ”¯æŒ

## å®ç°äº®ç‚¹

- **ç±»å‹å®‰å…¨**: ä½¿ç”¨dataclasså®šä¹‰æ˜ç¡®çš„æ•°æ®ç»“æ„
- **é”™è¯¯å¤„ç†**: å®Œå–„çš„å¼‚å¸¸å¤„ç†å’Œé‡è¯•æœºåˆ¶
- **å¯æ‰©å±•æ€§**: å·¥å…·æ³¨å†Œæœºåˆ¶ä¾¿äºæ·»åŠ æ–°åŠŸèƒ½
- **é…ç½®çµæ´»**: å¤šå±‚æ¬¡é…ç½®æ”¯æŒ
- **æ—¥å¿—å®Œæ•´**: è¯¦ç»†çš„æ‰§è¡Œæ—¥å¿—å’Œç»Ÿè®¡æŠ¥å‘Š