"""
ç»Ÿä¸€ API æœåŠ¡
é›†æˆ citu_app.py æŒ‡å®šAPI å’Œ react_agent/api.py çš„æ‰€æœ‰åŠŸèƒ½
æä¾›æ•°æ®åº“é—®ç­”ã€Rediså¯¹è¯ç®¡ç†ã€QAåé¦ˆã€è®­ç»ƒæ•°æ®ç®¡ç†ã€React Agentç­‰åŠŸèƒ½

ä½¿ç”¨æ™®é€š Flask åº”ç”¨ + ASGI åŒ…è£…å®ç°å¼‚æ­¥æ”¯æŒ
"""
import asyncio
import logging
import atexit
import os
import sys
from datetime import datetime, timedelta, timezone
import pytz
from typing import Optional, Dict, Any, TYPE_CHECKING, Union
import signal
from threading import Thread

if TYPE_CHECKING:
    from react_agent.agent import CustomReactAgent

# åˆå§‹åŒ–æ—¥å¿—ç³»ç»Ÿ - å¿…é¡»åœ¨æœ€å‰é¢
from core.logging import initialize_logging, get_app_logger
initialize_logging()

# æ ‡å‡† Flask å¯¼å…¥
from flask import Flask, request, jsonify, session, send_file, Response, stream_with_context
import redis.asyncio as redis
from werkzeug.utils import secure_filename

# å¯¼å…¥æ ‡å‡†åŒ–å“åº”æ ¼å¼
from common.result import success_response, internal_error_response, bad_request_response

# åŸºç¡€ä¾èµ–
import pandas as pd
import json
import sqlparse
import tempfile
import os
import psycopg2
import re

# é¡¹ç›®æ¨¡å—å¯¼å…¥
from core.vanna_llm_factory import create_vanna_instance
from common.redis_conversation_manager import RedisConversationManager
from common.qa_feedback_manager import QAFeedbackManager
# Data Pipeline ç›¸å…³å¯¼å…¥ - ä» citu_app.py è¿ç§»
from data_pipeline.api.simple_workflow import SimpleWorkflowManager, SimpleWorkflowExecutor
from data_pipeline.api.simple_file_manager import SimpleFileManager
from data_pipeline.api.table_inspector_api import TableInspectorAPI
from common.result import (
    success_response, bad_request_response, not_found_response, internal_error_response,
    error_response, service_unavailable_response, 
    agent_success_response, agent_error_response,
    validation_failed_response
)
from app_config import (
    USER_MAX_CONVERSATIONS, CONVERSATION_CONTEXT_COUNT, 
    DEFAULT_ANONYMOUS_USER, ENABLE_QUESTION_ANSWER_CACHE
)

# åˆ›å»ºæ ‡å‡† Flask åº”ç”¨
app = Flask(__name__)

# åˆ›å»ºæ—¥å¿—è®°å½•å™¨
logger = get_app_logger("UnifiedApp")

# React Agent å¯¼å…¥
try:
    from react_agent.agent import CustomReactAgent
    from react_agent.enhanced_redis_api import get_conversation_detail_from_redis
except ImportError:
    try:
        from test.custom_react_agent.agent import CustomReactAgent
        from test.custom_react_agent.enhanced_redis_api import get_conversation_detail_from_redis
    except ImportError:
        logger.warning("æ— æ³•å¯¼å…¥ CustomReactAgentï¼ŒReact AgentåŠŸèƒ½å°†ä¸å¯ç”¨")
        CustomReactAgent = None
        get_conversation_detail_from_redis = None

# åˆå§‹åŒ–æ ¸å¿ƒç»„ä»¶
vn = create_vanna_instance()
redis_conversation_manager = RedisConversationManager()

# ==================== React Agent å…¨å±€å®ä¾‹ç®¡ç† ====================

_react_agent_instance: Optional[Any] = None
_redis_client: Optional[redis.Redis] = None

def _format_timestamp_to_china_time(timestamp_str):
    """å°†ISOæ—¶é—´æˆ³è½¬æ¢ä¸ºä¸­å›½æ—¶åŒºçš„æŒ‡å®šæ ¼å¼"""
    if not timestamp_str:
        return None
    try:
        # è§£æISOæ—¶é—´æˆ³
        dt = datetime.fromisoformat(timestamp_str.replace('Z', '+00:00'))
        # è½¬æ¢ä¸ºä¸­å›½æ—¶åŒº
        china_tz = pytz.timezone('Asia/Shanghai')
        china_dt = dt.astimezone(china_tz)
        # æ ¼å¼åŒ–ä¸ºæŒ‡å®šæ ¼å¼
        return china_dt.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]  # ä¿ç•™3ä½æ¯«ç§’
    except Exception as e:
        logger.warning(f"âš ï¸ æ—¶é—´æ ¼å¼åŒ–å¤±è´¥: {e}")
        return timestamp_str

def _parse_conversation_created_time(conversation_id: str) -> Optional[str]:
    """ä»conversation_idè§£æåˆ›å»ºæ—¶é—´å¹¶è½¬æ¢ä¸ºä¸­å›½æ—¶åŒºæ ¼å¼"""
    try:
        # conversation_idæ ¼å¼: "wang10:20250717211620915"
        if ':' not in conversation_id:
            return None
        
        parts = conversation_id.split(':')
        if len(parts) < 2:
            return None
        
        timestamp_str = parts[1]  # "20250717211620915"
        
        # è§£ææ—¶é—´æˆ³: YYYYMMDDHHMMSSMMM (17ä½)
        if len(timestamp_str) != 17:
            logger.warning(f"âš ï¸ conversation_idæ—¶é—´æˆ³é•¿åº¦ä¸æ­£ç¡®: {timestamp_str}")
            return None
        
        year = timestamp_str[:4]
        month = timestamp_str[4:6]
        day = timestamp_str[6:8]
        hour = timestamp_str[8:10]
        minute = timestamp_str[10:12]
        second = timestamp_str[12:14]
        millisecond = timestamp_str[14:17]
        
        # æ„é€ datetimeå¯¹è±¡
        dt = datetime(
            int(year), int(month), int(day),
            int(hour), int(minute), int(second),
            int(millisecond) * 1000  # æ¯«ç§’è½¬å¾®ç§’
        )
        
        # è½¬æ¢ä¸ºä¸­å›½æ—¶åŒº
        china_tz = pytz.timezone('Asia/Shanghai')
        # å‡è®¾åŸå§‹æ—¶é—´æˆ³æ˜¯ä¸­å›½æ—¶åŒº
        china_dt = china_tz.localize(dt)
        
        # æ ¼å¼åŒ–ä¸ºè¦æ±‚çš„æ ¼å¼
        formatted_time = china_dt.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]  # ä¿ç•™3ä½æ¯«ç§’
        return formatted_time
        
    except Exception as e:
        logger.warning(f"âš ï¸ è§£æconversation_idæ—¶é—´æˆ³å¤±è´¥: {e}")
        return None

def _get_conversation_updated_time(redis_client, thread_id: str) -> Optional[str]:
    """è·å–å¯¹è¯çš„æœ€åæ›´æ–°æ—¶é—´ï¼ˆä»Redis checkpointæ•°æ®ä¸­çš„tså­—æ®µï¼‰"""
    try:
        # æ‰«æè¯¥threadçš„æ‰€æœ‰checkpoint keys
        pattern = f"checkpoint:{thread_id}:*"
        
        keys = []
        cursor = 0
        while True:
            cursor, batch = redis_client.scan(cursor=cursor, match=pattern, count=1000)
            keys.extend(batch)
            if cursor == 0:
                break
        
        if not keys:
            return None
        
        # è·å–æœ€æ–°çš„checkpointï¼ˆæŒ‰keyæ’åºï¼Œæœ€å¤§çš„æ˜¯æœ€æ–°çš„ï¼‰
        latest_key = max(keys)
        
        # æ£€æŸ¥keyç±»å‹å¹¶è·å–æ•°æ®
        key_type = redis_client.type(latest_key)
        
        data = None
        if key_type == 'string':
            data = redis_client.get(latest_key)
        elif key_type == 'ReJSON-RL':
            # RedisJSONç±»å‹
            try:
                data = redis_client.execute_command('JSON.GET', latest_key)
            except Exception as json_error:
                logger.error(f"âŒ JSON.GETå¤±è´¥: {json_error}")
                return None
        else:
            return None
        
        if not data:
            return None
        
        # è§£æJSONæ•°æ®
        try:
            checkpoint_data = json.loads(data)
        except json.JSONDecodeError:
            return None
        
        # æ£€æŸ¥checkpointä¸­çš„tså­—æ®µ
        if ('checkpoint' in checkpoint_data and 
            isinstance(checkpoint_data['checkpoint'], dict) and 
            'ts' in checkpoint_data['checkpoint']):
            
            ts_value = checkpoint_data['checkpoint']['ts']
            
            # è§£ætså­—æ®µï¼ˆåº”è¯¥æ˜¯ISOæ ¼å¼çš„æ—¶é—´æˆ³ï¼‰
            if isinstance(ts_value, str):
                try:
                    dt = datetime.fromisoformat(ts_value.replace('Z', '+00:00'))
                    china_tz = pytz.timezone('Asia/Shanghai')
                    china_dt = dt.astimezone(china_tz)
                    formatted_time = china_dt.strftime('%Y-%m-%d %H:%M:%S.%f')[:-3]
                    return formatted_time
                except Exception:
                    pass
        
        return None
        
    except Exception as e:
        logger.warning(f"âš ï¸ è·å–å¯¹è¯æ›´æ–°æ—¶é—´å¤±è´¥: {e}")
        return None

def validate_request_data(data: Dict[str, Any]) -> Dict[str, Any]:
    """éªŒè¯è¯·æ±‚æ•°æ®ï¼Œå¹¶æ”¯æŒä»thread_idä¸­æ¨æ–­user_id"""
    errors = []
    
    # éªŒè¯ questionï¼ˆå¿…å¡«ï¼‰
    question = data.get('question', '')
    if not question or not question.strip():
        errors.append('é—®é¢˜ä¸èƒ½ä¸ºç©º')
    elif len(question) > 2000:
        errors.append('é—®é¢˜é•¿åº¦ä¸èƒ½è¶…è¿‡2000å­—ç¬¦')
    
    # ä¼˜å…ˆè·å– thread_id
    thread_id = data.get('thread_id') or data.get('conversation_id')
    
    # è·å– user_idï¼Œä½†æš‚ä¸è®¾ç½®é»˜è®¤å€¼
    user_id = data.get('user_id')

    # å¦‚æœæ²¡æœ‰ä¼ é€’ user_idï¼Œåˆ™å°è¯•ä» thread_id ä¸­æ¨æ–­
    if not user_id:
        if thread_id and ':' in thread_id:
            inferred_user_id = thread_id.split(':', 1)[0]
            if inferred_user_id:
                user_id = inferred_user_id
                logger.info(f"ğŸ‘¤ æœªæä¾›user_idï¼Œä» thread_id '{thread_id}' ä¸­æ¨æ–­å‡º: '{user_id}'")
            else:
                user_id = 'guest'
        else:
            user_id = 'guest'

    # éªŒè¯ user_id é•¿åº¦
    if user_id and len(user_id) > 50:
        errors.append('ç”¨æˆ·IDé•¿åº¦ä¸èƒ½è¶…è¿‡50å­—ç¬¦')
    
    # ç”¨æˆ·IDä¸ä¼šè¯IDä¸€è‡´æ€§æ ¡éªŒ
    if thread_id:
        if ':' not in thread_id:
            errors.append('ä¼šè¯IDæ ¼å¼æ— æ•ˆï¼ŒæœŸæœ›æ ¼å¼ä¸º user_id:timestamp')
        else:
            thread_user_id = thread_id.split(':', 1)[0]
            if thread_user_id != user_id:
                errors.append(f'ä¼šè¯å½’å±éªŒè¯å¤±è´¥ï¼šä¼šè¯ID [{thread_id}] ä¸å±äºå½“å‰ç”¨æˆ· [{user_id}]')
    
    if errors:
        raise ValueError('; '.join(errors))
    
    return {
        'question': question.strip(),
        'user_id': user_id,
        'thread_id': thread_id  # å¯é€‰ï¼Œä¸ä¼ åˆ™è‡ªåŠ¨ç”Ÿæˆæ–°ä¼šè¯
    }

async def get_react_agent() -> Any:
    """è·å– React Agent å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
    global _react_agent_instance, _redis_client
    
    if _react_agent_instance is None:
        if CustomReactAgent is None:
            logger.error("âŒ CustomReactAgent æœªèƒ½å¯¼å…¥ï¼Œæ— æ³•åˆå§‹åŒ–")
            raise ImportError("CustomReactAgent æœªèƒ½å¯¼å…¥")
            
        logger.info("ğŸš€ æ­£åœ¨å¼‚æ­¥åˆå§‹åŒ– Custom React Agent...")
        try:
            # è®¾ç½®ç¯å¢ƒå˜é‡
            os.environ['REDIS_URL'] = 'redis://localhost:6379'
            
            # åˆå§‹åŒ–å…±äº«çš„Rediså®¢æˆ·ç«¯
            _redis_client = redis.from_url('redis://localhost:6379', decode_responses=True)
            await _redis_client.ping()
            logger.info("âœ… Rediså®¢æˆ·ç«¯è¿æ¥æˆåŠŸ")
            
            _react_agent_instance = await CustomReactAgent.create()
            logger.info("âœ… React Agent å¼‚æ­¥åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"âŒ React Agent å¼‚æ­¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    return _react_agent_instance

async def ensure_agent_ready() -> bool:
    """å¼‚æ­¥ç¡®ä¿Agentå®ä¾‹å¯ç”¨"""
    global _react_agent_instance
    
    if _react_agent_instance is None:
        await get_react_agent()
    
    # æµ‹è¯•Agentæ˜¯å¦è¿˜å¯ç”¨
    try:
        test_result = await _react_agent_instance.get_user_recent_conversations("__test__", 1)
        return True
    except Exception as e:
        logger.warning(f"âš ï¸ Agentå®ä¾‹ä¸å¯ç”¨: {e}")
        _react_agent_instance = None
        await get_react_agent()
        return True

def get_user_conversations_simple_sync(user_id: str, limit: int = 10):
    """ç›´æ¥ä»Redisè·å–ç”¨æˆ·å¯¹è¯ï¼Œæµ‹è¯•ç‰ˆæœ¬"""
    import redis
    import json
    
    try:
        # åˆ›å»ºRedisè¿æ¥
        redis_client = redis.Redis(host='localhost', port=6379, decode_responses=True)
        redis_client.ping()
        
        # æ‰«æç”¨æˆ·çš„checkpoint keys
        pattern = f"checkpoint:{user_id}:*"
        logger.info(f"ğŸ” æ‰«ææ¨¡å¼: {pattern}")
        
        keys = []
        cursor = 0
        while True:
            cursor, batch = redis_client.scan(cursor=cursor, match=pattern, count=1000)
            keys.extend(batch)
            if cursor == 0:
                break
        
        logger.info(f"ğŸ“‹ æ‰¾åˆ° {len(keys)} ä¸ªkeys")
        
        # è§£æthreadä¿¡æ¯
        thread_data = {}
        for key in keys:
            try:
                parts = key.split(':')
                if len(parts) >= 4:
                    thread_id = f"{parts[1]}:{parts[2]}"  # user_id:timestamp
                    timestamp = parts[2]
                    
                    if thread_id not in thread_data:
                        thread_data[thread_id] = {
                            "thread_id": thread_id,
                            "timestamp": timestamp,
                            "keys": []
                        }
                    thread_data[thread_id]["keys"].append(key)
            except Exception as e:
                logger.warning(f"è§£ækeyå¤±è´¥ {key}: {e}")
                continue
        
        logger.info(f"ğŸ“Š æ‰¾åˆ° {len(thread_data)} ä¸ªthread")
        
        # æŒ‰æ—¶é—´æˆ³æ’åº
        sorted_threads = sorted(
            thread_data.values(),
            key=lambda x: x["timestamp"],
            reverse=True
        )[:limit]
        
        # è·å–æ¯ä¸ªthreadçš„è¯¦ç»†ä¿¡æ¯
        conversations = []
        for thread_info in sorted_threads:
            try:
                thread_id = thread_info["thread_id"]
                
                # è·å–æœ€æ–°çš„checkpointæ•°æ®
                latest_key = max(thread_info["keys"])
                
                # å…ˆæ£€æŸ¥keyçš„æ•°æ®ç±»å‹
                key_type = redis_client.type(latest_key)
                logger.info(f"ğŸ” Key {latest_key} çš„ç±»å‹: {key_type}")
                
                data = None
                if key_type == 'string':
                    data = redis_client.get(latest_key)
                elif key_type == 'hash':
                    # å¦‚æœæ˜¯hashç±»å‹ï¼Œè·å–æ‰€æœ‰å­—æ®µ
                    hash_data = redis_client.hgetall(latest_key)
                    logger.info(f"ğŸ” Hashå­—æ®µ: {list(hash_data.keys())}")
                    # å°è¯•è·å–å¯èƒ½çš„æ•°æ®å­—æ®µ
                    for field in ['data', 'state', 'value', 'checkpoint']:
                        if field in hash_data:
                            data = hash_data[field]
                            break
                    if not data and hash_data:
                        # å¦‚æœæ²¡æ‰¾åˆ°é¢„æœŸå­—æ®µï¼Œå–ç¬¬ä¸€ä¸ªå€¼è¯•è¯•
                        data = list(hash_data.values())[0]
                elif key_type == 'list':
                    # å¦‚æœæ˜¯listç±»å‹ï¼Œè·å–æœ€åä¸€ä¸ªå…ƒç´ 
                    data = redis_client.lindex(latest_key, -1)
                elif key_type == 'ReJSON-RL':
                    # è¿™æ˜¯RedisJSONç±»å‹ï¼Œä½¿ç”¨JSON.GETå‘½ä»¤
                    logger.info(f"ğŸ” ä½¿ç”¨JSON.GETè·å–RedisJSONæ•°æ®")
                    try:
                        # ä½¿ç”¨JSON.GETå‘½ä»¤è·å–æ•´ä¸ªJSONå¯¹è±¡
                        json_data = redis_client.execute_command('JSON.GET', latest_key)
                        if json_data:
                            data = json_data  # JSON.GETè¿”å›çš„å°±æ˜¯JSONå­—ç¬¦ä¸²
                            logger.info(f"ğŸ” JSONæ•°æ®é•¿åº¦: {len(data)} å­—ç¬¦")
                        else:
                            logger.warning(f"âš ï¸ JSON.GET è¿”å›ç©ºæ•°æ®")
                            continue
                    except Exception as json_error:
                        logger.error(f"âŒ JSON.GET å¤±è´¥: {json_error}")
                        continue
                else:
                    logger.warning(f"âš ï¸ æœªçŸ¥çš„keyç±»å‹: {key_type}")
                    continue
                
                if data:
                    try:
                        checkpoint_data = json.loads(data)
                        
                        # è°ƒè¯•ï¼šæŸ¥çœ‹JSONæ•°æ®ç»“æ„
                        logger.info(f"ğŸ” JSONé¡¶çº§keys: {list(checkpoint_data.keys())}")
                        
                        # æ ¹æ®æ‚¨æä¾›çš„JSONç»“æ„ï¼Œæ¶ˆæ¯åœ¨ checkpoint.channel_values.messages
                        messages = []
                        
                        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰checkpointå­—æ®µ
                        if 'checkpoint' in checkpoint_data:
                            checkpoint = checkpoint_data['checkpoint']
                            if isinstance(checkpoint, dict) and 'channel_values' in checkpoint:
                                channel_values = checkpoint['channel_values']
                                if isinstance(channel_values, dict) and 'messages' in channel_values:
                                    messages = channel_values['messages']
                                    logger.info(f"ğŸ” æ‰¾åˆ°messages: {len(messages)} æ¡æ¶ˆæ¯")
                        
                        # å¦‚æœæ²¡æœ‰checkpointå­—æ®µï¼Œå°è¯•ç›´æ¥åœ¨channel_values
                        if not messages and 'channel_values' in checkpoint_data:
                            channel_values = checkpoint_data['channel_values']
                            if isinstance(channel_values, dict) and 'messages' in channel_values:
                                messages = channel_values['messages']
                                logger.info(f"ğŸ” æ‰¾åˆ°messages(ç›´æ¥è·¯å¾„): {len(messages)} æ¡æ¶ˆæ¯")
                        
                        # ç”Ÿæˆå¯¹è¯é¢„è§ˆ
                        preview = "ç©ºå¯¹è¯"
                        if messages:
                            for msg in messages:
                                # å¤„ç†LangChainæ¶ˆæ¯æ ¼å¼ï¼š{"lc": 1, "type": "constructor", "id": ["langchain", "schema", "messages", "HumanMessage"], "kwargs": {"content": "...", "type": "human"}}
                                if isinstance(msg, dict):
                                    # æ£€æŸ¥æ˜¯å¦æ˜¯LangChainæ ¼å¼çš„HumanMessage
                                    if (msg.get('lc') == 1 and 
                                        msg.get('type') == 'constructor' and 
                                        'id' in msg and 
                                        isinstance(msg['id'], list) and 
                                        len(msg['id']) >= 4 and
                                        msg['id'][3] == 'HumanMessage' and
                                        'kwargs' in msg):
                                        
                                        kwargs = msg['kwargs']
                                        if kwargs.get('type') == 'human' and 'content' in kwargs:
                                            content = str(kwargs['content'])
                                            preview = content[:50] + "..." if len(content) > 50 else content
                                            break
                                    # å…¼å®¹å…¶ä»–æ ¼å¼
                                    elif msg.get('type') == 'human' and 'content' in msg:
                                        content = str(msg['content'])
                                        preview = content[:50] + "..." if len(content) > 50 else content
                                        break
                        
                        # è§£ææ—¶é—´æˆ³
                        created_at = _parse_conversation_created_time(thread_id)
                        updated_at = _get_conversation_updated_time(redis_client, thread_id)
                        
                        # å¦‚æœæ— æ³•è·å–updated_atï¼Œä½¿ç”¨created_atä½œä¸ºå¤‡é€‰
                        if not updated_at:
                            updated_at = created_at
                        
                        conversations.append({
                            "conversation_id": thread_id,  # thread_id -> conversation_id
                            "user_id": user_id,
                            "message_count": len(messages),
                            "conversation_title": preview,  # conversation_preview -> conversation_title
                            "created_at": created_at,
                            "updated_at": updated_at
                        })
                        
                    except json.JSONDecodeError:
                        logger.error(f"âŒ JSONè§£æå¤±è´¥ï¼Œæ•°æ®ç±»å‹: {type(data)}, é•¿åº¦: {len(str(data))}")
                        logger.error(f"âŒ æ•°æ®å¼€å¤´: {str(data)[:200]}...")
                        continue
                    
            except Exception as e:
                logger.error(f"å¤„ç†thread {thread_info['thread_id']} å¤±è´¥: {e}")
                continue
        
        redis_client.close()
        logger.info(f"âœ… è¿”å› {len(conversations)} ä¸ªå¯¹è¯")
        return conversations
        
    except Exception as e:
        logger.error(f"âŒ RedisæŸ¥è¯¢å¤±è´¥: {e}")
        return []

def cleanup_resources():
    """æ¸…ç†èµ„æº"""
    global _react_agent_instance, _redis_client
    
    async def async_cleanup():
        if _react_agent_instance:
            await _react_agent_instance.close()
            logger.info("âœ… React Agent èµ„æºå·²æ¸…ç†")
        
        if _redis_client:
            await _redis_client.aclose()
            logger.info("âœ… Rediså®¢æˆ·ç«¯å·²å…³é—­")
    
    try:
        asyncio.run(async_cleanup())
    except Exception as e:
        logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")

atexit.register(cleanup_resources)

# ==================== åŸºç¡€è·¯ç”± ====================

@app.route("/")
def index():
    """æ ¹è·¯å¾„å¥åº·æ£€æŸ¥"""
    return jsonify({"message": "ç»Ÿä¸€APIæœåŠ¡æ­£åœ¨è¿è¡Œ", "version": "1.0.0"})

@app.route('/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥ç«¯ç‚¹"""
    try:
        health_status = {
            "status": "healthy",
            "react_agent_initialized": _react_agent_instance is not None,
            "timestamp": datetime.now().isoformat(),
            "services": {
                "redis": redis_conversation_manager.is_available(),
                "vanna": vn is not None
            }
        }
        return jsonify(health_status), 200
    except Exception as e:
        logger.error(f"å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
        return jsonify({"status": "unhealthy", "error": str(e)}), 500

# ==================== React Agent API ====================

@app.route("/api/v0/ask_react_agent", methods=["POST"])
async def ask_react_agent():
    """å¼‚æ­¥React Agentæ™ºèƒ½é—®ç­”æ¥å£ï¼ˆä» custom_react_agent è¿ç§»ï¼ŒåŸè·¯ç”±ï¼š/api/chatï¼‰"""
    global _react_agent_instance
    
    # ç¡®ä¿Agentå·²åˆå§‹åŒ–
    if not await ensure_agent_ready():
        return jsonify({
            "code": 503,
            "message": "æœåŠ¡æœªå°±ç»ª",
            "success": False,
            "error": "React Agent åˆå§‹åŒ–å¤±è´¥"
        }), 503
    
    try:
        # è·å–è¯·æ±‚æ•°æ®
        try:
            data = request.get_json(force=True)
        except Exception as json_error:
            logger.warning(f"âš ï¸ JSONè§£æå¤±è´¥: {json_error}")
            return jsonify({
                "code": 400,
                "message": "è¯·æ±‚æ ¼å¼é”™è¯¯",
                "success": False,
                "error": "æ— æ•ˆçš„JSONæ ¼å¼ï¼Œè¯·æ£€æŸ¥è¯·æ±‚ä½“ä¸­æ˜¯å¦å­˜åœ¨è¯­æ³•é”™è¯¯ï¼ˆå¦‚å¤šä½™çš„é€—å·ã€å¼•å·ä¸åŒ¹é…ç­‰ï¼‰",
                "details": str(json_error)
            }), 400
        
        if not data:
            return jsonify({
                "code": 400,
                "message": "è¯·æ±‚å‚æ•°é”™è¯¯",
                "success": False,
                "error": "è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º"
            }), 400
        
        # éªŒè¯è¯·æ±‚æ•°æ®
        validated_data = validate_request_data(data)
        
        logger.info(f"ğŸ“¨ æ”¶åˆ°React Agentè¯·æ±‚ - User: {validated_data['user_id']}, Question: {validated_data['question'][:50]}...")
        
        # å¼‚æ­¥è°ƒç”¨å¤„ç†
        agent_result = await _react_agent_instance.chat(
            message=validated_data['question'],
            user_id=validated_data['user_id'],
            thread_id=validated_data['thread_id']
        )
        
        if not agent_result.get("success", False):
            # Agentå¤„ç†å¤±è´¥
            error_msg = agent_result.get("error", "React Agentå¤„ç†å¤±è´¥")
            logger.error(f"âŒ React Agentå¤„ç†å¤±è´¥: {error_msg}")
            
            # æ£€æŸ¥æ˜¯å¦å»ºè®®é‡è¯•
            retry_suggested = agent_result.get("retry_suggested", False)
            error_code = 503 if retry_suggested else 500
            message = "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•" if retry_suggested else "å¤„ç†å¤±è´¥"
            
            return jsonify({
                "code": error_code,
                "message": message,
                "success": False,
                "error": error_msg,
                "retry_suggested": retry_suggested,
                "data": {
                    "conversation_id": agent_result.get("thread_id"),
                    "user_id": validated_data['user_id'],
                    "timestamp": datetime.now().isoformat()
                }
            }), error_code
        
        # Agentå¤„ç†æˆåŠŸ
        api_data = agent_result.get("api_data", {})
        
        # æ„å»ºå“åº”æ•°æ®ï¼ˆæŒ‰ç…§ react_agent/api.py çš„æ­£ç¡®æ ¼å¼ï¼‰
        response_data = {
            "response": api_data.get("response", ""),
            "conversation_id": agent_result.get("thread_id"),
            "user_id": validated_data['user_id'],
            "react_agent_meta": api_data.get("react_agent_meta", {
                "thread_id": agent_result.get("thread_id"),
                "agent_version": "custom_react_v1_async"
            }),
            "timestamp": datetime.now().isoformat()
        }
        
        # å¯é€‰å­—æ®µï¼šSQLï¼ˆä»…å½“æ‰§è¡ŒSQLæ—¶å­˜åœ¨ï¼‰
        if "sql" in api_data:
            response_data["sql"] = api_data["sql"]
        
        # å¯é€‰å­—æ®µï¼šrecordsï¼ˆä»…å½“æœ‰æŸ¥è¯¢ç»“æœæ—¶å­˜åœ¨ï¼‰
        if "records" in api_data:
            response_data["records"] = api_data["records"]
        
        return jsonify({
            "code": 200,
            "message": "å¤„ç†æˆåŠŸ",
            "success": True,
            "data": response_data
        }), 200
        
    except ValueError as ve:
        # å‚æ•°éªŒè¯é”™è¯¯
        logger.warning(f"âš ï¸ å‚æ•°éªŒè¯å¤±è´¥: {ve}")
        return jsonify({
            "code": 400,
            "message": "å‚æ•°éªŒè¯å¤±è´¥",
            "success": False,
            "error": str(ve)
        }), 400
        
    except Exception as e:
        logger.error(f"âŒ React Agent API å¼‚å¸¸: {e}")
        return jsonify({
            "code": 500,
            "message": "å†…éƒ¨æœåŠ¡é”™è¯¯",
            "success": False,
            "error": "æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•"
        }), 500

# ==================== LangGraph Agent API ====================

# å…¨å±€Agentå®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
citu_langraph_agent = None

def get_citu_langraph_agent():
    """è·å–LangGraph Agentå®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
    global citu_langraph_agent
    if citu_langraph_agent is None:
        try:
            from agent.citu_agent import CituLangGraphAgent
            logger.info("å¼€å§‹åˆ›å»ºLangGraph Agentå®ä¾‹...")
            citu_langraph_agent = CituLangGraphAgent()
            logger.info("LangGraph Agentå®ä¾‹åˆ›å»ºæˆåŠŸ")
        except ImportError as e:
            logger.critical(f"Agentæ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}")
            raise Exception(f"Agentæ¨¡å—å¯¼å…¥å¤±è´¥: {str(e)}")
        except Exception as e:
            logger.critical(f"LangGraph Agentå®ä¾‹åˆ›å»ºå¤±è´¥: {str(e)}")
            raise Exception(f"Agentåˆå§‹åŒ–å¤±è´¥: {str(e)}")
    return citu_langraph_agent

@app.route('/api/v0/ask_agent', methods=['POST'])
def ask_agent():
    """æ”¯æŒå¯¹è¯ä¸Šä¸‹æ–‡çš„ask_agent API"""
    req = request.get_json(force=True)
    question = req.get("question", None)
    browser_session_id = req.get("session_id", None)
    user_id_input = req.get("user_id", None)
    conversation_id_input = req.get("conversation_id", None)
    continue_conversation = req.get("continue_conversation", False)
    api_routing_mode = req.get("routing_mode", None)
    
    VALID_ROUTING_MODES = ["database_direct", "chat_direct", "hybrid", "llm_only"]
    
    if not question:
        return jsonify(bad_request_response(
            response_text="ç¼ºå°‘å¿…éœ€å‚æ•°ï¼šquestion",
            missing_params=["question"]
        )), 400
    
    if api_routing_mode and api_routing_mode not in VALID_ROUTING_MODES:
        return jsonify(bad_request_response(
            response_text=f"æ— æ•ˆçš„routing_modeå‚æ•°å€¼: {api_routing_mode}ï¼Œæ”¯æŒçš„å€¼: {VALID_ROUTING_MODES}",
            invalid_params=["routing_mode"]
        )), 400

    try:
        # è·å–ç™»å½•ç”¨æˆ·ID
        login_user_id = session.get('user_id') if 'user_id' in session else None
        
        # æ™ºèƒ½IDè§£æ
        user_id = redis_conversation_manager.resolve_user_id(
            user_id_input, browser_session_id, request.remote_addr, login_user_id
        )
        conversation_id, conversation_status = redis_conversation_manager.resolve_conversation_id(
            user_id, conversation_id_input, continue_conversation
        )
        
        # è·å–ä¸Šä¸‹æ–‡å’Œä¸Šä¸‹æ–‡ç±»å‹ï¼ˆæå‰åˆ°ç¼“å­˜æ£€æŸ¥ä¹‹å‰ï¼‰
        context = redis_conversation_manager.get_context(conversation_id)
        
        # è·å–ä¸Šä¸‹æ–‡ç±»å‹ï¼šä»æœ€åä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯çš„metadataä¸­è·å–ç±»å‹
        context_type = None
        if context:
            try:
                # è·å–æœ€åä¸€æ¡åŠ©æ‰‹æ¶ˆæ¯çš„metadata
                messages = redis_conversation_manager.get_messages(conversation_id, limit=10)
                for message in reversed(messages):  # ä»æœ€æ–°çš„å¼€å§‹æ‰¾
                    if message.get("role") == "assistant":
                        metadata = message.get("metadata", {})
                        context_type = metadata.get("type")
                        if context_type:
                            logger.info(f"[AGENT_API] æ£€æµ‹åˆ°ä¸Šä¸‹æ–‡ç±»å‹: {context_type}")
                            break
            except Exception as e:
                logger.warning(f"è·å–ä¸Šä¸‹æ–‡ç±»å‹å¤±è´¥: {str(e)}")
        
        # æ£€æŸ¥ç¼“å­˜ï¼ˆæ–°é€»è¾‘ï¼šæ”¾å®½ä½¿ç”¨æ¡ä»¶ï¼Œä¸¥æ§å­˜å‚¨æ¡ä»¶ï¼‰
        cached_answer = redis_conversation_manager.get_cached_answer(question, context)
        if cached_answer:
            logger.info(f"[AGENT_API] ä½¿ç”¨ç¼“å­˜ç­”æ¡ˆ")
            
            # ç¡®å®šç¼“å­˜ç­”æ¡ˆçš„åŠ©æ‰‹å›å¤å†…å®¹ï¼ˆä½¿ç”¨ä¸éç¼“å­˜ç›¸åŒçš„ä¼˜å…ˆçº§é€»è¾‘ï¼‰
            cached_response_type = cached_answer.get("type", "UNKNOWN")
            if cached_response_type == "DATABASE":
                # DATABASEç±»å‹ï¼šæŒ‰ä¼˜å…ˆçº§é€‰æ‹©å†…å®¹
                if cached_answer.get("response"):
                    # ä¼˜å…ˆçº§1ï¼šé”™è¯¯æˆ–è§£é‡Šæ€§å›å¤ï¼ˆå¦‚SQLç”Ÿæˆå¤±è´¥ï¼‰
                    assistant_response = cached_answer.get("response")
                elif cached_answer.get("summary"):
                    # ä¼˜å…ˆçº§2ï¼šæŸ¥è¯¢æˆåŠŸçš„æ‘˜è¦
                    assistant_response = cached_answer.get("summary")
                elif cached_answer.get("query_result"):
                    # ä¼˜å…ˆçº§3ï¼šæ„é€ ç®€å•æè¿°
                    query_result = cached_answer.get("query_result")
                    row_count = query_result.get("row_count", 0)
                    assistant_response = f"æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œå…±è¿”å› {row_count} æ¡è®°å½•ã€‚"
                else:
                    # å¼‚å¸¸æƒ…å†µ
                    assistant_response = "æ•°æ®åº“æŸ¥è¯¢å·²å¤„ç†ã€‚"
            else:
                # CHATç±»å‹ï¼šç›´æ¥ä½¿ç”¨response
                assistant_response = cached_answer.get("response", "")
            
            # æ›´æ–°å¯¹è¯å†å²
            redis_conversation_manager.save_message(conversation_id, "user", question)
            redis_conversation_manager.save_message(
                conversation_id, "assistant", 
                assistant_response,
                metadata={"from_cache": True}
            )
            
            # æ·»åŠ å¯¹è¯ä¿¡æ¯åˆ°ç¼“å­˜ç»“æœ
            cached_answer["conversation_id"] = conversation_id
            cached_answer["user_id"] = user_id
            cached_answer["from_cache"] = True
            cached_answer.update(conversation_status)
            
            # ä½¿ç”¨agent_success_responseè¿”å›æ ‡å‡†æ ¼å¼
            return jsonify(agent_success_response(
                response_type=cached_answer.get("type", "UNKNOWN"),
                response=cached_answer.get("response", ""),
                sql=cached_answer.get("sql"),
                records=cached_answer.get("query_result"),
                summary=cached_answer.get("summary"),
                session_id=browser_session_id,
                execution_path=cached_answer.get("execution_path", []),
                classification_info=cached_answer.get("classification_info", {}),
                conversation_id=conversation_id,
                user_id=user_id,
                is_guest_user=(user_id == DEFAULT_ANONYMOUS_USER),
                context_used=bool(context),
                from_cache=True,
                conversation_status=conversation_status["status"],
                conversation_message=conversation_status["message"],
                requested_conversation_id=conversation_status.get("requested_id")
            ))
        
        # ä¿å­˜ç”¨æˆ·æ¶ˆæ¯
        redis_conversation_manager.save_message(conversation_id, "user", question)
        
        # æ„å»ºå¸¦ä¸Šä¸‹æ–‡çš„é—®é¢˜
        if context:
            enhanced_question = f"\n[CONTEXT]\n{context}\n\n[CURRENT]\n{question}"
            logger.info(f"[AGENT_API] ä½¿ç”¨ä¸Šä¸‹æ–‡ï¼Œé•¿åº¦: {len(context)}å­—ç¬¦")
        else:
            enhanced_question = question
            logger.info(f"[AGENT_API] æ–°å¯¹è¯ï¼Œæ— ä¸Šä¸‹æ–‡")
        
        # ç¡®å®šæœ€ç»ˆä½¿ç”¨çš„è·¯ç”±æ¨¡å¼ï¼ˆä¼˜å…ˆçº§é€»è¾‘ï¼‰
        if api_routing_mode:
            # APIä¼ äº†å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨
            effective_routing_mode = api_routing_mode
            logger.info(f"[AGENT_API] ä½¿ç”¨APIæŒ‡å®šçš„è·¯ç”±æ¨¡å¼: {effective_routing_mode}")
        else:
            # APIæ²¡ä¼ å‚æ•°ï¼Œä½¿ç”¨é…ç½®æ–‡ä»¶
            try:
                from app_config import QUESTION_ROUTING_MODE
                effective_routing_mode = QUESTION_ROUTING_MODE
                logger.info(f"[AGENT_API] ä½¿ç”¨é…ç½®æ–‡ä»¶è·¯ç”±æ¨¡å¼: {effective_routing_mode}")
            except ImportError:
                effective_routing_mode = "hybrid"
                logger.info(f"[AGENT_API] é…ç½®æ–‡ä»¶è¯»å–å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤è·¯ç”±æ¨¡å¼: {effective_routing_mode}")
        
        # Agentå¤„ç†
        try:
            agent = get_citu_langraph_agent()
        except Exception as e:
            logger.critical(f"Agentåˆå§‹åŒ–å¤±è´¥: {str(e)}")
            return jsonify(service_unavailable_response(
                response_text="AIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼Œè¯·ç¨åé‡è¯•",
                can_retry=True
            )), 503
        
        # å¼‚æ­¥è°ƒç”¨Agentå¤„ç†é—®é¢˜
        import asyncio
        agent_result = asyncio.run(agent.process_question(
            question=enhanced_question,  # ä½¿ç”¨å¢å¼ºåçš„é—®é¢˜
            session_id=browser_session_id,
            context_type=context_type,  # ä¼ é€’ä¸Šä¸‹æ–‡ç±»å‹
            routing_mode=effective_routing_mode  # æ–°å¢ï¼šä¼ é€’è·¯ç”±æ¨¡å¼
        ))
        
        # å¤„ç†Agentç»“æœ
        if agent_result.get("success", False):
            response_type = agent_result.get("type", "UNKNOWN")
            response_text = agent_result.get("response", "")
            sql = agent_result.get("sql")
            query_result = agent_result.get("query_result")
            summary = agent_result.get("summary")
            execution_path = agent_result.get("execution_path", [])
            classification_info = agent_result.get("classification_info", {})
            
            # ç¡®å®šåŠ©æ‰‹å›å¤å†…å®¹çš„ä¼˜å…ˆçº§
            if response_type == "DATABASE":
                if response_text:
                    assistant_response = response_text
                elif summary:
                    assistant_response = summary
                elif query_result:
                    row_count = query_result.get("row_count", 0)
                    assistant_response = f"æŸ¥è¯¢æ‰§è¡Œå®Œæˆï¼Œå…±è¿”å› {row_count} æ¡è®°å½•ã€‚"
                else:
                    assistant_response = "æ•°æ®åº“æŸ¥è¯¢å·²å¤„ç†ã€‚"
            else:
                assistant_response = response_text
            
            # ä¿å­˜åŠ©æ‰‹å›å¤
            redis_conversation_manager.save_message(
                conversation_id, "assistant", assistant_response,
                metadata={
                    "type": response_type,
                    "sql": sql,
                    "execution_path": execution_path
                }
            )
            
            # ç¼“å­˜æˆåŠŸçš„ç­”æ¡ˆï¼ˆæ–°é€»è¾‘ï¼šåªç¼“å­˜æ— ä¸Šä¸‹æ–‡çš„é—®ç­”ï¼‰
            # ç›´æ¥ç¼“å­˜agent_resultï¼Œå®ƒå·²ç»åŒ…å«æ‰€æœ‰éœ€è¦çš„å­—æ®µ
            redis_conversation_manager.cache_answer(question, agent_result, context)
            
            # ä½¿ç”¨agent_success_responseçš„æ­£ç¡®æ–¹å¼
            return jsonify(agent_success_response(
                response_type=response_type,
                response=response_text,
                sql=sql,
                records=query_result,
                summary=summary,
                session_id=browser_session_id,
                execution_path=execution_path,
                classification_info=classification_info,
                conversation_id=conversation_id,
                user_id=user_id,
                is_guest_user=(user_id == DEFAULT_ANONYMOUS_USER),
                context_used=bool(context),
                from_cache=False,
                conversation_status=conversation_status["status"],
                conversation_message=conversation_status["message"],
                requested_conversation_id=conversation_status.get("requested_id"),
                routing_mode_used=effective_routing_mode,  # æ–°å¢ï¼šå®é™…ä½¿ç”¨çš„è·¯ç”±æ¨¡å¼
                routing_mode_source="api" if api_routing_mode else "config"  # æ–°å¢ï¼šè·¯ç”±æ¨¡å¼æ¥æº
            ))
        else:
            # é”™è¯¯å¤„ç†
            error_message = agent_result.get("error", "Agentå¤„ç†å¤±è´¥")
            error_code = agent_result.get("error_code", 500)
            
            return jsonify(agent_error_response(
                response_text=error_message,
                error_type="agent_processing_failed",
                code=error_code,
                session_id=browser_session_id,
                conversation_id=conversation_id,
                user_id=user_id
            )), error_code
        
    except Exception as e:
        logger.error(f"ask_agentæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="æŸ¥è¯¢å¤„ç†å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

# ==================== QAåé¦ˆç³»ç»ŸAPI ====================

qa_feedback_manager = None

def get_qa_feedback_manager():
    """è·å–QAåé¦ˆç®¡ç†å™¨å®ä¾‹ï¼ˆæ‡’åŠ è½½ï¼‰"""
    global qa_feedback_manager
    if qa_feedback_manager is None:
        try:
            qa_feedback_manager = QAFeedbackManager(vanna_instance=vn)
            logger.info("QAåé¦ˆç®¡ç†å™¨å®ä¾‹åˆ›å»ºæˆåŠŸ")
        except Exception as e:
            logger.critical(f"QAåé¦ˆç®¡ç†å™¨åˆ›å»ºå¤±è´¥: {str(e)}")
            raise Exception(f"QAåé¦ˆç®¡ç†å™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}")
    return qa_feedback_manager

@app.route('/api/v0/qa_feedback/query', methods=['POST'])
def qa_feedback_query():
    """
    æŸ¥è¯¢åé¦ˆè®°å½•API
    æ”¯æŒåˆ†é¡µã€ç­›é€‰å’Œæ’åºåŠŸèƒ½
    """
    try:
        req = request.get_json(force=True)
        
        # è§£æå‚æ•°ï¼Œè®¾ç½®é»˜è®¤å€¼
        page = req.get('page', 1)
        page_size = req.get('page_size', 20)
        is_thumb_up = req.get('is_thumb_up')
        create_time_start = req.get('create_time_start')
        create_time_end = req.get('create_time_end')
        is_in_training_data = req.get('is_in_training_data')
        sort_by = req.get('sort_by', 'create_time')
        sort_order = req.get('sort_order', 'desc')
        
        # å‚æ•°éªŒè¯
        if page < 1:
            return jsonify(bad_request_response(
                response_text="é¡µç å¿…é¡»å¤§äº0",
                invalid_params=["page"]
            )), 400
        
        if page_size < 1 or page_size > 100:
            return jsonify(bad_request_response(
                response_text="æ¯é¡µå¤§å°å¿…é¡»åœ¨1-100ä¹‹é—´",
                invalid_params=["page_size"]
            )), 400
        
        # è·å–åé¦ˆç®¡ç†å™¨å¹¶æŸ¥è¯¢
        manager = get_qa_feedback_manager()
        records, total = manager.query_feedback(
            page=page,
            page_size=page_size,
            is_thumb_up=is_thumb_up,
            create_time_start=create_time_start,
            create_time_end=create_time_end,
            is_in_training_data=is_in_training_data,
            sort_by=sort_by,
            sort_order=sort_order
        )
        
        total_pages = (total + page_size - 1) // page_size
        
        return jsonify(success_response(
            response_text=f"æŸ¥è¯¢æˆåŠŸï¼Œå…±æ‰¾åˆ° {total} æ¡è®°å½•",
            data={
                "records": records,
                "pagination": {
                    "page": page,
                    "page_size": page_size,
                    "total": total,
                    "total_pages": total_pages,
                    "has_next": page < total_pages,
                    "has_prev": page > 1
                }
            }
        ))
        
    except Exception as e:
        logger.error(f"qa_feedback_queryæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="æŸ¥è¯¢åé¦ˆè®°å½•å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/qa_feedback/delete/<int:feedback_id>', methods=['DELETE'])
def qa_feedback_delete(feedback_id):
    """åˆ é™¤åé¦ˆè®°å½•API"""
    try:
        manager = get_qa_feedback_manager()
        success = manager.delete_feedback(feedback_id)
        
        if success:
            return jsonify(success_response(
                response_text=f"åé¦ˆè®°å½•åˆ é™¤æˆåŠŸ",
                data={"deleted_id": feedback_id}
            ))
        else:
            return jsonify(not_found_response(
                response_text=f"åé¦ˆè®°å½•ä¸å­˜åœ¨ (ID: {feedback_id})"
            )), 404
            
    except Exception as e:
        logger.error(f"qa_feedback_deleteæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="åˆ é™¤åé¦ˆè®°å½•å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/qa_feedback/update/<int:feedback_id>', methods=['PUT'])
def qa_feedback_update(feedback_id):
    """æ›´æ–°åé¦ˆè®°å½•API"""
    try:
        req = request.get_json(force=True)
        
        allowed_fields = ['question', 'sql', 'is_thumb_up', 'user_id', 'is_in_training_data']
        update_data = {}
        
        for field in allowed_fields:
            if field in req:
                update_data[field] = req[field]
        
        if not update_data:
            return jsonify(bad_request_response(
                response_text="æ²¡æœ‰æä¾›æœ‰æ•ˆçš„æ›´æ–°å­—æ®µ",
                missing_params=allowed_fields
            )), 400
        
        manager = get_qa_feedback_manager()
        success = manager.update_feedback(feedback_id, **update_data)
        
        if success:
            return jsonify(success_response(
                response_text="åé¦ˆè®°å½•æ›´æ–°æˆåŠŸ",
                data={
                    "updated_id": feedback_id,
                    "updated_fields": list(update_data.keys())
                }
            ))
        else:
            return jsonify(not_found_response(
                response_text=f"åé¦ˆè®°å½•ä¸å­˜åœ¨æˆ–æ— å˜åŒ– (ID: {feedback_id})"
            )), 404
            
    except Exception as e:
        logger.error(f"qa_feedback_updateæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="æ›´æ–°åé¦ˆè®°å½•å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/qa_feedback/add_to_training', methods=['POST'])
def qa_feedback_add_to_training():
    """
    å°†åé¦ˆè®°å½•æ·»åŠ åˆ°è®­ç»ƒæ•°æ®é›†API
    æ”¯æŒæ··åˆæ‰¹é‡å¤„ç†ï¼šæ­£å‘åé¦ˆåŠ å…¥SQLè®­ç»ƒé›†ï¼Œè´Ÿå‘åé¦ˆåŠ å…¥error_sqlè®­ç»ƒé›†
    """
    try:
        req = request.get_json(force=True)
        feedback_ids = req.get('feedback_ids', [])
        
        if not feedback_ids or not isinstance(feedback_ids, list):
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘æœ‰æ•ˆçš„åé¦ˆIDåˆ—è¡¨",
                missing_params=["feedback_ids"]
            )), 400
        
        manager = get_qa_feedback_manager()
        
        # è·å–åé¦ˆè®°å½•
        records = manager.get_feedback_by_ids(feedback_ids)
        
        if not records:
            return jsonify(not_found_response(
                response_text="æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„åé¦ˆè®°å½•"
            )), 404
        
        # åˆ†åˆ«å¤„ç†æ­£å‘å’Œè´Ÿå‘åé¦ˆ
        positive_count = 0  # æ­£å‘è®­ç»ƒè®¡æ•°
        negative_count = 0  # è´Ÿå‘è®­ç»ƒè®¡æ•°
        already_trained_count = 0  # å·²è®­ç»ƒè®¡æ•°
        error_count = 0  # é”™è¯¯è®¡æ•°
        
        successfully_trained_ids = []  # æˆåŠŸè®­ç»ƒçš„IDåˆ—è¡¨
        
        for record in records:
            try:
                # æ£€æŸ¥æ˜¯å¦å·²ç»åœ¨è®­ç»ƒæ•°æ®ä¸­
                if record['is_in_training_data']:
                    already_trained_count += 1
                    continue
                
                if record['is_thumb_up']:
                    # æ­£å‘åé¦ˆ - åŠ å…¥æ ‡å‡†SQLè®­ç»ƒé›†
                    training_id = vn.train(
                        question=record['question'], 
                        sql=record['sql']
                    )
                    positive_count += 1
                    logger.info(f"æ­£å‘è®­ç»ƒæˆåŠŸ - ID: {record['id']}, TrainingID: {training_id}")
                else:
                    # è´Ÿå‘åé¦ˆ - åŠ å…¥é”™è¯¯SQLè®­ç»ƒé›†
                    training_id = vn.train_error_sql(
                        question=record['question'], 
                        sql=record['sql']
                    )
                    negative_count += 1
                    logger.info(f"è´Ÿå‘è®­ç»ƒæˆåŠŸ - ID: {record['id']}, TrainingID: {training_id}")
                
                successfully_trained_ids.append(record['id'])
                
            except Exception as e:
                logger.error(f"è®­ç»ƒå¤±è´¥ - åé¦ˆID: {record['id']}, é”™è¯¯: {e}")
                error_count += 1
        
        # æ›´æ–°è®­ç»ƒçŠ¶æ€
        if successfully_trained_ids:
            updated_count = manager.mark_training_status(successfully_trained_ids, True)
            logger.info(f"æ‰¹é‡æ›´æ–°è®­ç»ƒçŠ¶æ€å®Œæˆï¼Œå½±å“ {updated_count} æ¡è®°å½•")
        
        # æ„å»ºå“åº”
        total_processed = positive_count + negative_count + already_trained_count + error_count
        
        return jsonify(success_response(
            response_text=f"è®­ç»ƒæ•°æ®æ·»åŠ å®Œæˆï¼ŒæˆåŠŸå¤„ç† {positive_count + negative_count} æ¡è®°å½•",
            data={
                "summary": {
                    "total_requested": len(feedback_ids),
                    "total_processed": total_processed,
                    "positive_trained": positive_count,
                    "negative_trained": negative_count,
                    "already_trained": already_trained_count,
                    "errors": error_count
                },
                "successfully_trained_ids": successfully_trained_ids,
                "training_details": {
                    "sql_training_count": positive_count,
                    "error_sql_training_count": negative_count
                }
            }
        ))
        
    except Exception as e:
        logger.error(f"qa_feedback_add_to_trainingæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="æ·»åŠ è®­ç»ƒæ•°æ®å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/qa_feedback/add', methods=['POST'])
def qa_feedback_add():
    """
    æ·»åŠ åé¦ˆè®°å½•API
    ç”¨äºå‰ç«¯ç›´æ¥åˆ›å»ºåé¦ˆè®°å½•
    """
    try:
        req = request.get_json(force=True)
        question = req.get('question')
        sql = req.get('sql')
        is_thumb_up = req.get('is_thumb_up')
        user_id = req.get('user_id', 'guest')
        
        # å‚æ•°éªŒè¯
        if not question:
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘å¿…éœ€å‚æ•°ï¼šquestion",
                missing_params=["question"]
            )), 400
        
        if not sql:
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘å¿…éœ€å‚æ•°ï¼šsql",
                missing_params=["sql"]
            )), 400
        
        if is_thumb_up is None:
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘å¿…éœ€å‚æ•°ï¼šis_thumb_up",
                missing_params=["is_thumb_up"]
            )), 400
        
        manager = get_qa_feedback_manager()
        feedback_id = manager.add_feedback(
            question=question,
            sql=sql,
            is_thumb_up=bool(is_thumb_up),
            user_id=user_id
        )
        
        return jsonify(success_response(
            response_text="åé¦ˆè®°å½•åˆ›å»ºæˆåŠŸ",
            data={
                "feedback_id": feedback_id
            }
        ))
        
    except Exception as e:
        logger.error(f"qa_feedback_addæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="åˆ›å»ºåé¦ˆè®°å½•å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/qa_feedback/stats', methods=['GET'])
def qa_feedback_stats():
    """
    åé¦ˆç»Ÿè®¡API
    è¿”å›åé¦ˆæ•°æ®çš„ç»Ÿè®¡ä¿¡æ¯
    """
    try:
        manager = get_qa_feedback_manager()
        
        # æŸ¥è¯¢å„ç§ç»Ÿè®¡æ•°æ®
        all_records, total_count = manager.query_feedback(page=1, page_size=1)
        positive_records, positive_count = manager.query_feedback(page=1, page_size=1, is_thumb_up=True)
        negative_records, negative_count = manager.query_feedback(page=1, page_size=1, is_thumb_up=False)
        trained_records, trained_count = manager.query_feedback(page=1, page_size=1, is_in_training_data=True)
        untrained_records, untrained_count = manager.query_feedback(page=1, page_size=1, is_in_training_data=False)
        
        return jsonify(success_response(
            response_text="ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ",
            data={
                "total_feedback": total_count,
                "positive_feedback": positive_count,
                "negative_feedback": negative_count,
                "trained_feedback": trained_count,
                "untrained_feedback": untrained_count,
                "positive_rate": round(positive_count / max(total_count, 1) * 100, 2),
                "training_rate": round(trained_count / max(total_count, 1) * 100, 2)
            }
        ))
        
    except Exception as e:
        logger.error(f"qa_feedback_statsæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

# ==================== Rediså¯¹è¯ç®¡ç†API ====================

@app.route('/api/v0/user/<user_id>/conversations', methods=['GET'])
def get_user_conversations_redis(user_id: str):
    """è·å–ç”¨æˆ·çš„å¯¹è¯åˆ—è¡¨"""
    try:
        limit = request.args.get('limit', USER_MAX_CONVERSATIONS, type=int)
        conversations = redis_conversation_manager.get_conversations(user_id, limit)
        
        return jsonify(success_response(
            response_text="è·å–ç”¨æˆ·å¯¹è¯åˆ—è¡¨æˆåŠŸ",
            data={
                "user_id": user_id,
                "conversations": conversations,
                "total_count": len(conversations)
            }
        ))
        
    except Exception as e:
        return jsonify(internal_error_response(
            response_text="è·å–å¯¹è¯åˆ—è¡¨å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/conversation/<conversation_id>/messages', methods=['GET'])
def get_conversation_messages_redis(conversation_id: str):
    """è·å–ç‰¹å®šå¯¹è¯çš„æ¶ˆæ¯å†å²"""
    try:
        limit = request.args.get('limit', type=int)
        messages = redis_conversation_manager.get_conversation_messages(conversation_id, limit)
        meta = redis_conversation_manager.get_conversation_meta(conversation_id)
        
        return jsonify(success_response(
            response_text="è·å–å¯¹è¯æ¶ˆæ¯æˆåŠŸ",
            data={
                "conversation_id": conversation_id,
                "conversation_meta": meta,
                "messages": messages,
                "message_count": len(messages)
            }
        ))
        
    except Exception as e:
        return jsonify(internal_error_response(
            response_text="è·å–å¯¹è¯æ¶ˆæ¯å¤±è´¥"
        )), 500

@app.route('/api/v0/conversation_stats', methods=['GET'])
def conversation_stats():
    """è·å–å¯¹è¯ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = redis_conversation_manager.get_stats()
        
        return jsonify(success_response(
            response_text="è·å–ç»Ÿè®¡ä¿¡æ¯æˆåŠŸ",
            data=stats
        ))
        
    except Exception as e:
        return jsonify(internal_error_response(
            response_text="è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/conversation_cleanup', methods=['POST'])
def conversation_cleanup():
    """æ‰‹åŠ¨æ¸…ç†è¿‡æœŸå¯¹è¯"""
    try:
        redis_conversation_manager.cleanup_expired_conversations()
        
        return jsonify(success_response(
            response_text="å¯¹è¯æ¸…ç†å®Œæˆ"
        ))
        
    except Exception as e:
        return jsonify(internal_error_response(
            response_text="å¯¹è¯æ¸…ç†å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/embedding_cache_stats', methods=['GET'])
def embedding_cache_stats():
    """è·å–embeddingç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
    try:
        from common.embedding_cache_manager import get_embedding_cache_manager
        
        cache_manager = get_embedding_cache_manager()
        stats = cache_manager.get_cache_stats()
        
        return jsonify(success_response(
            response_text="è·å–embeddingç¼“å­˜ç»Ÿè®¡æˆåŠŸ",
            data=stats
        ))
        
    except Exception as e:
        logger.error(f"è·å–embeddingç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="è·å–embeddingç¼“å­˜ç»Ÿè®¡å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/embedding_cache_cleanup', methods=['POST'])
def embedding_cache_cleanup():
    """æ¸…ç©ºæ‰€æœ‰embeddingç¼“å­˜"""
    try:
        from common.embedding_cache_manager import get_embedding_cache_manager
        
        cache_manager = get_embedding_cache_manager()
        
        if not cache_manager.is_available():
            return jsonify(internal_error_response(
                response_text="Embeddingç¼“å­˜åŠŸèƒ½æœªå¯ç”¨æˆ–ä¸å¯ç”¨"
            )), 400
        
        success = cache_manager.clear_all_cache()
        
        if success:
            return jsonify(success_response(
                response_text="æ‰€æœ‰embeddingç¼“å­˜å·²æ¸…ç©º",
                data={"cleared": True}
            ))
        else:
            return jsonify(internal_error_response(
                response_text="æ¸…ç©ºembeddingç¼“å­˜å¤±è´¥"
            )), 500
        
    except Exception as e:
        logger.error(f"æ¸…ç©ºembeddingç¼“å­˜å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="æ¸…ç©ºembeddingç¼“å­˜å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

# ==================== è®­ç»ƒæ•°æ®ç®¡ç†API ====================

def validate_sql_syntax(sql: str) -> tuple[bool, str]:
    """SQLè¯­æ³•æ£€æŸ¥"""
    try:
        parsed = sqlparse.parse(sql.strip())
        
        if not parsed or not parsed[0].tokens:
            return False, "SQLè¯­æ³•é”™è¯¯ï¼šç©ºè¯­å¥"
        
        sql_upper = sql.strip().upper()
        if not any(sql_upper.startswith(keyword) for keyword in 
                  ['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE', 'ALTER', 'DROP']):
            return False, "SQLè¯­æ³•é”™è¯¯ï¼šä¸æ˜¯æœ‰æ•ˆçš„SQLè¯­å¥"
        
        return True, ""
    except Exception as e:
        return False, f"SQLè¯­æ³•é”™è¯¯ï¼š{str(e)}"

def paginate_data(data_list: list, page: int, page_size: int):
    """åˆ†é¡µç®—æ³•"""
    total = len(data_list)
    start_idx = (page - 1) * page_size
    end_idx = start_idx + page_size
    page_data = data_list[start_idx:end_idx]
    total_pages = (total + page_size - 1) // page_size
    
    return {
        "data": page_data,
        "pagination": {
            "page": page,
            "page_size": page_size,
            "total": total,
            "total_pages": total_pages,
            "has_next": end_idx < total,
            "has_prev": page > 1
        }
    }

def filter_by_type(data_list: list, training_data_type: str):
    """æŒ‰ç±»å‹ç­›é€‰ç®—æ³•"""
    if not training_data_type:
        return data_list
    
    return [
        record for record in data_list 
        if record.get('training_data_type') == training_data_type
    ]

def search_in_data(data_list: list, search_keyword: str):
    """åœ¨æ•°æ®ä¸­æœç´¢å…³é”®è¯"""
    if not search_keyword:
        return data_list
    
    keyword_lower = search_keyword.lower()
    return [
        record for record in data_list
        if (record.get('question') and keyword_lower in record['question'].lower()) or
           (record.get('content') and keyword_lower in record['content'].lower())
    ]

def get_total_training_count():
    """è·å–å½“å‰è®­ç»ƒæ•°æ®æ€»æ•°"""
    try:
        training_data = vn.get_training_data()
        if training_data is not None and not training_data.empty:
            return len(training_data)
        return 0
    except Exception as e:
        logger.warning(f"è·å–è®­ç»ƒæ•°æ®æ€»æ•°å¤±è´¥: {e}")
        return 0

def process_single_training_item(item: dict, index: int) -> dict:
    """å¤„ç†å•ä¸ªè®­ç»ƒæ•°æ®é¡¹"""
    training_type = item.get('training_data_type')
    
    if training_type == 'sql':
        sql = item.get('sql')
        if not sql:
            raise ValueError("SQLå­—æ®µæ˜¯å¿…éœ€çš„")
        
        # SQLè¯­æ³•æ£€æŸ¥
        is_valid, error_msg = validate_sql_syntax(sql)
        if not is_valid:
            raise ValueError(error_msg)
        
        question = item.get('question')
        if question:
            training_id = vn.train(question=question, sql=sql)
        else:
            training_id = vn.train(sql=sql)
            
    elif training_type == 'error_sql':
        # error_sqlä¸éœ€è¦è¯­æ³•æ£€æŸ¥
        question = item.get('question')
        sql = item.get('sql')
        if not question or not sql:
            raise ValueError("questionå’Œsqlå­—æ®µéƒ½æ˜¯å¿…éœ€çš„")
        training_id = vn.train_error_sql(question=question, sql=sql)
        
    elif training_type == 'documentation':
        content = item.get('content')
        if not content:
            raise ValueError("contentå­—æ®µæ˜¯å¿…éœ€çš„")
        training_id = vn.train(documentation=content)
        
    elif training_type == 'ddl':
        ddl = item.get('ddl')
        if not ddl:
            raise ValueError("ddlå­—æ®µæ˜¯å¿…éœ€çš„")
        training_id = vn.train(ddl=ddl)
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„è®­ç»ƒæ•°æ®ç±»å‹: {training_type}")
    
    return {
        "index": index,
        "success": True,
        "training_id": training_id,
        "type": training_type,
        "message": f"{training_type}è®­ç»ƒæ•°æ®åˆ›å»ºæˆåŠŸ"
    }

@app.route('/api/v0/training_data/stats', methods=['GET'])
def training_data_stats():
    """è·å–è®­ç»ƒæ•°æ®ç»Ÿè®¡ä¿¡æ¯API"""
    try:
        training_data = vn.get_training_data()
        
        if training_data is None or training_data.empty:
            return jsonify(success_response(
                response_text="ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ",
                data={
                    "total_count": 0,
                    "type_breakdown": {
                        "sql": 0,
                        "documentation": 0,
                        "ddl": 0,
                        "error_sql": 0
                    },
                    "type_percentages": {
                        "sql": 0.0,
                        "documentation": 0.0,
                        "ddl": 0.0,
                        "error_sql": 0.0
                    },
                    "last_updated": datetime.now().isoformat()
                }
            ))
        
        total_count = len(training_data)
        
        # ç»Ÿè®¡å„ç±»å‹æ•°é‡
        type_breakdown = {"sql": 0, "documentation": 0, "ddl": 0, "error_sql": 0}
        
        if 'training_data_type' in training_data.columns:
            type_counts = training_data['training_data_type'].value_counts()
            for data_type, count in type_counts.items():
                if data_type in type_breakdown:
                    type_breakdown[data_type] = int(count)
        
        # è®¡ç®—ç™¾åˆ†æ¯”
        type_percentages = {}
        for data_type, count in type_breakdown.items():
            type_percentages[data_type] = round(count / max(total_count, 1) * 100, 2)
        
        return jsonify(success_response(
            response_text="ç»Ÿè®¡ä¿¡æ¯è·å–æˆåŠŸ",
            data={
                "total_count": total_count,
                "type_breakdown": type_breakdown,
                "type_percentages": type_percentages,
                "last_updated": datetime.now().isoformat()
            }
        ))
        
    except Exception as e:
        logger.error(f"training_data_statsæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="è·å–ç»Ÿè®¡ä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/training_data/query', methods=['POST'])
def training_data_query():
    """åˆ†é¡µæŸ¥è¯¢è®­ç»ƒæ•°æ®API - æ”¯æŒç±»å‹ç­›é€‰ã€æœç´¢å’Œæ’åºåŠŸèƒ½"""
    try:
        req = request.get_json(force=True)
        
        # è§£æå‚æ•°ï¼Œè®¾ç½®é»˜è®¤å€¼
        page = req.get('page', 1)
        page_size = req.get('page_size', 20)
        training_data_type = req.get('training_data_type')
        sort_by = req.get('sort_by', 'id')
        sort_order = req.get('sort_order', 'desc')
        search_keyword = req.get('search_keyword')
        
        # å‚æ•°éªŒè¯
        if page < 1:
            return jsonify(bad_request_response(
                response_text="é¡µç å¿…é¡»å¤§äº0",
                missing_params=["page"]
            )), 400
        
        if page_size < 1 or page_size > 100:
            return jsonify(bad_request_response(
                response_text="æ¯é¡µå¤§å°å¿…é¡»åœ¨1-100ä¹‹é—´",
                missing_params=["page_size"]
            )), 400
        
        if search_keyword and len(search_keyword) > 100:
            return jsonify(bad_request_response(
                response_text="æœç´¢å…³é”®è¯æœ€å¤§é•¿åº¦ä¸º100å­—ç¬¦",
                missing_params=["search_keyword"]
            )), 400
        
        # è·å–è®­ç»ƒæ•°æ®
        training_data = vn.get_training_data()
        
        if training_data is None or training_data.empty:
            return jsonify(success_response(
                response_text="æŸ¥è¯¢æˆåŠŸï¼Œæš‚æ— è®­ç»ƒæ•°æ®",
                data={
                    "records": [],
                    "pagination": {
                        "page": page,
                        "page_size": page_size,
                        "total": 0,
                        "total_pages": 0,
                        "has_next": False,
                        "has_prev": False
                    },
                    "filters_applied": {
                        "training_data_type": training_data_type,
                        "search_keyword": search_keyword
                    }
                }
            ))
        
        # è½¬æ¢ä¸ºåˆ—è¡¨æ ¼å¼
        records = training_data.to_dict(orient="records")
        
        # åº”ç”¨ç­›é€‰æ¡ä»¶
        if training_data_type:
            records = filter_by_type(records, training_data_type)
        
        if search_keyword:
            records = search_in_data(records, search_keyword)
        
        # æ’åº
        if sort_by in ['id', 'training_data_type']:
            reverse = (sort_order.lower() == 'desc')
            records.sort(key=lambda x: x.get(sort_by, ''), reverse=reverse)
        
        # åˆ†é¡µ
        paginated_result = paginate_data(records, page, page_size)
        
        return jsonify(success_response(
            response_text=f"æŸ¥è¯¢æˆåŠŸï¼Œå…±æ‰¾åˆ° {paginated_result['pagination']['total']} æ¡è®°å½•",
            data={
                "records": paginated_result["data"],
                "pagination": paginated_result["pagination"],
                "filters_applied": {
                    "training_data_type": training_data_type,
                    "search_keyword": search_keyword
                }
            }
        ))
        
    except Exception as e:
        logger.error(f"training_data_queryæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="æŸ¥è¯¢è®­ç»ƒæ•°æ®å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/training_data/create', methods=['POST'])
def training_data_create():
    """åˆ›å»ºè®­ç»ƒæ•°æ®API - æ”¯æŒå•æ¡å’Œæ‰¹é‡åˆ›å»ºï¼Œæ”¯æŒå››ç§æ•°æ®ç±»å‹"""
    try:
        req = request.get_json(force=True)
        data = req.get('data')
        
        if not data:
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘å¿…éœ€å‚æ•°ï¼šdata",
                missing_params=["data"]
            )), 400
        
        # ç»Ÿä¸€å¤„ç†ä¸ºåˆ—è¡¨æ ¼å¼
        if isinstance(data, dict):
            data_list = [data]
        elif isinstance(data, list):
            data_list = data
        else:
            return jsonify(bad_request_response(
                response_text="dataå­—æ®µæ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºå¯¹è±¡æˆ–æ•°ç»„"
            )), 400
        
        # æ‰¹é‡æ“ä½œé™åˆ¶
        if len(data_list) > 50:
            return jsonify(bad_request_response(
                response_text="æ‰¹é‡æ“ä½œæœ€å¤§æ”¯æŒ50æ¡è®°å½•"
            )), 400
        
        results = []
        successful_count = 0
        type_summary = {"sql": 0, "documentation": 0, "ddl": 0, "error_sql": 0}
        
        for index, item in enumerate(data_list):
            try:
                result = process_single_training_item(item, index)
                results.append(result)
                if result['success']:
                    successful_count += 1
                    type_summary[result['type']] += 1
            except Exception as e:
                results.append({
                    "index": index,
                    "success": False,
                    "type": item.get('training_data_type', 'unknown'),
                    "error": str(e),
                    "message": "åˆ›å»ºå¤±è´¥"
                })
        
        # è·å–åˆ›å»ºåçš„æ€»è®°å½•æ•°
        current_total = get_total_training_count()
        
        # æ ¹æ®å®é™…æ‰§è¡Œç»“æœå†³å®šå“åº”çŠ¶æ€
        failed_count = len(data_list) - successful_count
        
        if failed_count == 0:
            # å…¨éƒ¨æˆåŠŸ
            return jsonify(success_response(
                response_text="è®­ç»ƒæ•°æ®åˆ›å»ºå®Œæˆ",
                data={
                    "total_requested": len(data_list),
                    "successfully_created": successful_count,
                    "failed_count": failed_count,
                    "results": results,
                    "summary": type_summary,
                    "current_total_count": current_total
                }
            ))
        elif successful_count == 0:
            # å…¨éƒ¨å¤±è´¥
            return jsonify(error_response(
                response_text="è®­ç»ƒæ•°æ®åˆ›å»ºå¤±è´¥",
                data={
                    "total_requested": len(data_list),
                    "successfully_created": successful_count,
                    "failed_count": failed_count,
                    "results": results,
                    "summary": type_summary,
                    "current_total_count": current_total
                }
            )), 400
        else:
            # éƒ¨åˆ†æˆåŠŸï¼Œéƒ¨åˆ†å¤±è´¥
            return jsonify(error_response(
                response_text=f"è®­ç»ƒæ•°æ®åˆ›å»ºéƒ¨åˆ†æˆåŠŸï¼ŒæˆåŠŸ{successful_count}æ¡ï¼Œå¤±è´¥{failed_count}æ¡",
                data={
                    "total_requested": len(data_list),
                    "successfully_created": successful_count,
                    "failed_count": failed_count,
                    "results": results,
                    "summary": type_summary,
                    "current_total_count": current_total
                }
            )), 207
        
    except Exception as e:
        logger.error(f"training_data_createæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="åˆ›å»ºè®­ç»ƒæ•°æ®å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/training_data/delete', methods=['POST'])
def training_data_delete():
    """åˆ é™¤è®­ç»ƒæ•°æ®API - æ”¯æŒæ‰¹é‡åˆ é™¤"""
    try:
        req = request.get_json(force=True)
        ids = req.get('ids', [])
        confirm = req.get('confirm', False)
        
        if not ids or not isinstance(ids, list):
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘æœ‰æ•ˆçš„IDåˆ—è¡¨",
                missing_params=["ids"]
            )), 400
        
        if not confirm:
            return jsonify(bad_request_response(
                response_text="åˆ é™¤æ“ä½œéœ€è¦ç¡®è®¤ï¼Œè¯·è®¾ç½®confirmä¸ºtrue"
            )), 400
        
        # æ‰¹é‡æ“ä½œé™åˆ¶
        if len(ids) > 50:
            return jsonify(bad_request_response(
                response_text="æ‰¹é‡åˆ é™¤æœ€å¤§æ”¯æŒ50æ¡è®°å½•"
            )), 400
        
        deleted_ids = []
        failed_ids = []
        failed_details = []
        
        for training_id in ids:
            try:
                success = vn.remove_training_data(training_id)
                if success:
                    deleted_ids.append(training_id)
                else:
                    failed_ids.append(training_id)
                    failed_details.append({
                        "id": training_id,
                        "error": "è®°å½•ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥"
                    })
            except Exception as e:
                failed_ids.append(training_id)
                failed_details.append({
                    "id": training_id,
                    "error": str(e)
                })
        
        # è·å–åˆ é™¤åçš„æ€»è®°å½•æ•°
        current_total = get_total_training_count()
        
        # æ ¹æ®å®é™…æ‰§è¡Œç»“æœå†³å®šå“åº”çŠ¶æ€
        failed_count = len(failed_ids)
        
        if failed_count == 0:
            # å…¨éƒ¨æˆåŠŸ
            return jsonify(success_response(
                response_text="è®­ç»ƒæ•°æ®åˆ é™¤å®Œæˆ",
                data={
                    "total_requested": len(ids),
                    "successfully_deleted": len(deleted_ids),
                    "failed_count": failed_count,
                    "deleted_ids": deleted_ids,
                    "failed_ids": failed_ids,
                    "failed_details": failed_details,
                    "current_total_count": current_total
                }
            ))
        elif len(deleted_ids) == 0:
            # å…¨éƒ¨å¤±è´¥
            return jsonify(error_response(
                response_text="è®­ç»ƒæ•°æ®åˆ é™¤å¤±è´¥",
                data={
                    "total_requested": len(ids),
                    "successfully_deleted": len(deleted_ids),
                    "failed_count": failed_count,
                    "deleted_ids": deleted_ids,
                    "failed_ids": failed_ids,
                    "failed_details": failed_details,
                    "current_total_count": current_total
                }
            )), 400
        else:
            # éƒ¨åˆ†æˆåŠŸï¼Œéƒ¨åˆ†å¤±è´¥
            return jsonify(error_response(
                response_text=f"è®­ç»ƒæ•°æ®åˆ é™¤éƒ¨åˆ†æˆåŠŸï¼ŒæˆåŠŸ{len(deleted_ids)}æ¡ï¼Œå¤±è´¥{failed_count}æ¡",
                data={
                    "total_requested": len(ids),
                    "successfully_deleted": len(deleted_ids),
                    "failed_count": failed_count,
                    "deleted_ids": deleted_ids,
                    "failed_ids": failed_ids,
                    "failed_details": failed_details,
                    "current_total_count": current_total
                }
            )), 207
        
    except Exception as e:
        logger.error(f"training_data_deleteæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="åˆ é™¤è®­ç»ƒæ•°æ®å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/training_data/update', methods=['POST'])
def training_data_update():
    """æ›´æ–°è®­ç»ƒæ•°æ®API - æ”¯æŒå•æ¡æ›´æ–°ï¼Œé‡‡ç”¨å…ˆåˆ é™¤åæ’å…¥ç­–ç•¥"""
    try:
        req = request.get_json(force=True)
        
        # 1. å‚æ•°éªŒè¯
        original_id = req.get('id')
        if not original_id:
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘å¿…éœ€å‚æ•°ï¼šid",
                missing_params=["id"]
            )), 400
        
        training_type = req.get('training_data_type')
        if not training_type:
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘å¿…éœ€å‚æ•°ï¼štraining_data_type",
                missing_params=["training_data_type"]
            )), 400
        
        # 2. å…ˆåˆ é™¤åŸå§‹è®°å½•
        try:
            success = vn.remove_training_data(original_id)
            if not success:
                return jsonify(bad_request_response(
                    response_text=f"åŸå§‹è®°å½• {original_id} ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥"
                )), 400
        except Exception as e:
            return jsonify(internal_error_response(
                response_text=f"åˆ é™¤åŸå§‹è®°å½•å¤±è´¥: {str(e)}"
            )), 500
        
        # 3. æ ¹æ®ç±»å‹éªŒè¯å’Œå‡†å¤‡æ–°æ•°æ®
        try:
            if training_type == 'sql':
                sql = req.get('sql')
                if not sql:
                    return jsonify(bad_request_response(
                        response_text="SQLå­—æ®µæ˜¯å¿…éœ€çš„",
                        missing_params=["sql"]
                    )), 400
                
                # SQLè¯­æ³•æ£€æŸ¥
                is_valid, error_msg = validate_sql_syntax(sql)
                if not is_valid:
                    return jsonify(bad_request_response(
                        response_text=f"SQLè¯­æ³•é”™è¯¯: {error_msg}"
                    )), 400
                
                question = req.get('question')
                if question:
                    training_id = vn.train(question=question, sql=sql)
                else:
                    training_id = vn.train(sql=sql)
                    
            elif training_type == 'error_sql':
                question = req.get('question')
                sql = req.get('sql')
                if not question or not sql:
                    return jsonify(bad_request_response(
                        response_text="questionå’Œsqlå­—æ®µéƒ½æ˜¯å¿…éœ€çš„",
                        missing_params=["question", "sql"]
                    )), 400
                training_id = vn.train_error_sql(question=question, sql=sql)
                
            elif training_type == 'documentation':
                content = req.get('content')
                if not content:
                    return jsonify(bad_request_response(
                        response_text="contentå­—æ®µæ˜¯å¿…éœ€çš„",
                        missing_params=["content"]
                    )), 400
                training_id = vn.train(documentation=content)
                
            elif training_type == 'ddl':
                ddl = req.get('ddl')
                if not ddl:
                    return jsonify(bad_request_response(
                        response_text="ddlå­—æ®µæ˜¯å¿…éœ€çš„",
                        missing_params=["ddl"]
                    )), 400
                training_id = vn.train(ddl=ddl)
                
            else:
                return jsonify(bad_request_response(
                    response_text=f"ä¸æ”¯æŒçš„è®­ç»ƒæ•°æ®ç±»å‹: {training_type}"
                )), 400
                
        except Exception as e:
            return jsonify(internal_error_response(
                response_text=f"åˆ›å»ºæ–°è®­ç»ƒæ•°æ®å¤±è´¥: {str(e)}"
            )), 500
        
        # 4. è·å–æ›´æ–°åçš„æ€»è®°å½•æ•°
        current_total = get_total_training_count()
        
        return jsonify(success_response(
            response_text="è®­ç»ƒæ•°æ®æ›´æ–°æˆåŠŸ",
            data={
                "original_id": original_id,
                "new_training_id": training_id,
                "type": training_type,
                "current_total_count": current_total
            }
        ))
        
    except Exception as e:
        logger.error(f"training_data_updateæ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="æ›´æ–°è®­ç»ƒæ•°æ®å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

# å¯¼å…¥ç°æœ‰çš„ä¸“ä¸šè®­ç»ƒå‡½æ•°
from data_pipeline.trainer.run_training import (
    train_ddl_statements,
    train_documentation_blocks, 
    train_json_question_sql_pairs,
    train_formatted_question_sql_pairs,
    train_sql_examples
)

def get_allowed_extensions(file_type: str) -> list:
    """æ ¹æ®æ–‡ä»¶ç±»å‹è¿”å›å…è®¸çš„æ‰©å±•å"""
    type_specific_extensions = {
        'ddl': ['ddl', 'sql', 'txt', ''],  # æ”¯æŒæ— æ‰©å±•å
        'markdown': ['md', 'markdown'],    # ä¸æ”¯æŒæ— æ‰©å±•å
        'sql_pair_json': ['json', 'txt', ''],  # æ”¯æŒæ— æ‰©å±•å
        'sql_pair': ['sql', 'txt', ''],   # æ”¯æŒæ— æ‰©å±•å
        'sql': ['sql', 'txt', '']         # æ”¯æŒæ— æ‰©å±•å
    }
    
    return type_specific_extensions.get(file_type, [])

def validate_file_content(content: str, file_type: str) -> dict:
    """éªŒè¯æ–‡ä»¶å†…å®¹æ ¼å¼"""
    try:
        if file_type == 'ddl':
            # æ£€æŸ¥æ˜¯å¦åŒ…å«CREATEè¯­å¥
            if not re.search(r'\bCREATE\b', content, re.IGNORECASE):
                return {'valid': False, 'error': 'æ–‡ä»¶å†…å®¹ä¸ç¬¦åˆDDLæ ¼å¼ï¼Œå¿…é¡»åŒ…å«CREATEè¯­å¥'}
            
        elif file_type == 'markdown':
            # æ£€æŸ¥æ˜¯å¦åŒ…å«##æ ‡é¢˜
            if '##' not in content:
                return {'valid': False, 'error': 'æ–‡ä»¶å†…å®¹ä¸ç¬¦åˆMarkdownæ ¼å¼ï¼Œå¿…é¡»åŒ…å«##æ ‡é¢˜'}
            
        elif file_type == 'sql_pair_json':
            # æ£€æŸ¥æ˜¯å¦ä¸ºæœ‰æ•ˆJSON
            try:
                data = json.loads(content)
                if not isinstance(data, list) or not data:
                    return {'valid': False, 'error': 'æ–‡ä»¶å†…å®¹ä¸ç¬¦åˆJSONé—®ç­”å¯¹æ ¼å¼ï¼Œå¿…é¡»æ˜¯éç©ºæ•°ç»„'}
                
                # æ£€æŸ¥æ˜¯å¦åŒ…å«questionå’Œsqlå­—æ®µ
                for item in data:
                    if not isinstance(item, dict):
                        return {'valid': False, 'error': 'æ–‡ä»¶å†…å®¹ä¸ç¬¦åˆJSONé—®ç­”å¯¹æ ¼å¼ï¼Œæ•°ç»„å…ƒç´ å¿…é¡»æ˜¯å¯¹è±¡'}
                    
                    has_question = any(key.lower() == 'question' for key in item.keys())
                    has_sql = any(key.lower() == 'sql' for key in item.keys())
                    
                    if not has_question or not has_sql:
                        return {'valid': False, 'error': 'æ–‡ä»¶å†…å®¹ä¸ç¬¦åˆJSONé—®ç­”å¯¹æ ¼å¼ï¼Œå¿…é¡»åŒ…å«questionå’Œsqlå­—æ®µ'}
                        
            except json.JSONDecodeError:
                return {'valid': False, 'error': 'æ–‡ä»¶å†…å®¹ä¸ç¬¦åˆJSONé—®ç­”å¯¹æ ¼å¼ï¼ŒJSONæ ¼å¼é”™è¯¯'}
            
        elif file_type == 'sql_pair':
            # æ£€æŸ¥æ˜¯å¦åŒ…å«Question:å’ŒSQL:
            if not re.search(r'\bQuestion\s*:', content, re.IGNORECASE):
                return {'valid': False, 'error': 'æ–‡ä»¶å†…å®¹ä¸ç¬¦åˆé—®ç­”å¯¹æ ¼å¼ï¼Œå¿…é¡»åŒ…å«Question:'}
            if not re.search(r'\bSQL\s*:', content, re.IGNORECASE):
                return {'valid': False, 'error': 'æ–‡ä»¶å†…å®¹ä¸ç¬¦åˆé—®ç­”å¯¹æ ¼å¼ï¼Œå¿…é¡»åŒ…å«SQL:'}
            
        elif file_type == 'sql':
            # æ£€æŸ¥æ˜¯å¦åŒ…å«;åˆ†éš”ç¬¦
            if ';' not in content:
                return {'valid': False, 'error': 'æ–‡ä»¶å†…å®¹ä¸ç¬¦åˆSQLæ ¼å¼ï¼Œå¿…é¡»åŒ…å«;åˆ†éš”ç¬¦'}
        
        return {'valid': True}
        
    except Exception as e:
        return {'valid': False, 'error': f'æ–‡ä»¶å†…å®¹éªŒè¯å¤±è´¥: {str(e)}'}

@app.route('/api/v0/training_data/upload', methods=['POST'])
def upload_training_data():
    """ä¸Šä¼ è®­ç»ƒæ•°æ®æ–‡ä»¶API - æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼çš„è‡ªåŠ¨è§£æå’Œå¯¼å…¥"""
    try:
        # 1. å‚æ•°éªŒè¯
        if 'file' not in request.files:
            return jsonify(bad_request_response("æœªæä¾›æ–‡ä»¶"))
        
        file = request.files['file']
        if file.filename == '':
            return jsonify(bad_request_response("æœªé€‰æ‹©æ–‡ä»¶"))
        
        # è·å–file_typeå‚æ•°
        file_type = request.form.get('file_type')
        if not file_type:
            return jsonify(bad_request_response("ç¼ºå°‘å¿…éœ€å‚æ•°ï¼šfile_type"))
        
        # éªŒè¯file_typeå‚æ•°
        valid_file_types = ['ddl', 'markdown', 'sql_pair_json', 'sql_pair', 'sql']
        if file_type not in valid_file_types:
            return jsonify(bad_request_response(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š{file_type}ï¼Œæ”¯æŒçš„ç±»å‹ï¼š{', '.join(valid_file_types)}"))
        
        # 2. æ–‡ä»¶å¤§å°éªŒè¯ (500KB)
        file.seek(0, 2)
        file_size = file.tell()
        file.seek(0)
        
        if file_size > 500 * 1024:  # 500KB
            return jsonify(bad_request_response("æ–‡ä»¶å¤§å°ä¸èƒ½è¶…è¿‡500KB"))
        
        # 3. éªŒè¯æ–‡ä»¶æ‰©å±•åï¼ˆåŸºäºfile_typeï¼‰
        filename = secure_filename(file.filename)
        allowed_extensions = get_allowed_extensions(file_type)
        file_ext = filename.split('.')[-1].lower() if '.' in filename else ''
        
        if file_ext not in allowed_extensions:
            # æ„å»ºå‹å¥½çš„é”™è¯¯ä¿¡æ¯
            non_empty_extensions = [ext for ext in allowed_extensions if ext]
            if '' in allowed_extensions:
                ext_message = f"{', '.join(non_empty_extensions)} æˆ–æ— æ‰©å±•å"
            else:
                ext_message = ', '.join(non_empty_extensions)
            return jsonify(bad_request_response(f"æ–‡ä»¶ç±»å‹ {file_type} ä¸æ”¯æŒçš„æ–‡ä»¶æ‰©å±•åï¼š{file_ext}ï¼Œæ”¯æŒçš„æ‰©å±•åï¼š{ext_message}"))
        
        # 4. è¯»å–æ–‡ä»¶å†…å®¹å¹¶éªŒè¯æ ¼å¼
        file.seek(0)
        content = file.read().decode('utf-8')
        
        # æ ¼å¼éªŒè¯
        validation_result = validate_file_content(content, file_type)
        if not validation_result['valid']:
            return jsonify(bad_request_response(validation_result['error']))
        
        # 5. åˆ›å»ºä¸´æ—¶æ–‡ä»¶ï¼ˆå¤ç”¨ç°æœ‰å‡½æ•°éœ€è¦æ–‡ä»¶è·¯å¾„ï¼‰
        temp_file_path = None
        
        try:
            with tempfile.NamedTemporaryFile(mode='w', delete=False, suffix='.tmp', encoding='utf-8') as tmp_file:
                tmp_file.write(content)
                temp_file_path = tmp_file.name
            
            # 6. æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨ç°æœ‰çš„è®­ç»ƒå‡½æ•°
            if file_type == 'ddl':
                train_ddl_statements(temp_file_path)
                
            elif file_type == 'markdown':
                train_documentation_blocks(temp_file_path)
                
            elif file_type == 'sql_pair_json':
                train_json_question_sql_pairs(temp_file_path)
                
            elif file_type == 'sql_pair':
                train_formatted_question_sql_pairs(temp_file_path)
                
            elif file_type == 'sql':
                train_sql_examples(temp_file_path)
            
            return jsonify(success_response(
                response_text=f"æ–‡ä»¶ä¸Šä¼ å¹¶è®­ç»ƒæˆåŠŸï¼š{filename}",
                data={
                    "filename": filename,
                    "file_type": file_type,
                    "file_size": file_size,
                    "status": "completed"
                }
            ))
            
        except Exception as e:
            logger.error(f"è®­ç»ƒå¤±è´¥: {str(e)}")
            return jsonify(internal_error_response(f"è®­ç»ƒå¤±è´¥: {str(e)}"))
        finally:
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            if temp_file_path and os.path.exists(temp_file_path):
                try:
                    os.unlink(temp_file_path)
                except Exception as e:
                    logger.warning(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {str(e)}")
        
    except Exception as e:
        logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"))

def get_db_connection():
    """è·å–æ•°æ®åº“è¿æ¥"""
    try:
        from app_config import PGVECTOR_CONFIG
        return psycopg2.connect(**PGVECTOR_CONFIG)
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        raise

def get_db_connection_for_transaction():
    """è·å–ç”¨äºäº‹åŠ¡æ“ä½œçš„æ•°æ®åº“è¿æ¥ï¼ˆéè‡ªåŠ¨æäº¤æ¨¡å¼ï¼‰"""
    try:
        from app_config import PGVECTOR_CONFIG
        conn = psycopg2.connect(**PGVECTOR_CONFIG)
        conn.autocommit = False  # è®¾ç½®ä¸ºéè‡ªåŠ¨æäº¤æ¨¡å¼ï¼Œå…è®¸æ‰‹åŠ¨æ§åˆ¶äº‹åŠ¡
        return conn
    except Exception as e:
        logger.error(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {str(e)}")
        raise

@app.route('/api/v0/training_data/combine', methods=['POST'])
def combine_training_data():
    """åˆå¹¶è®­ç»ƒæ•°æ®API - æ”¯æŒåˆå¹¶é‡å¤è®°å½•"""
    try:
        # 1. å‚æ•°éªŒè¯
        data = request.get_json()
        if not data:
            return jsonify(bad_request_response("è¯·æ±‚ä½“ä¸èƒ½ä¸ºç©º"))
        
        collection_names = data.get('collection_names', [])
        if not collection_names or not isinstance(collection_names, list):
            return jsonify(bad_request_response("collection_names å‚æ•°å¿…é¡»æ˜¯éç©ºæ•°ç»„"))
        
        # éªŒè¯é›†åˆåç§°
        valid_collections = ['sql', 'ddl', 'documentation', 'error_sql']
        invalid_collections = [name for name in collection_names if name not in valid_collections]
        if invalid_collections:
            return jsonify(bad_request_response(f"ä¸æ”¯æŒçš„é›†åˆåç§°: {invalid_collections}"))
        
        dry_run = data.get('dry_run', True)
        keep_strategy = data.get('keep_strategy', 'first')
        
        if keep_strategy not in ['first', 'last', 'by_metadata_time']:
            return jsonify(bad_request_response("keep_strategy å¿…é¡»æ˜¯ 'first', 'last' æˆ– 'by_metadata_time'"))
        
        # 2. è·å–æ•°æ®åº“è¿æ¥ï¼ˆç”¨äºäº‹åŠ¡æ“ä½œï¼‰
        conn = get_db_connection_for_transaction()
        cursor = conn.cursor()
        
        # 3. æŸ¥æ‰¾é‡å¤è®°å½•
        duplicate_groups = []
        total_before = 0
        total_duplicates = 0
        collections_stats = {}
        
        for collection_name in collection_names:
            # è·å–é›†åˆID
            cursor.execute(
                "SELECT uuid FROM langchain_pg_collection WHERE name = %s",
                (collection_name,)
            )
            collection_result = cursor.fetchone()
            if not collection_result:
                continue
            
            collection_id = collection_result[0]
            
            # ç»Ÿè®¡è¯¥é›†åˆçš„è®°å½•æ•°
            cursor.execute(
                "SELECT COUNT(*) FROM langchain_pg_embedding WHERE collection_id = %s",
                (collection_id,)
            )
            collection_before = cursor.fetchone()[0]
            total_before += collection_before
            
            # æŸ¥æ‰¾é‡å¤è®°å½•
            if keep_strategy in ['first', 'last']:
                order_by = "id"
            else:
                order_by = "COALESCE((cmetadata->>'createdat')::timestamp, '1970-01-01'::timestamp) DESC, id"
            
            cursor.execute(f"""
                SELECT document, COUNT(*) as duplicate_count, 
                       array_agg(id ORDER BY {order_by}) as record_ids
                FROM langchain_pg_embedding 
                WHERE collection_id = %s 
                GROUP BY document 
                HAVING COUNT(*) > 1
            """, (collection_id,))
            
            collection_duplicates = 0
            for row in cursor.fetchall():
                document_content, duplicate_count, record_ids = row
                collection_duplicates += duplicate_count - 1  # å‡å»è¦ä¿ç•™çš„ä¸€æ¡
                
                # æ ¹æ®ä¿ç•™ç­–ç•¥é€‰æ‹©è¦ä¿ç•™çš„è®°å½•
                if keep_strategy == 'first':
                    keep_id = record_ids[0]
                    remove_ids = record_ids[1:]
                elif keep_strategy == 'last':
                    keep_id = record_ids[-1]
                    remove_ids = record_ids[:-1]
                else:  # by_metadata_time
                    keep_id = record_ids[0]  # å·²ç»æŒ‰æ—¶é—´æ’åº
                    remove_ids = record_ids[1:]
                
                duplicate_groups.append({
                    "collection_name": collection_name,
                    "document_content": document_content[:100] + "..." if len(document_content) > 100 else document_content,
                    "duplicate_count": duplicate_count,
                    "kept_record_id" if not dry_run else "records_to_keep": keep_id,
                    "removed_record_ids" if not dry_run else "records_to_remove": remove_ids
                })
            
            total_duplicates += collection_duplicates
            collections_stats[collection_name] = {
                "before": collection_before,
                "after": collection_before - collection_duplicates,
                "duplicates_removed" if not dry_run else "duplicates_to_remove": collection_duplicates
            }
        
        # 4. æ‰§è¡Œåˆå¹¶æ“ä½œï¼ˆå¦‚æœä¸æ˜¯dry_runï¼‰
        if not dry_run:
            try:
                # è¿æ¥å·²ç»è®¾ç½®ä¸ºéè‡ªåŠ¨æäº¤æ¨¡å¼ï¼Œç›´æ¥å¼€å§‹äº‹åŠ¡
                for group in duplicate_groups:
                    remove_ids = group["removed_record_ids"]
                    if remove_ids:
                        cursor.execute(
                            "DELETE FROM langchain_pg_embedding WHERE id = ANY(%s)",
                            (remove_ids,)
                        )
                
                conn.commit()
                
            except Exception as e:
                conn.rollback()
                return jsonify(internal_error_response(f"åˆå¹¶æ“ä½œå¤±è´¥: {str(e)}"))
        
        # 5. æ„å»ºå“åº”
        total_after = total_before - total_duplicates
        
        summary = {
            "total_records_before": total_before,
            "total_records_after": total_after,
            "duplicates_removed" if not dry_run else "duplicates_to_remove": total_duplicates,
            "collections_stats": collections_stats
        }
        
        if dry_run:
            response_text = f"å‘ç° {total_duplicates} æ¡é‡å¤è®°å½•ï¼Œé¢„è®¡åˆ é™¤åå°†ä» {total_before} æ¡å‡å°‘åˆ° {total_after} æ¡è®°å½•"
            data_key = "duplicate_groups"
        else:
            response_text = f"æˆåŠŸåˆå¹¶é‡å¤è®°å½•ï¼Œåˆ é™¤äº† {total_duplicates} æ¡é‡å¤è®°å½•ï¼Œä» {total_before} æ¡å‡å°‘åˆ° {total_after} æ¡è®°å½•"
            data_key = "merged_groups"
        
        return jsonify(success_response(
            response_text=response_text,
            data={
                "dry_run": dry_run,
                "collections_processed": collection_names,
                "summary": summary,
                data_key: duplicate_groups
            }
        ))
        
    except Exception as e:
        return jsonify(internal_error_response(f"åˆå¹¶æ“ä½œå¤±è´¥: {str(e)}"))
    finally:
        if 'cursor' in locals():
            cursor.close()
        if 'conn' in locals():
            conn.close()

# ==================== React Agent æ‰©å±•API ====================

@app.route('/api/v0/react/users/<user_id>/conversations', methods=['GET'])
async def get_user_conversations_react(user_id: str):
    """å¼‚æ­¥è·å–ç”¨æˆ·çš„èŠå¤©è®°å½•åˆ—è¡¨ï¼ˆä» custom_react_agent è¿ç§»ï¼‰"""
    global _react_agent_instance
    
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        limit = request.args.get('limit', 10, type=int)
        
        # é™åˆ¶limitçš„èŒƒå›´
        limit = max(1, min(limit, 50))  # é™åˆ¶åœ¨1-50ä¹‹é—´
        
        logger.info(f"ğŸ“‹ å¼‚æ­¥è·å–ç”¨æˆ· {user_id} çš„å¯¹è¯åˆ—è¡¨ï¼Œé™åˆ¶ {limit} æ¡")
        
        # ç¡®ä¿Agentå¯ç”¨
        if not await ensure_agent_ready():
            return jsonify(service_unavailable_response(
                response_text="Agent æœªå°±ç»ª"
            )), 503
        
        # ç›´æ¥è°ƒç”¨å¼‚æ­¥æ–¹æ³•
        conversations = await _react_agent_instance.get_user_recent_conversations(user_id, limit)
        
        return jsonify(success_response(
            response_text="è·å–ç”¨æˆ·å¯¹è¯åˆ—è¡¨æˆåŠŸ",
            data={
                "user_id": user_id,
                "conversations": conversations,
                "total_count": len(conversations),
                "limit": limit
            }
        )), 200
        
    except Exception as e:
        logger.error(f"âŒ å¼‚æ­¥è·å–ç”¨æˆ· {user_id} å¯¹è¯åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify(internal_error_response(
            response_text=f"è·å–ç”¨æˆ·å¯¹è¯åˆ—è¡¨å¤±è´¥: {str(e)}"
        )), 500

@app.route('/api/v0/react/conversations/<thread_id>', methods=['GET'])
async def get_user_conversation_detail_react(thread_id: str):
    """å¼‚æ­¥è·å–ç‰¹å®šå¯¹è¯çš„è¯¦ç»†å†å²ï¼ˆä» custom_react_agent è¿ç§»ï¼‰"""
    global _react_agent_instance
    
    try:
        # ä»thread_idä¸­æå–user_id
        user_id = thread_id.split(':')[0] if ':' in thread_id else 'unknown'
        
        logger.info(f"ğŸ“– å¼‚æ­¥è·å–ç”¨æˆ· {user_id} çš„å¯¹è¯ {thread_id} è¯¦æƒ…")
        
        # ç¡®ä¿Agentå¯ç”¨
        if not await ensure_agent_ready():
            return jsonify(service_unavailable_response(
                response_text="Agent æœªå°±ç»ª"
            )), 503
        
        # è·å–æŸ¥è¯¢å‚æ•°
        include_tools = request.args.get('include_tools', 'false').lower() == 'true'
        
        # ç›´æ¥è°ƒç”¨å¼‚æ­¥æ–¹æ³•
        conversation_data = await _react_agent_instance.get_conversation_history(thread_id, include_tools=include_tools)
        messages = conversation_data.get("messages", [])
        
        logger.info(f"âœ… å¼‚æ­¥æˆåŠŸè·å–å¯¹è¯å†å²ï¼Œæ¶ˆæ¯æ•°é‡: {len(messages)}")
        
        if not messages:
            return jsonify(not_found_response(
                response_text=f"æœªæ‰¾åˆ°å¯¹è¯ {thread_id}"
            )), 404
        
        # æ ¼å¼åŒ–æ¶ˆæ¯
        formatted_messages = []
        for msg in messages:
            formatted_msg = {
                "message_id": msg["id"],  # id -> message_id
                "role": msg["type"],      # type -> role
                "content": msg["content"],
                "timestamp": _format_timestamp_to_china_time(msg["timestamp"])  # è½¬æ¢ä¸ºä¸­å›½æ—¶åŒº
            }
            formatted_messages.append(formatted_msg)
        
        return jsonify(success_response(
            response_text="è·å–å¯¹è¯è¯¦æƒ…æˆåŠŸ",
            data={
                "user_id": user_id,
                "thread_id": thread_id,
                "conversation_id": thread_id,  # æ–°å¢conversation_idå­—æ®µ
                "message_count": len(formatted_messages),
                "messages": formatted_messages,
                "created_at": conversation_data.get("thread_created_at"),  # å·²ç»åŒ…å«æ¯«ç§’
                "total_checkpoints": conversation_data.get("total_checkpoints", 0)
            }
        )), 200
        
    except Exception as e:
        import traceback
        logger.error(f"âŒ å¼‚æ­¥è·å–å¯¹è¯ {thread_id} è¯¦æƒ…å¤±è´¥: {e}")
        logger.error(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        return jsonify(internal_error_response(
            response_text=f"è·å–å¯¹è¯è¯¦æƒ…å¤±è´¥: {str(e)}"
        )), 500

@app.route('/api/test/redis', methods=['GET'])
def test_redis_connection():
    """æµ‹è¯•Redisè¿æ¥å’ŒåŸºæœ¬æŸ¥è¯¢ï¼ˆä» custom_react_agent è¿ç§»ï¼‰"""
    try:
        import redis
        
        # åˆ›å»ºRedisè¿æ¥
        r = redis.Redis(host='localhost', port=6379, decode_responses=True)
        r.ping()
        
        # æ‰«æcheckpoint keys
        pattern = "checkpoint:*"
        keys = []
        cursor = 0
        count = 0
        
        while True:
            cursor, batch = r.scan(cursor=cursor, match=pattern, count=100)
            keys.extend(batch)
            count += len(batch)
            if cursor == 0 or count > 500:  # é™åˆ¶æ‰«ææ•°é‡
                break
        
        # ç»Ÿè®¡ç”¨æˆ·
        users = {}
        for key in keys:
            try:
                parts = key.split(':')
                if len(parts) >= 2:
                    user_id = parts[1]
                    users[user_id] = users.get(user_id, 0) + 1
            except:
                continue
        
        r.close()
        
        return jsonify({
            "success": True,
            "data": {
                "redis_connected": True,
                "total_checkpoint_keys": len(keys),
                "users_found": list(users.keys()),
                "user_key_counts": users,
                "sample_keys": keys[:5] if keys else []
            },
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"âŒ Redisæµ‹è¯•å¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/api/v0/react/direct/users/<user_id>/conversations', methods=['GET'])
def test_get_user_conversations_simple(user_id: str):
    """ç›´æ¥ä»Redisè·å–ç”¨æˆ·å¯¹è¯åˆ—è¡¨"""
    try:
        limit = request.args.get('limit', 10, type=int)
        limit = max(1, min(limit, 50))
        
        logger.info(f"ğŸ“‹ è·å–ç”¨æˆ· {user_id} çš„å¯¹è¯åˆ—è¡¨ï¼ˆç›´æ¥Redisæ–¹å¼ï¼‰")
        
        # ä½¿ç”¨ç®€å•RedisæŸ¥è¯¢
        conversations = get_user_conversations_simple_sync(user_id, limit)
        
        return jsonify(success_response(
            response_text="è·å–ç”¨æˆ·å¯¹è¯åˆ—è¡¨æˆåŠŸ",
            data={
                "user_id": user_id,
                "conversations": conversations,
                "total_count": len(conversations),
                "limit": limit
            }
        )), 200
        
    except Exception as e:
        logger.error(f"âŒ è·å–ç”¨æˆ·å¯¹è¯åˆ—è¡¨å¤±è´¥: {e}")
        return jsonify(internal_error_response(
            response_text=f"è·å–ç”¨æˆ·å¯¹è¯åˆ—è¡¨å¤±è´¥: {str(e)}"
        )), 500

@app.route('/api/v0/react/direct/conversations/<thread_id>', methods=['GET'])
def get_conversation_detail_api(thread_id: str):
    """
    è·å–ç‰¹å®šå¯¹è¯çš„è¯¦ç»†ä¿¡æ¯ - æ”¯æŒinclude_toolså¼€å…³å‚æ•°ï¼ˆä» custom_react_agent è¿ç§»ï¼‰
    
    Query Parameters:
        - include_tools: bool, æ˜¯å¦åŒ…å«å·¥å…·è°ƒç”¨ä¿¡æ¯ï¼Œé»˜è®¤false
                        true: è¿”å›å®Œæ•´å¯¹è¯ï¼ˆhuman/ai/tool/systemï¼‰
                        false: åªè¿”å›human/aiæ¶ˆæ¯ï¼Œæ¸…ç†å·¥å…·è°ƒç”¨ä¿¡æ¯
        - user_id: str, å¯é€‰çš„ç”¨æˆ·IDéªŒè¯
        
    Examples:
        GET /api/conversations/wang:20250709195048728?include_tools=true   # å®Œæ•´æ¨¡å¼
        GET /api/conversations/wang:20250709195048728?include_tools=false  # ç®€åŒ–æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
        GET /api/conversations/wang:20250709195048728                      # ç®€åŒ–æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
    """
    try:
        # è·å–æŸ¥è¯¢å‚æ•°
        include_tools = request.args.get('include_tools', 'false').lower() == 'true'
        user_id = request.args.get('user_id')
        
        # éªŒè¯thread_idæ ¼å¼
        if ':' not in thread_id:
            return jsonify({
                "success": False,
                "error": "Invalid thread_id format. Expected format: user_id:timestamp",
                "timestamp": datetime.now().isoformat()
            }), 400
        
        # å¦‚æœæä¾›äº†user_idï¼ŒéªŒè¯thread_idæ˜¯å¦å±äºè¯¥ç”¨æˆ·
        thread_user_id = thread_id.split(':')[0]
        if user_id and thread_user_id != user_id:
            return jsonify(bad_request_response(
                response_text=f"Thread ID {thread_id} does not belong to user {user_id}"
            )), 400
        
        logger.info(f"ğŸ“– è·å–å¯¹è¯è¯¦æƒ… - Thread: {thread_id}, Include Tools: {include_tools}")
        
        # æ£€æŸ¥enhanced_redis_apiæ˜¯å¦å¯ç”¨
        if get_conversation_detail_from_redis is None:
            return jsonify(service_unavailable_response(
                response_text="enhanced_redis_api æ¨¡å—ä¸å¯ç”¨"
            )), 503
        
        # ä»Redisè·å–å¯¹è¯è¯¦æƒ…ï¼ˆä½¿ç”¨æˆ‘ä»¬çš„æ–°å‡½æ•°ï¼‰
        result = get_conversation_detail_from_redis(thread_id, include_tools)
        
        if not result['success']:
            logger.warning(f"âš ï¸ è·å–å¯¹è¯è¯¦æƒ…å¤±è´¥: {result['error']}")
            return jsonify(internal_error_response(
                response_text=result['error']
            )), 404
        
        # æ·»åŠ APIå…ƒæ•°æ®
        result['data']['api_metadata'] = {
            "timestamp": datetime.now().isoformat(),
            "api_version": "v1",
            "endpoint": "get_conversation_detail",
            "query_params": {
                "include_tools": include_tools,
                "user_id": user_id
            }
        }
        
        mode_desc = "å®Œæ•´æ¨¡å¼" if include_tools else "ç®€åŒ–æ¨¡å¼"
        logger.info(f"âœ… æˆåŠŸè·å–å¯¹è¯è¯¦æƒ… - Messages: {result['data']['message_count']}, Mode: {mode_desc}")
        
        return jsonify(success_response(
            response_text=f"è·å–å¯¹è¯è¯¦æƒ…æˆåŠŸ ({mode_desc})",
            data=result['data']
        )), 200
        
    except Exception as e:
        import traceback
        logger.error(f"âŒ è·å–å¯¹è¯è¯¦æƒ…å¼‚å¸¸: {e}")
        logger.error(f"âŒ è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
        
        return jsonify(internal_error_response(
            response_text=f"è·å–å¯¹è¯è¯¦æƒ…å¤±è´¥: {str(e)}"
        )), 500

@app.route('/api/v0/react/direct/conversations/<thread_id>/compare', methods=['GET'])
def compare_conversation_modes_api(thread_id: str):
    """
    æ¯”è¾ƒå®Œæ•´æ¨¡å¼å’Œç®€åŒ–æ¨¡å¼çš„å¯¹è¯å†…å®¹
    ç”¨äºè°ƒè¯•å’Œç†è§£ä¸¤ç§æ¨¡å¼çš„å·®å¼‚ï¼ˆä» custom_react_agent è¿ç§»ï¼‰
    
    Examples:
        GET /api/conversations/wang:20250709195048728/compare
    """
    try:
        logger.info(f"ğŸ” æ¯”è¾ƒå¯¹è¯æ¨¡å¼ - Thread: {thread_id}")
        
        # æ£€æŸ¥enhanced_redis_apiæ˜¯å¦å¯ç”¨
        if get_conversation_detail_from_redis is None:
            return jsonify({
                "success": False,
                "error": "enhanced_redis_api æ¨¡å—ä¸å¯ç”¨",
                "timestamp": datetime.now().isoformat()
            }), 503
        
        # è·å–å®Œæ•´æ¨¡å¼
        full_result = get_conversation_detail_from_redis(thread_id, include_tools=True)
        
        # è·å–ç®€åŒ–æ¨¡å¼
        simple_result = get_conversation_detail_from_redis(thread_id, include_tools=False)
        
        if not (full_result['success'] and simple_result['success']):
            return jsonify({
                "success": False,
                "error": "æ— æ³•è·å–å¯¹è¯æ•°æ®è¿›è¡Œæ¯”è¾ƒ",
                "timestamp": datetime.now().isoformat()
            }), 404
        
        # æ„å»ºæ¯”è¾ƒç»“æœ
        comparison = {
            "thread_id": thread_id,
            "full_mode": {
                "message_count": full_result['data']['message_count'],
                "stats": full_result['data']['stats'],
                "sample_messages": full_result['data']['messages'][:3]  # åªæ˜¾ç¤ºå‰3æ¡ä½œä¸ºç¤ºä¾‹
            },
            "simple_mode": {
                "message_count": simple_result['data']['message_count'],
                "stats": simple_result['data']['stats'],
                "sample_messages": simple_result['data']['messages'][:3]  # åªæ˜¾ç¤ºå‰3æ¡ä½œä¸ºç¤ºä¾‹
            },
            "comparison_summary": {
                "message_count_difference": full_result['data']['message_count'] - simple_result['data']['message_count'],
                "tools_filtered_out": full_result['data']['stats'].get('tool_messages', 0),
                "ai_messages_with_tools": full_result['data']['stats'].get('messages_with_tools', 0),
                "filtering_effectiveness": "æœ‰æ•ˆ" if (full_result['data']['message_count'] - simple_result['data']['message_count']) > 0 else "æ— å·®å¼‚"
            },
            "metadata": {
                "timestamp": datetime.now().isoformat(),
                "note": "sample_messages åªæ˜¾ç¤ºå‰3æ¡æ¶ˆæ¯ä½œä¸ºç¤ºä¾‹ï¼Œå®Œæ•´æ¶ˆæ¯è¯·ä½¿ç”¨ç›¸åº”çš„è¯¦æƒ…API"
            }
        }
        
        logger.info(f"âœ… æ¨¡å¼æ¯”è¾ƒå®Œæˆ - å®Œæ•´: {comparison['full_mode']['message_count']}, ç®€åŒ–: {comparison['simple_mode']['message_count']}")
        
        return jsonify({
            "success": True,
            "data": comparison,
            "timestamp": datetime.now().isoformat()
        }), 200
        
    except Exception as e:
        logger.error(f"âŒ å¯¹è¯æ¨¡å¼æ¯”è¾ƒå¤±è´¥: {e}")
        return jsonify({
            "success": False,
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }), 500

@app.route('/api/v0/react/direct/conversations/<thread_id>/summary', methods=['GET'])
def get_conversation_summary_api(thread_id: str):
    """
    è·å–å¯¹è¯æ‘˜è¦ä¿¡æ¯ï¼ˆåªåŒ…å«åŸºæœ¬ç»Ÿè®¡ï¼Œä¸è¿”å›å…·ä½“æ¶ˆæ¯ï¼‰ï¼ˆä» custom_react_agent è¿ç§»ï¼‰
    
    Query Parameters:
        - include_tools: bool, å½±å“ç»Ÿè®¡ä¿¡æ¯çš„è®¡ç®—æ–¹å¼
        
    Examples:
        GET /api/conversations/wang:20250709195048728/summary?include_tools=true
    """
    try:
        include_tools = request.args.get('include_tools', 'false').lower() == 'true'
        
        # éªŒè¯thread_idæ ¼å¼
        if ':' not in thread_id:
            return jsonify(bad_request_response(
                response_text="Invalid thread_id format. Expected format: user_id:timestamp"
            )), 400
        
        logger.info(f"ğŸ“Š è·å–å¯¹è¯æ‘˜è¦ - Thread: {thread_id}, Include Tools: {include_tools}")
        
        # æ£€æŸ¥enhanced_redis_apiæ˜¯å¦å¯ç”¨
        if get_conversation_detail_from_redis is None:
            return jsonify(service_unavailable_response(
                response_text="enhanced_redis_api æ¨¡å—ä¸å¯ç”¨"
            )), 503
        
        # è·å–å®Œæ•´å¯¹è¯ä¿¡æ¯
        result = get_conversation_detail_from_redis(thread_id, include_tools)
        
        if not result['success']:
            return jsonify(internal_error_response(
                response_text=result['error']
            )), 404
        
        # åªè¿”å›æ‘˜è¦ä¿¡æ¯ï¼Œä¸åŒ…å«å…·ä½“æ¶ˆæ¯
        data = result['data']
        summary = {
            "thread_id": data['thread_id'],
            "user_id": data['user_id'],
            "include_tools": data['include_tools'],
            "message_count": data['message_count'],
            "stats": data['stats'],
            "metadata": data['metadata'],
            "first_message_preview": None,
            "last_message_preview": None,
            "conversation_preview": None
        }
        
        # æ·»åŠ æ¶ˆæ¯é¢„è§ˆ
        messages = data.get('messages', [])
        if messages:
            # ç¬¬ä¸€æ¡humanæ¶ˆæ¯é¢„è§ˆ
            for msg in messages:
                if msg['role'] == 'human':
                    content = str(msg['content'])
                    summary['first_message_preview'] = content[:100] + "..." if len(content) > 100 else content
                    break
            
            # æœ€åä¸€æ¡aiæ¶ˆæ¯é¢„è§ˆ
            for msg in reversed(messages):
                if msg['role'] == 'ai' and msg.get('content', '').strip():
                    content = str(msg['content'])
                    summary['last_message_preview'] = content[:100] + "..." if len(content) > 100 else content
                    break
            
            # ç”Ÿæˆå¯¹è¯é¢„è§ˆï¼ˆç¬¬ä¸€æ¡humanæ¶ˆæ¯ï¼‰
            summary['conversation_preview'] = summary['first_message_preview']
        
        # æ·»åŠ APIå…ƒæ•°æ®
        summary['api_metadata'] = {
            "timestamp": datetime.now().isoformat(),
            "api_version": "v1",
            "endpoint": "get_conversation_summary"
        }
        
        logger.info(f"âœ… æˆåŠŸè·å–å¯¹è¯æ‘˜è¦")
        
        return jsonify(success_response(
            response_text="è·å–å¯¹è¯æ‘˜è¦æˆåŠŸ",
            data=summary
        )), 200
        
    except Exception as e:
        logger.error(f"âŒ è·å–å¯¹è¯æ‘˜è¦å¤±è´¥: {e}")
        return jsonify(internal_error_response(
            response_text=f"è·å–å¯¹è¯æ‘˜è¦å¤±è´¥: {str(e)}"
        )), 500



# Data Pipeline å…¨å±€å˜é‡ - ä» citu_app.py è¿ç§»
data_pipeline_manager = None
data_pipeline_file_manager = None

def get_data_pipeline_manager():
    """è·å–Data Pipelineç®¡ç†å™¨å•ä¾‹ï¼ˆä» citu_app.py è¿ç§»ï¼‰"""
    global data_pipeline_manager
    if data_pipeline_manager is None:
        data_pipeline_manager = SimpleWorkflowManager()
    return data_pipeline_manager

def get_data_pipeline_file_manager():
    """è·å–Data Pipelineæ–‡ä»¶ç®¡ç†å™¨å•ä¾‹ï¼ˆä» citu_app.py è¿ç§»ï¼‰"""
    global data_pipeline_file_manager
    if data_pipeline_file_manager is None:
        data_pipeline_file_manager = SimpleFileManager()
    return data_pipeline_file_manager

# ==================== QAç¼“å­˜ç®¡ç†API (ä» citu_app.py è¿ç§») ====================

@app.route('/api/v0/qa_cache_stats', methods=['GET'])
def qa_cache_stats():
    """è·å–é—®ç­”ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯ï¼ˆä» citu_app.py è¿ç§»ï¼‰"""
    try:
        stats = redis_conversation_manager.get_qa_cache_stats()
        
        return jsonify(success_response(
            response_text="è·å–é—®ç­”ç¼“å­˜ç»Ÿè®¡æˆåŠŸ",
            data=stats
        ))
        
    except Exception as e:
        logger.error(f"è·å–é—®ç­”ç¼“å­˜ç»Ÿè®¡å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="è·å–é—®ç­”ç¼“å­˜ç»Ÿè®¡å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/qa_cache_cleanup', methods=['POST'])
def qa_cache_cleanup():
    """æ¸…ç©ºæ‰€æœ‰é—®ç­”ç¼“å­˜ï¼ˆä» citu_app.py è¿ç§»ï¼‰"""
    try:
        if not redis_conversation_manager.is_available():
            return jsonify(internal_error_response(
                response_text="Redisè¿æ¥ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œæ¸…ç†æ“ä½œ"
            )), 500
        
        deleted_count = redis_conversation_manager.clear_all_qa_cache()
        
        return jsonify(success_response(
            response_text="é—®ç­”ç¼“å­˜æ¸…ç†å®Œæˆ",
            data={
                "deleted_count": deleted_count,
                "cleared": deleted_count > 0,
                "cleanup_time": datetime.now().isoformat()
            }
        ))
        
    except Exception as e:
        logger.error(f"æ¸…ç©ºé—®ç­”ç¼“å­˜å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="æ¸…ç©ºé—®ç­”ç¼“å­˜å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

# ==================== Database API (ä» citu_app.py è¿ç§») ====================

@app.route('/api/v0/database/tables', methods=['POST'])
def get_database_tables():
    """
    è·å–æ•°æ®åº“è¡¨åˆ—è¡¨ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    
    è¯·æ±‚ä½“:
    {
        "db_connection": "postgresql://postgres:postgres@192.168.67.1:5432/highway_db",  // å¯é€‰ï¼Œä¸ä¼ åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        "schema": "public,ods",  // å¯é€‰ï¼Œæ”¯æŒå¤šä¸ªschemaç”¨é€—å·åˆ†éš”ï¼Œé»˜è®¤ä¸ºpublic
        "table_name_pattern": "ods_*"  // å¯é€‰ï¼Œè¡¨åæ¨¡å¼åŒ¹é…ï¼Œæ”¯æŒé€šé…ç¬¦ï¼šods_*ã€*_dimã€*fact*ã€ods_%
    }
    
    å“åº”:
    {
        "success": true,
        "code": 200,
        "message": "è·å–è¡¨åˆ—è¡¨æˆåŠŸ",
        "data": {
            "tables": ["public.table1", "public.table2", "ods.table3"],
            "total": 3,
            "schemas": ["public", "ods"],
            "table_name_pattern": "ods_*"
        }
    }
    """
    try:
        req = request.get_json(force=True)
        
        # å¤„ç†æ•°æ®åº“è¿æ¥å‚æ•°ï¼ˆå¯é€‰ï¼‰
        db_connection = req.get('db_connection')
        if not db_connection:
            # ä½¿ç”¨app_configçš„é»˜è®¤æ•°æ®åº“é…ç½®
            import app_config
            db_params = app_config.APP_DB_CONFIG
            db_connection = f"postgresql://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['dbname']}"
            logger.info("ä½¿ç”¨é»˜è®¤æ•°æ®åº“é…ç½®è·å–è¡¨åˆ—è¡¨")
        else:
            logger.info("ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ•°æ®åº“é…ç½®è·å–è¡¨åˆ—è¡¨")
        
        # å¯é€‰å‚æ•°
        schema = req.get('schema', '')
        table_name_pattern = req.get('table_name_pattern')
        
        # åˆ›å»ºè¡¨æ£€æŸ¥APIå®ä¾‹
        table_inspector = TableInspectorAPI()
        
        # ä½¿ç”¨asyncioè¿è¡Œå¼‚æ­¥æ–¹æ³•
        async def get_tables():
            return await table_inspector.get_tables_list(db_connection, schema, table_name_pattern)
        
        # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥æ–¹æ³•
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            tables = loop.run_until_complete(get_tables())
        finally:
            loop.close()
        
        # è§£æschemaä¿¡æ¯
        parsed_schemas = table_inspector._parse_schemas(schema)
        
        response_data = {
            "tables": tables,
            "total": len(tables),
            "schemas": parsed_schemas,
            "db_connection_info": {
                "database": db_connection.split('/')[-1].split('?')[0] if '/' in db_connection else "unknown"
            }
        }
        
        # å¦‚æœä½¿ç”¨äº†è¡¨åæ¨¡å¼ï¼Œæ·»åŠ åˆ°å“åº”ä¸­
        if table_name_pattern:
            response_data["table_name_pattern"] = table_name_pattern
        
        return jsonify(success_response(
            response_text="è·å–è¡¨åˆ—è¡¨æˆåŠŸ",
            data=response_data
        )), 200
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®åº“è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text=f"è·å–è¡¨åˆ—è¡¨å¤±è´¥: {str(e)}"
        )), 500

@app.route('/api/v0/database/table/ddl', methods=['POST'])
def get_table_ddl():
    """
    è·å–è¡¨çš„DDLè¯­å¥æˆ–MDæ–‡æ¡£ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    
    è¯·æ±‚ä½“:
    {
        "db_connection": "postgresql://postgres:postgres@192.168.67.1:5432/highway_db",  // å¯é€‰ï¼Œä¸ä¼ åˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        "table": "public.test",
        "business_context": "è¿™æ˜¯é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºçš„ç›¸å…³æ•°æ®",  // å¯é€‰
        "type": "ddl"  // å¯é€‰ï¼Œæ”¯æŒddl/md/bothï¼Œé»˜è®¤ä¸ºddl
    }
    
    å“åº”:
    {
        "success": true,
        "code": 200,
        "message": "è·å–è¡¨DDLæˆåŠŸ",
        "data": {
            "ddl": "create table public.test (...);",
            "md": "## testè¡¨...",  // ä»…å½“typeä¸ºmdæˆ–bothæ—¶è¿”å›
            "table_info": {
                "table_name": "test",
                "schema_name": "public",
                "full_name": "public.test",
                "comment": "æµ‹è¯•è¡¨",
                "field_count": 10,
                "row_count": 1000
            },
            "fields": [...]
        }
    }
    """
    try:
        req = request.get_json(force=True)
        
        # å¤„ç†å‚æ•°ï¼ˆtableä»ä¸ºå¿…éœ€ï¼Œdb_connectionå¯é€‰ï¼‰
        table = req.get('table')
        db_connection = req.get('db_connection')
        
        if not table:
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘å¿…éœ€å‚æ•°ï¼štable",
                missing_params=['table']
            )), 400
        
        if not db_connection:
            # ä½¿ç”¨app_configçš„é»˜è®¤æ•°æ®åº“é…ç½®
            import app_config
            db_params = app_config.APP_DB_CONFIG
            db_connection = f"postgresql://{db_params['user']}:{db_params['password']}@{db_params['host']}:{db_params['port']}/{db_params['dbname']}"
            logger.info("ä½¿ç”¨é»˜è®¤æ•°æ®åº“é…ç½®è·å–è¡¨DDL")
        else:
            logger.info("ä½¿ç”¨ç”¨æˆ·æŒ‡å®šçš„æ•°æ®åº“é…ç½®è·å–è¡¨DDL")
        
        # å¯é€‰å‚æ•°
        business_context = req.get('business_context', '')
        output_type = req.get('type', 'ddl')
        
        # éªŒè¯typeå‚æ•°
        valid_types = ['ddl', 'md', 'both']
        if output_type not in valid_types:
            return jsonify(bad_request_response(
                response_text=f"æ— æ•ˆçš„typeå‚æ•°: {output_type}ï¼Œæ”¯æŒçš„å€¼: {valid_types}",
                invalid_params=['type']
            )), 400
        
        # åˆ›å»ºè¡¨æ£€æŸ¥APIå®ä¾‹
        table_inspector = TableInspectorAPI()
        
        # ä½¿ç”¨asyncioè¿è¡Œå¼‚æ­¥æ–¹æ³•
        async def get_ddl():
            return await table_inspector.get_table_ddl(
                db_connection=db_connection,
                table=table,
                business_context=business_context,
                output_type=output_type
            )
        
        # åœ¨æ–°çš„äº‹ä»¶å¾ªç¯ä¸­è¿è¡Œå¼‚æ­¥æ–¹æ³•
        try:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            result = loop.run_until_complete(get_ddl())
        finally:
            loop.close()
        
        response_data = {
            **result,
            "generation_info": {
                "business_context": business_context,
                "output_type": output_type,
                "has_llm_comments": bool(business_context),
                "database": db_connection.split('/')[-1].split('?')[0] if '/' in db_connection else "unknown"
            }
        }
        
        return jsonify(success_response(
            response_text=f"è·å–è¡¨{output_type.upper()}æˆåŠŸ",
            data=response_data
        )), 200
        
    except Exception as e:
        logger.error(f"è·å–è¡¨DDLå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text=f"è·å–è¡¨{output_type.upper() if 'output_type' in locals() else 'DDL'}å¤±è´¥: {str(e)}"
        )), 500

# ==================== Data Pipeline API (ä» citu_app.py è¿ç§») ====================

@app.route('/api/v0/data_pipeline/tasks', methods=['POST'])
def create_data_pipeline_task():
    """åˆ›å»ºæ•°æ®ç®¡é“ä»»åŠ¡ï¼ˆä» citu_app.py è¿ç§»ï¼‰"""
    try:
        req = request.get_json(force=True)
        
        # table_list_fileå’Œbusiness_contextç°åœ¨éƒ½æ˜¯å¯é€‰å‚æ•°
        # å¦‚æœæœªæä¾›table_list_fileï¼Œå°†ä½¿ç”¨æ–‡ä»¶ä¸Šä¼ æ¨¡å¼
        
        # åˆ›å»ºä»»åŠ¡ï¼ˆæ”¯æŒå¯é€‰çš„db_connectionå‚æ•°ï¼‰
        manager = get_data_pipeline_manager()
        task_id = manager.create_task(
            table_list_file=req.get('table_list_file'),
            business_context=req.get('business_context'),
            db_name=req.get('db_name'),  # å¯é€‰å‚æ•°ï¼Œç”¨äºæŒ‡å®šç‰¹å®šæ•°æ®åº“åç§°
            db_connection=req.get('db_connection'),  # å¯é€‰å‚æ•°ï¼Œç”¨äºæŒ‡å®šæ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²
            task_name=req.get('task_name'),  # å¯é€‰å‚æ•°ï¼Œç”¨äºæŒ‡å®šä»»åŠ¡åç§°
            enable_sql_validation=req.get('enable_sql_validation', True),
            enable_llm_repair=req.get('enable_llm_repair', True),
            modify_original_file=req.get('modify_original_file', True),
            enable_training_data_load=req.get('enable_training_data_load', True)
        )
        
        # è·å–ä»»åŠ¡ä¿¡æ¯
        task_info = manager.get_task_status(task_id)
        
        response_data = {
            "task_id": task_id,
            "task_name": task_info.get('task_name'),
            "status": task_info.get('status'),
            "created_at": task_info.get('created_at').isoformat() if task_info.get('created_at') else None
        }
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶ä¸Šä¼ æ¨¡å¼
        file_upload_mode = not req.get('table_list_file')
        response_message = "ä»»åŠ¡åˆ›å»ºæˆåŠŸ"
        
        if file_upload_mode:
            response_data["file_upload_mode"] = True
            response_data["next_step"] = f"POST /api/v0/data_pipeline/tasks/{task_id}/upload-table-list"
            response_message += "ï¼Œè¯·ä¸Šä¼ è¡¨æ¸…å•æ–‡ä»¶åå†æ‰§è¡Œä»»åŠ¡"
        
        return jsonify(success_response(
            response_text=response_message,
            data=response_data
        )), 201
        
    except Exception as e:
        logger.error(f"åˆ›å»ºæ•°æ®ç®¡é“ä»»åŠ¡å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="åˆ›å»ºä»»åŠ¡å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/<task_id>/execute', methods=['POST'])
def execute_data_pipeline_task(task_id):
    """æ‰§è¡Œæ•°æ®ç®¡é“ä»»åŠ¡ï¼ˆä» citu_app.py è¿ç§»ï¼‰"""
    try:
        req = request.get_json(force=True) if request.is_json else {}
        execution_mode = req.get('execution_mode', 'complete')
        step_name = req.get('step_name')
        
        # æ–°å¢ï¼šVectorè¡¨ç®¡ç†å‚æ•°
        backup_vector_tables = req.get('backup_vector_tables', False)
        truncate_vector_tables = req.get('truncate_vector_tables', False)
        skip_training = req.get('skip_training', False)
        
        # éªŒè¯æ‰§è¡Œæ¨¡å¼
        if execution_mode not in ['complete', 'step']:
            return jsonify(bad_request_response(
                response_text="æ— æ•ˆçš„æ‰§è¡Œæ¨¡å¼ï¼Œå¿…é¡»æ˜¯ 'complete' æˆ– 'step'",
                invalid_params=['execution_mode']
            )), 400
        
        # å¦‚æœæ˜¯æ­¥éª¤æ‰§è¡Œæ¨¡å¼ï¼ŒéªŒè¯æ­¥éª¤åç§°
        if execution_mode == 'step':
            if not step_name:
                return jsonify(bad_request_response(
                    response_text="æ­¥éª¤æ‰§è¡Œæ¨¡å¼éœ€è¦æŒ‡å®šstep_name",
                    missing_params=['step_name']
                )), 400
            
            valid_steps = ['ddl_generation', 'qa_generation', 'sql_validation', 'training_load']
            if step_name not in valid_steps:
                return jsonify(bad_request_response(
                    response_text=f"æ— æ•ˆçš„æ­¥éª¤åç§°ï¼Œæ”¯æŒçš„æ­¥éª¤: {', '.join(valid_steps)}",
                    invalid_params=['step_name']
                )), 400
        
        # æ–°å¢ï¼šVectorè¡¨ç®¡ç†å‚æ•°éªŒè¯å’Œè­¦å‘Š
        if execution_mode == 'step' and step_name != 'training_load':
            if backup_vector_tables or truncate_vector_tables or skip_training:
                logger.warning(
                    f"âš ï¸ Vectorè¡¨ç®¡ç†å‚æ•°ä»…åœ¨training_loadæ­¥éª¤æœ‰æ•ˆï¼Œå½“å‰æ­¥éª¤: {step_name}ï¼Œå¿½ç•¥å‚æ•°"
                )
                backup_vector_tables = False
                truncate_vector_tables = False
                skip_training = False
        
        # æ£€æŸ¥ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        manager = get_data_pipeline_manager()
        task_info = manager.get_task_status(task_id)
        if not task_info:
            return jsonify(not_found_response(
                response_text=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
            )), 404
        
        # ä½¿ç”¨subprocesså¯åŠ¨ç‹¬ç«‹è¿›ç¨‹æ‰§è¡Œä»»åŠ¡
        def run_task_subprocess():
            try:
                import subprocess
                import sys
                from pathlib import Path
                
                # æ„å»ºæ‰§è¡Œå‘½ä»¤
                python_executable = sys.executable
                script_path = Path(__file__).parent / "data_pipeline" / "task_executor.py"
                
                cmd = [
                    python_executable,
                    str(script_path),
                    "--task-id", task_id,
                    "--execution-mode", execution_mode
                ]
                
                if step_name:
                    cmd.extend(["--step-name", step_name])
                    
                # æ–°å¢ï¼šVectorè¡¨ç®¡ç†å‚æ•°ä¼ é€’
                if backup_vector_tables:
                    cmd.append("--backup-vector-tables")
                if truncate_vector_tables:
                    cmd.append("--truncate-vector-tables")
                if skip_training:
                    cmd.append("--skip-training")
                
                logger.info(f"å¯åŠ¨ä»»åŠ¡è¿›ç¨‹: {' '.join(cmd)}")
                
                # å¯åŠ¨åå°è¿›ç¨‹ï¼ˆä¸ç­‰å¾…å®Œæˆï¼‰
                process = subprocess.Popen(
                    cmd,
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    text=True,
                    cwd=Path(__file__).parent
                )
                
                logger.info(f"ä»»åŠ¡è¿›ç¨‹å·²å¯åŠ¨: PID={process.pid}, task_id={task_id}")
                
            except Exception as e:
                logger.error(f"å¯åŠ¨ä»»åŠ¡è¿›ç¨‹å¤±è´¥: {task_id}, é”™è¯¯: {str(e)}")
        
        # åœ¨æ–°çº¿ç¨‹ä¸­å¯åŠ¨subprocessï¼ˆé¿å…é˜»å¡APIå“åº”ï¼‰
        thread = Thread(target=run_task_subprocess, daemon=True)
        thread.start()
        
        # æ–°å¢ï¼šè®°å½•Vectorè¡¨ç®¡ç†å‚æ•°åˆ°æ—¥å¿—
        if backup_vector_tables or truncate_vector_tables:
            logger.info(f"ğŸ“‹ APIè¯·æ±‚åŒ…å«Vectorè¡¨ç®¡ç†å‚æ•°: backup={backup_vector_tables}, truncate={truncate_vector_tables}")
        
        response_data = {
            "task_id": task_id,
            "execution_mode": execution_mode,
            "step_name": step_name if execution_mode == 'step' else None,
            "message": "ä»»åŠ¡æ­£åœ¨åå°æ‰§è¡Œï¼Œè¯·é€šè¿‡çŠ¶æ€æ¥å£æŸ¥è¯¢è¿›åº¦"
        }
        
        return jsonify(success_response(
            response_text="ä»»åŠ¡æ‰§è¡Œå·²å¯åŠ¨",
            data=response_data
        )), 202
        
    except Exception as e:
        logger.error(f"å¯åŠ¨æ•°æ®ç®¡é“ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="å¯åŠ¨ä»»åŠ¡æ‰§è¡Œå¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/<task_id>', methods=['GET'])
def get_data_pipeline_task_status(task_id):
    """
    è·å–æ•°æ®ç®¡é“ä»»åŠ¡çŠ¶æ€ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    
    å“åº”:
    {
        "success": true,
        "code": 200,
        "message": "è·å–ä»»åŠ¡çŠ¶æ€æˆåŠŸ",
        "data": {
            "task_id": "task_20250627_143052",
            "status": "in_progress",
            "step_status": {
                "ddl_generation": "completed",
                "qa_generation": "running",
                "sql_validation": "pending",
                "training_load": "pending"
            },
            "created_at": "2025-06-27T14:30:52",
            "started_at": "2025-06-27T14:31:00",
            "parameters": {...},
            "current_execution": {...},
            "total_executions": 2
        }
    }
    """
    try:
        manager = get_data_pipeline_manager()
        task_info = manager.get_task_status(task_id)
        
        if not task_info:
            return jsonify(not_found_response(
                response_text=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
            )), 404
        
        # è·å–æ­¥éª¤çŠ¶æ€
        steps = manager.get_task_steps(task_id)
        current_step = None
        for step in steps:
            if step['step_status'] == 'running':
                current_step = step
                break
        
        # æ„å»ºæ­¥éª¤çŠ¶æ€æ‘˜è¦
        step_status_summary = {}
        for step in steps:
            step_status_summary[step['step_name']] = step['step_status']
        
        response_data = {
            "task_id": task_info['task_id'],
            "task_name": task_info.get('task_name'),
            "status": task_info['status'],
            "step_status": step_status_summary,
            "created_at": task_info['created_at'].isoformat() if task_info.get('created_at') else None,
            "started_at": task_info['started_at'].isoformat() if task_info.get('started_at') else None,
            "completed_at": task_info['completed_at'].isoformat() if task_info.get('completed_at') else None,
            "parameters": task_info.get('parameters', {}),
            "result": task_info.get('result'),
            "error_message": task_info.get('error_message'),
            "current_step": {
                "execution_id": current_step['execution_id'],
                "step": current_step['step_name'],
                "status": current_step['step_status'],
                "started_at": current_step['started_at'].isoformat() if current_step and current_step.get('started_at') else None
            } if current_step else None,
            "total_steps": len(steps),
            "steps": [{
                "step_name": step['step_name'],
                "step_status": step['step_status'],
                "started_at": step['started_at'].isoformat() if step.get('started_at') else None,
                "completed_at": step['completed_at'].isoformat() if step.get('completed_at') else None,
                "error_message": step.get('error_message')
            } for step in steps]
        }
        
        return jsonify(success_response(
            response_text="è·å–ä»»åŠ¡çŠ¶æ€æˆåŠŸ",
            data=response_data
        ))
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®ç®¡é“ä»»åŠ¡çŠ¶æ€å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="è·å–ä»»åŠ¡çŠ¶æ€å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/<task_id>/logs', methods=['GET'])
def get_data_pipeline_task_logs(task_id):
    """
    è·å–æ•°æ®ç®¡é“ä»»åŠ¡æ—¥å¿—ï¼ˆä»ä»»åŠ¡ç›®å½•æ–‡ä»¶è¯»å–ï¼‰ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    
    æŸ¥è¯¢å‚æ•°:
    - limit: æ—¥å¿—è¡Œæ•°é™åˆ¶ï¼Œé»˜è®¤100
    - level: æ—¥å¿—çº§åˆ«è¿‡æ»¤ï¼Œå¯é€‰
    
    å“åº”:
    {
        "success": true,
        "code": 200,
        "message": "è·å–ä»»åŠ¡æ—¥å¿—æˆåŠŸ",
        "data": {
            "task_id": "task_20250627_143052",
            "logs": [
                {
                    "timestamp": "2025-06-27 14:30:52",
                    "level": "INFO",
                    "message": "ä»»åŠ¡å¼€å§‹æ‰§è¡Œ"
                }
            ],
            "total": 15,
            "source": "file"
        }
    }
    """
    try:
        limit = request.args.get('limit', 100, type=int)
        level = request.args.get('level')
        
        # é™åˆ¶æœ€å¤§æŸ¥è¯¢æ•°é‡
        limit = min(limit, 1000)
        
        manager = get_data_pipeline_manager()
        
        # éªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        task_info = manager.get_task_status(task_id)
        if not task_info:
            return jsonify(not_found_response(
                response_text=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
            )), 404
        
        # è·å–ä»»åŠ¡ç›®å½•ä¸‹çš„æ—¥å¿—æ–‡ä»¶
        import os
        from pathlib import Path
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
        project_root = Path(__file__).parent.absolute()
        task_dir = project_root / "data_pipeline" / "training_data" / task_id
        log_file = task_dir / "data_pipeline.log"
        
        logs = []
        if log_file.exists():
            try:
                # è¯»å–æ—¥å¿—æ–‡ä»¶çš„æœ€åNè¡Œ
                with open(log_file, 'r', encoding='utf-8') as f:
                    lines = f.readlines()
                    
                # å–æœ€ålimitè¡Œ
                recent_lines = lines[-limit:] if len(lines) > limit else lines
                
                # è§£ææ—¥å¿—è¡Œ
                import re
                log_pattern = r'^(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}) \[(\w+)\] (.+?): (.+)$'
                
                for line in recent_lines:
                    line = line.strip()
                    if not line:
                        continue
                        
                    match = re.match(log_pattern, line)
                    if match:
                        timestamp, log_level, logger_name, message = match.groups()
                        
                        # çº§åˆ«è¿‡æ»¤
                        if level and log_level != level.upper():
                            continue
                            
                        logs.append({
                            "timestamp": timestamp,
                            "level": log_level,
                            "logger": logger_name,
                            "message": message
                        })
                    else:
                        # å¤„ç†å¤šè¡Œæ—¥å¿—ï¼ˆå¦‚å¼‚å¸¸å †æ ˆï¼‰
                        if logs:
                            logs[-1]["message"] += f"\n{line}"
                        
            except Exception as e:
                logger.error(f"è¯»å–æ—¥å¿—æ–‡ä»¶å¤±è´¥: {e}")
        
        response_data = {
            "task_id": task_id,
            "logs": logs,
            "total": len(logs),
            "source": "file",
            "log_file": str(log_file) if log_file.exists() else None
        }
        
        return jsonify(success_response(
            response_text="è·å–ä»»åŠ¡æ—¥å¿—æˆåŠŸ",
            data=response_data
        ))
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®ç®¡é“ä»»åŠ¡æ—¥å¿—å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="è·å–ä»»åŠ¡æ—¥å¿—å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks', methods=['GET'])
def list_data_pipeline_tasks():
    """è·å–æ•°æ®ç®¡é“ä»»åŠ¡åˆ—è¡¨ï¼ˆä» citu_app.py è¿ç§»ï¼‰"""
    try:
        limit = request.args.get('limit', 50, type=int)
        offset = request.args.get('offset', 0, type=int)
        status_filter = request.args.get('status')
        
        # é™åˆ¶æŸ¥è¯¢æ•°é‡
        limit = min(limit, 100)
        
        manager = get_data_pipeline_manager()
        tasks = manager.get_tasks_list(
            limit=limit,
            offset=offset,
            status_filter=status_filter
        )
        
        # æ ¼å¼åŒ–ä»»åŠ¡åˆ—è¡¨
        formatted_tasks = []
        for task in tasks:
            formatted_tasks.append({
                "task_id": task.get('task_id'),
                "task_name": task.get('task_name'),
                "status": task.get('status'),
                "step_status": task.get('step_status'),
                "created_at": task['created_at'].isoformat() if task.get('created_at') else None,
                "started_at": task['started_at'].isoformat() if task.get('started_at') else None,
                "completed_at": task['completed_at'].isoformat() if task.get('completed_at') else None,
                "created_by": task.get('by_user'),
                "db_name": task.get('db_name'),
                "business_context": task.get('parameters', {}).get('business_context') if task.get('parameters') else None,
                # æ–°å¢å­—æ®µ
                "directory_exists": task.get('directory_exists', True),  # é»˜è®¤ä¸ºTrueï¼Œå…¼å®¹æ—§æ•°æ®
                "updated_at": task['updated_at'].isoformat() if task.get('updated_at') else None
            })
        
        response_data = {
            "tasks": formatted_tasks,
            "total": len(formatted_tasks),
            "limit": limit,
            "offset": offset
        }
        
        return jsonify(success_response(
            response_text="è·å–ä»»åŠ¡åˆ—è¡¨æˆåŠŸ",
            data=response_data
        ))
        
    except Exception as e:
        logger.error(f"è·å–æ•°æ®ç®¡é“ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="è·å–ä»»åŠ¡åˆ—è¡¨å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/query', methods=['POST'])
def query_data_pipeline_tasks():
    """
    é«˜çº§æŸ¥è¯¢æ•°æ®ç®¡é“ä»»åŠ¡åˆ—è¡¨ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    
    æ”¯æŒå¤æ‚ç­›é€‰ã€æ’åºã€åˆ†é¡µåŠŸèƒ½
    
    è¯·æ±‚ä½“:
    {
        "page": 1,                          // é¡µç ï¼Œå¿…é¡»å¤§äº0ï¼Œé»˜è®¤1
        "page_size": 20,                    // æ¯é¡µå¤§å°ï¼Œ1-100ä¹‹é—´ï¼Œé»˜è®¤20
        "status": "completed",              // å¯é€‰ï¼Œä»»åŠ¡çŠ¶æ€ç­›é€‰ï¼š"pending"|"running"|"completed"|"failed"|"cancelled"
        "task_name": "highway",             // å¯é€‰ï¼Œä»»åŠ¡åç§°æ¨¡ç³Šæœç´¢ï¼Œæœ€å¤§100å­—ç¬¦
        "created_by": "user123",            // å¯é€‰ï¼Œåˆ›å»ºè€…ç²¾ç¡®åŒ¹é…
        "db_name": "highway_db",            // å¯é€‰ï¼Œæ•°æ®åº“åç§°ç²¾ç¡®åŒ¹é…
        "created_time_start": "2025-01-01T00:00:00",  // å¯é€‰ï¼Œåˆ›å»ºæ—¶é—´èŒƒå›´å¼€å§‹
        "created_time_end": "2025-12-31T23:59:59",    // å¯é€‰ï¼Œåˆ›å»ºæ—¶é—´èŒƒå›´ç»“æŸ
        "started_time_start": "2025-01-01T00:00:00",  // å¯é€‰ï¼Œå¼€å§‹æ—¶é—´èŒƒå›´å¼€å§‹
        "started_time_end": "2025-12-31T23:59:59",    // å¯é€‰ï¼Œå¼€å§‹æ—¶é—´èŒƒå›´ç»“æŸ
        "completed_time_start": "2025-01-01T00:00:00", // å¯é€‰ï¼Œå®Œæˆæ—¶é—´èŒƒå›´å¼€å§‹
        "completed_time_end": "2025-12-31T23:59:59",   // å¯é€‰ï¼Œå®Œæˆæ—¶é—´èŒƒå›´ç»“æŸ
        "sort_by": "created_at",            // å¯é€‰ï¼Œæ’åºå­—æ®µï¼š"created_at"|"started_at"|"completed_at"|"task_name"|"status"ï¼Œé»˜è®¤"created_at"
        "sort_order": "desc"                // å¯é€‰ï¼Œæ’åºæ–¹å‘ï¼š"asc"|"desc"ï¼Œé»˜è®¤"desc"
    }
    """
    try:
        # è·å–è¯·æ±‚æ•°æ®
        req = request.get_json(force=True) if request.is_json else {}
        
        # è§£æå‚æ•°ï¼Œè®¾ç½®é»˜è®¤å€¼
        page = req.get('page', 1)
        page_size = req.get('page_size', 20)
        status = req.get('status')
        task_name = req.get('task_name')
        created_by = req.get('created_by')
        db_name = req.get('db_name')
        created_time_start = req.get('created_time_start')
        created_time_end = req.get('created_time_end')
        started_time_start = req.get('started_time_start')
        started_time_end = req.get('started_time_end')
        completed_time_start = req.get('completed_time_start')
        completed_time_end = req.get('completed_time_end')
        sort_by = req.get('sort_by', 'created_at')
        sort_order = req.get('sort_order', 'desc')
        
        # å‚æ•°éªŒè¯
        # éªŒè¯åˆ†é¡µå‚æ•°
        if page < 1:
            return jsonify(bad_request_response(
                response_text="é¡µç å¿…é¡»å¤§äº0",
                invalid_params=['page']
            )), 400
        
        if page_size < 1 or page_size > 100:
            return jsonify(bad_request_response(
                response_text="æ¯é¡µå¤§å°å¿…é¡»åœ¨1-100ä¹‹é—´",
                invalid_params=['page_size']
            )), 400
        
        # éªŒè¯ä»»åŠ¡åç§°é•¿åº¦
        if task_name and len(task_name) > 100:
            return jsonify(bad_request_response(
                response_text="ä»»åŠ¡åç§°æœç´¢å…³é”®è¯æœ€å¤§é•¿åº¦ä¸º100å­—ç¬¦",
                invalid_params=['task_name']
            )), 400
        
        # éªŒè¯æ’åºå‚æ•°
        allowed_sort_fields = ['created_at', 'started_at', 'completed_at', 'task_name', 'status']
        if sort_by not in allowed_sort_fields:
            return jsonify(bad_request_response(
                response_text=f"ä¸æ”¯æŒçš„æ’åºå­—æ®µ: {sort_by}ï¼Œæ”¯æŒçš„å­—æ®µ: {', '.join(allowed_sort_fields)}",
                invalid_params=['sort_by']
            )), 400
        
        if sort_order.lower() not in ['asc', 'desc']:
            return jsonify(bad_request_response(
                response_text="æ’åºæ–¹å‘å¿…é¡»æ˜¯ 'asc' æˆ– 'desc'",
                invalid_params=['sort_order']
            )), 400
        
        # éªŒè¯çŠ¶æ€ç­›é€‰
        if status:
            allowed_statuses = ['pending', 'running', 'completed', 'failed', 'cancelled']
            if status not in allowed_statuses:
                return jsonify(bad_request_response(
                    response_text=f"ä¸æ”¯æŒçš„çŠ¶æ€å€¼: {status}ï¼Œæ”¯æŒçš„çŠ¶æ€: {', '.join(allowed_statuses)}",
                    invalid_params=['status']
                )), 400
        
        # è°ƒç”¨ç®¡ç†å™¨æ‰§è¡ŒæŸ¥è¯¢
        manager = get_data_pipeline_manager()
        result = manager.query_tasks_advanced(
            page=page,
            page_size=page_size,
            status=status,
            task_name=task_name,
            created_by=created_by,
            db_name=db_name,
            created_time_start=created_time_start,
            created_time_end=created_time_end,
            started_time_start=started_time_start,
            started_time_end=started_time_end,
            completed_time_start=completed_time_start,
            completed_time_end=completed_time_end,
            sort_by=sort_by,
            sort_order=sort_order
        )
        
        # æ ¼å¼åŒ–ä»»åŠ¡åˆ—è¡¨
        formatted_tasks = []
        for task in result['tasks']:
            formatted_tasks.append({
                "task_id": task.get('task_id'),
                "task_name": task.get('task_name'),
                "status": task.get('status'),
                "step_status": task.get('step_status'),
                "created_at": task['created_at'].isoformat() if task.get('created_at') else None,
                "started_at": task['started_at'].isoformat() if task.get('started_at') else None,
                "completed_at": task['completed_at'].isoformat() if task.get('completed_at') else None,
                "created_by": task.get('by_user'),
                "db_name": task.get('db_name'),
                "business_context": task.get('parameters', {}).get('business_context') if task.get('parameters') else None,
                "directory_exists": task.get('directory_exists', True),
                "updated_at": task['updated_at'].isoformat() if task.get('updated_at') else None
            })
        
        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "tasks": formatted_tasks,
            "pagination": result['pagination'],
            "filters_applied": {
                k: v for k, v in {
                    "status": status,
                    "task_name": task_name,
                    "created_by": created_by,
                    "db_name": db_name,
                    "created_time_start": created_time_start,
                    "created_time_end": created_time_end,
                    "started_time_start": started_time_start,
                    "started_time_end": started_time_end,
                    "completed_time_start": completed_time_start,
                    "completed_time_end": completed_time_end
                }.items() if v
            },
            "sort_applied": {
                "sort_by": sort_by,
                "sort_order": sort_order
            },
            "query_time": result.get('query_time', '0.000s')
        }
        
        return jsonify(success_response(
            response_text="æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨æˆåŠŸ",
            data=response_data
        ))
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ•°æ®ç®¡é“ä»»åŠ¡åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="æŸ¥è¯¢ä»»åŠ¡åˆ—è¡¨å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/<task_id>/files', methods=['GET'])
def get_data_pipeline_task_files(task_id):
    """è·å–ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨ï¼ˆä» citu_app.py è¿ç§»ï¼‰"""
    try:
        file_manager = get_data_pipeline_file_manager()
        
        # è·å–ä»»åŠ¡æ–‡ä»¶
        files = file_manager.get_task_files(task_id)
        directory_info = file_manager.get_directory_info(task_id)
        
        # æ ¼å¼åŒ–æ–‡ä»¶ä¿¡æ¯
        formatted_files = []
        for file_info in files:
            formatted_files.append({
                "file_name": file_info['file_name'],
                "file_type": file_info['file_type'],
                "file_size": file_info['file_size'],
                "file_size_formatted": file_info['file_size_formatted'],
                "created_at": file_info['created_at'].isoformat() if file_info.get('created_at') else None,
                "modified_at": file_info['modified_at'].isoformat() if file_info.get('modified_at') else None,
                "is_readable": file_info['is_readable']
            })
        
        response_data = {
            "task_id": task_id,
            "files": formatted_files,
            "directory_info": directory_info
        }
        
        return jsonify(success_response(
            response_text="è·å–ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨æˆåŠŸ",
            data=response_data
        ))
        
    except Exception as e:
        logger.error(f"è·å–ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="è·å–ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/<task_id>/files/<file_name>', methods=['GET'])
def download_data_pipeline_task_file(task_id, file_name):
    """ä¸‹è½½ä»»åŠ¡æ–‡ä»¶ï¼ˆä» citu_app.py è¿ç§»ï¼‰"""
    try:
        logger.info(f"å¼€å§‹ä¸‹è½½æ–‡ä»¶: task_id={task_id}, file_name={file_name}")
        
        # ç›´æ¥æ„å»ºæ–‡ä»¶è·¯å¾„ï¼Œé¿å…ä¾èµ–æ•°æ®åº“
        from pathlib import Path
        import os
        
        # è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
        project_root = Path(__file__).parent.absolute()
        task_dir = project_root / "data_pipeline" / "training_data" / task_id
        file_path = task_dir / file_name
        
        logger.info(f"æ–‡ä»¶è·¯å¾„: {file_path}")
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not file_path.exists():
            logger.warning(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            return jsonify(not_found_response(
                response_text=f"æ–‡ä»¶ä¸å­˜åœ¨: {file_name}"
            )), 404
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºæ–‡ä»¶ï¼ˆè€Œä¸æ˜¯ç›®å½•ï¼‰
        if not file_path.is_file():
            logger.warning(f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}")
            return jsonify(bad_request_response(
                response_text=f"è·¯å¾„ä¸æ˜¯æœ‰æ•ˆæ–‡ä»¶: {file_name}"
            )), 400
        
        # å®‰å…¨æ£€æŸ¥ï¼šç¡®ä¿æ–‡ä»¶åœ¨å…è®¸çš„ç›®å½•å†…
        try:
            file_path.resolve().relative_to(task_dir.resolve())
        except ValueError:
            logger.warning(f"æ–‡ä»¶è·¯å¾„ä¸å®‰å…¨: {file_path}")
            return jsonify(bad_request_response(
                response_text="éæ³•çš„æ–‡ä»¶è·¯å¾„"
            )), 400
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å¯è¯»
        if not os.access(file_path, os.R_OK):
            logger.warning(f"æ–‡ä»¶ä¸å¯è¯»: {file_path}")
            return jsonify(bad_request_response(
                response_text="æ–‡ä»¶ä¸å¯è¯»"
            )), 400
        
        logger.info(f"å¼€å§‹å‘é€æ–‡ä»¶: {file_path}")
        return send_file(
            file_path,
            as_attachment=True,
            download_name=file_name
        )
        
    except Exception as e:
        logger.error(f"ä¸‹è½½ä»»åŠ¡æ–‡ä»¶å¤±è´¥: task_id={task_id}, file_name={file_name}, é”™è¯¯: {str(e)}", exc_info=True)
        return jsonify(internal_error_response(
            response_text="ä¸‹è½½æ–‡ä»¶å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/<task_id>/upload-table-list', methods=['POST'])
def upload_table_list_file(task_id):
    """
    ä¸Šä¼ è¡¨æ¸…å•æ–‡ä»¶ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    
    è¡¨å•å‚æ•°:
    - file: è¦ä¸Šä¼ çš„è¡¨æ¸…å•æ–‡ä»¶ï¼ˆmultipart/form-dataï¼‰
    
    å“åº”:
    {
        "success": true,
        "code": 200,
        "message": "è¡¨æ¸…å•æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
        "data": {
            "task_id": "task_20250701_123456",
            "filename": "table_list.txt",
            "file_size": 1024,
            "file_size_formatted": "1.0 KB"
        }
    }
    """
    try:
        # éªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        manager = get_data_pipeline_manager()
        task_info = manager.get_task_status(task_id)
        if not task_info:
            return jsonify(not_found_response(
                response_text=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
            )), 404
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šä¼ 
        if 'file' not in request.files:
            return jsonify(bad_request_response(
                response_text="è¯·é€‰æ‹©è¦ä¸Šä¼ çš„è¡¨æ¸…å•æ–‡ä»¶",
                missing_params=['file']
            )), 400
        
        file = request.files['file']
        
        # éªŒè¯æ–‡ä»¶å
        if file.filename == '':
            return jsonify(bad_request_response(
                response_text="è¯·é€‰æ‹©æœ‰æ•ˆçš„æ–‡ä»¶"
            )), 400
        
        try:
            # ä½¿ç”¨æ–‡ä»¶ç®¡ç†å™¨ä¸Šä¼ æ–‡ä»¶
            file_manager = get_data_pipeline_file_manager()
            result = file_manager.upload_table_list_file(task_id, file)
            
            response_data = {
                "task_id": task_id,
                "filename": result["filename"],
                "file_size": result["file_size"],
                "file_size_formatted": result["file_size_formatted"],
                "upload_time": result["upload_time"].isoformat() if result.get("upload_time") else None
            }
            
            return jsonify(success_response(
                response_text="è¡¨æ¸…å•æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
                data=response_data
            )), 200
            
        except ValueError as e:
            # æ–‡ä»¶éªŒè¯é”™è¯¯ï¼ˆå¦‚æ–‡ä»¶å¤ªå¤§ã€ç©ºæ–‡ä»¶ç­‰ï¼‰
            return jsonify(bad_request_response(
                response_text=str(e)
            )), 400
        except Exception as e:
            logger.error(f"ä¸Šä¼ è¡¨æ¸…å•æ–‡ä»¶å¤±è´¥: {str(e)}")
            return jsonify(internal_error_response(
                response_text="æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
            )), 500
        
    except Exception as e:
        logger.error(f"å¤„ç†è¡¨æ¸…å•æ–‡ä»¶ä¸Šä¼ è¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="å¤„ç†ä¸Šä¼ è¯·æ±‚å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/<task_id>/table-list-info', methods=['GET'])
def get_table_list_info(task_id):
    """
    è·å–ä»»åŠ¡çš„è¡¨æ¸…å•æ–‡ä»¶ä¿¡æ¯ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    
    å“åº”:
    {
        "success": true,
        "code": 200,
        "message": "è·å–è¡¨æ¸…å•æ–‡ä»¶ä¿¡æ¯æˆåŠŸ",
        "data": {
            "task_id": "task_20250701_123456",
            "has_file": true,
            "filename": "table_list.txt",
            "file_path": "./data_pipeline/training_data/task_20250701_123456/table_list.txt",
            "file_size": 1024,
            "file_size_formatted": "1.0 KB",
            "uploaded_at": "2025-07-01T12:34:56",
            "table_count": 5,
            "is_readable": true
        }
    }
    """
    try:
        file_manager = get_data_pipeline_file_manager()
        
        # è·å–è¡¨æ¸…å•æ–‡ä»¶ä¿¡æ¯
        table_list_info = file_manager.get_table_list_file_info(task_id)
        
        response_data = {
            "task_id": task_id,
            "has_file": table_list_info.get("exists", False),
            **table_list_info
        }
        
        return jsonify(success_response(
            response_text="è·å–è¡¨æ¸…å•æ–‡ä»¶ä¿¡æ¯æˆåŠŸ",
            data=response_data
        ))
        
    except Exception as e:
        logger.error(f"è·å–è¡¨æ¸…å•æ–‡ä»¶ä¿¡æ¯å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="è·å–è¡¨æ¸…å•æ–‡ä»¶ä¿¡æ¯å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/<task_id>/table-list', methods=['POST'])
def create_table_list_from_names(task_id):
    """
    é€šè¿‡POSTæ–¹å¼æäº¤è¡¨ååˆ—è¡¨å¹¶åˆ›å»ºtable_list.txtæ–‡ä»¶ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    
    è¯·æ±‚ä½“:
    {
        "tables": ["table1", "schema.table2", "table3"]
    }
    æˆ–è€…:
    {
        "tables": "table1,schema.table2,table3"
    }
    
    å“åº”:
    {
        "success": true,
        "code": 200,
        "message": "è¡¨æ¸…å•å·²æˆåŠŸåˆ›å»º",
        "data": {
            "task_id": "task_20250701_123456",
            "filename": "table_list.txt",
            "table_count": 3,
            "file_size": 45,
            "file_size_formatted": "45 B",
            "created_time": "2025-07-01T12:34:56"
        }
    }
    """
    try:
        # éªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        manager = get_data_pipeline_manager()
        task_info = manager.get_task_status(task_id)
        if not task_info:
            return jsonify(not_found_response(
                response_text=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
            )), 404
        
        # è·å–è¯·æ±‚æ•°æ®
        req = request.get_json(force=True)
        tables_param = req.get('tables')
        
        if not tables_param:
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘å¿…éœ€å‚æ•°ï¼štables",
                missing_params=['tables']
            )), 400
        
        # å¤„ç†ä¸åŒæ ¼å¼çš„è¡¨åå‚æ•°
        try:
            if isinstance(tables_param, str):
                # é€—å·åˆ†éš”çš„å­—ç¬¦ä¸²æ ¼å¼
                table_names = [name.strip() for name in tables_param.split(',') if name.strip()]
            elif isinstance(tables_param, list):
                # æ•°ç»„æ ¼å¼
                table_names = [str(name).strip() for name in tables_param if str(name).strip()]
            else:
                return jsonify(bad_request_response(
                    response_text="tableså‚æ•°æ ¼å¼é”™è¯¯ï¼Œåº”ä¸ºå­—ç¬¦ä¸²ï¼ˆé€—å·åˆ†éš”ï¼‰æˆ–æ•°ç»„"
                )), 400
            
            if not table_names:
                return jsonify(bad_request_response(
                    response_text="è¡¨ååˆ—è¡¨ä¸èƒ½ä¸ºç©º"
                )), 400
                
        except Exception as e:
            return jsonify(bad_request_response(
                response_text=f"è§£ætableså‚æ•°å¤±è´¥: {str(e)}"
            )), 400
        
        try:
            # ä½¿ç”¨æ–‡ä»¶ç®¡ç†å™¨åˆ›å»ºè¡¨æ¸…å•æ–‡ä»¶
            file_manager = get_data_pipeline_file_manager()
            result = file_manager.create_table_list_from_names(task_id, table_names)
            
            response_data = {
                "task_id": task_id,
                "filename": result["filename"],
                "table_count": result["table_count"],
                "unique_table_count": result["unique_table_count"],
                "file_size": result["file_size"],
                "file_size_formatted": result["file_size_formatted"],
                "created_time": result["created_time"].isoformat() if result.get("created_time") else None,
                "original_count": len(table_names) if isinstance(table_names, list) else len(tables_param.split(','))
            }
            
            return jsonify(success_response(
                response_text=f"è¡¨æ¸…å•å·²æˆåŠŸåˆ›å»ºï¼ŒåŒ…å« {result['table_count']} ä¸ªè¡¨",
                data=response_data
            )), 200
            
        except ValueError as e:
            # è¡¨åéªŒè¯é”™è¯¯ï¼ˆå¦‚æ ¼å¼é”™è¯¯ã€æ•°é‡é™åˆ¶ç­‰ï¼‰
            return jsonify(bad_request_response(
                response_text=str(e)
            )), 400
        except Exception as e:
            logger.error(f"åˆ›å»ºè¡¨æ¸…å•æ–‡ä»¶å¤±è´¥: {str(e)}")
            return jsonify(internal_error_response(
                response_text="åˆ›å»ºè¡¨æ¸…å•æ–‡ä»¶å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
            )), 500
        
    except Exception as e:
        logger.error(f"å¤„ç†è¡¨æ¸…å•åˆ›å»ºè¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="å¤„ç†è¯·æ±‚å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/<task_id>/files', methods=['POST'])
def upload_file_to_task(task_id):
    """
    ä¸Šä¼ æ–‡ä»¶åˆ°æŒ‡å®šä»»åŠ¡ç›®å½•ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    
    è¡¨å•å‚æ•°:
    - file: è¦ä¸Šä¼ çš„æ–‡ä»¶ï¼ˆmultipart/form-dataï¼‰
    - overwrite_mode: é‡åå¤„ç†æ¨¡å¼ (backup, replace, skip)ï¼Œé»˜è®¤ä¸ºbackup
    
    æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š
    - .ddl: DDLæ–‡ä»¶
    - .md: Markdownæ–‡æ¡£
    - .txt: æ–‡æœ¬æ–‡ä»¶
    - .json: JSONæ–‡ä»¶
    - .sql: SQLæ–‡ä»¶
    - .csv: CSVæ–‡ä»¶
    
    é‡åå¤„ç†æ¨¡å¼ï¼š
    - backup: å¤‡ä»½åŸæ–‡ä»¶ï¼ˆé»˜è®¤ï¼‰
    - replace: ç›´æ¥è¦†ç›–
    - skip: è·³è¿‡ä¸Šä¼ 
    """
    try:
        # éªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        manager = get_data_pipeline_manager()
        task_info = manager.get_task_status(task_id)
        if not task_info:
            return jsonify(not_found_response(
                response_text=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
            )), 404
        
        # æ£€æŸ¥æ˜¯å¦æœ‰æ–‡ä»¶ä¸Šä¼ 
        if 'file' not in request.files:
            return jsonify(bad_request_response(
                response_text="è¯·é€‰æ‹©è¦ä¸Šä¼ çš„æ–‡ä»¶",
                missing_params=['file']
            )), 400
        
        file = request.files['file']
        
        # éªŒè¯æ–‡ä»¶å
        if file.filename == '':
            return jsonify(bad_request_response(
                response_text="è¯·é€‰æ‹©æœ‰æ•ˆçš„æ–‡ä»¶"
            )), 400
        
        # è·å–é‡åå¤„ç†æ¨¡å¼
        overwrite_mode = request.form.get('overwrite_mode', 'backup')
        
        # éªŒè¯é‡åå¤„ç†æ¨¡å¼
        valid_modes = ['backup', 'replace', 'skip']
        if overwrite_mode not in valid_modes:
            return jsonify(bad_request_response(
                response_text=f"æ— æ•ˆçš„overwrite_modeå‚æ•°: {overwrite_mode}ï¼Œæ”¯æŒçš„å€¼: {valid_modes}",
                invalid_params=['overwrite_mode']
            )), 400
        
        try:
            # ä½¿ç”¨æ–‡ä»¶ç®¡ç†å™¨ä¸Šä¼ æ–‡ä»¶
            file_manager = get_data_pipeline_file_manager()
            result = file_manager.upload_file_to_task(task_id, file, file.filename, overwrite_mode)
            
            # æ£€æŸ¥æ˜¯å¦è·³è¿‡ä¸Šä¼ 
            if result.get('skipped'):
                return jsonify(success_response(
                    response_text=result.get('message', 'æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸Šä¼ '),
                    data=result
                )), 200
            
            return jsonify(success_response(
                response_text="æ–‡ä»¶ä¸Šä¼ æˆåŠŸ",
                data=result
            )), 200
            
        except ValueError as e:
            # æ–‡ä»¶éªŒè¯é”™è¯¯ï¼ˆå¦‚æ–‡ä»¶å¤ªå¤§ã€ç©ºæ–‡ä»¶ã€ä¸æ”¯æŒçš„ç±»å‹ç­‰ï¼‰
            return jsonify(bad_request_response(
                response_text=str(e)
            )), 400
        except Exception as e:
            logger.error(f"ä¸Šä¼ æ–‡ä»¶å¤±è´¥: {str(e)}")
            return jsonify(internal_error_response(
                response_text="æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
            )), 500
        
    except Exception as e:
        logger.error(f"å¤„ç†æ–‡ä»¶ä¸Šä¼ è¯·æ±‚å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="å¤„ç†ä¸Šä¼ è¯·æ±‚å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

# ä»»åŠ¡ç›®å½•åˆ é™¤åŠŸèƒ½ï¼ˆä» citu_app.py è¿ç§»ï¼‰
import shutil
from pathlib import Path
import psycopg2
from app_config import PGVECTOR_CONFIG

def delete_task_directory_simple(task_id, delete_database_records=False):
    """
    ç®€å•çš„ä»»åŠ¡ç›®å½•åˆ é™¤åŠŸèƒ½ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    - åˆ é™¤ data_pipeline/training_data/{task_id} ç›®å½•
    - æ›´æ–°æ•°æ®åº“ä¸­çš„ directory_exists å­—æ®µ
    - å¯é€‰ï¼šåˆ é™¤æ•°æ®åº“è®°å½•
    """
    try:
        # 1. åˆ é™¤ç›®å½•
        project_root = Path(__file__).parent.absolute()
        task_dir = project_root / "data_pipeline" / "training_data" / task_id
        
        deleted_files_count = 0
        deleted_size = 0
        
        if task_dir.exists():
            # è®¡ç®—åˆ é™¤å‰çš„ç»Ÿè®¡ä¿¡æ¯
            for file_path in task_dir.rglob('*'):
                if file_path.is_file():
                    deleted_files_count += 1
                    deleted_size += file_path.stat().st_size
            
            # åˆ é™¤ç›®å½•
            shutil.rmtree(task_dir)
            directory_deleted = True
            operation_message = "ç›®å½•åˆ é™¤æˆåŠŸ"
        else:
            directory_deleted = False
            operation_message = "ç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤"
        
        # 2. æ›´æ–°æ•°æ®åº“
        database_records_deleted = False
        
        try:
            conn = psycopg2.connect(**PGVECTOR_CONFIG)
            cur = conn.cursor()
            
            if delete_database_records:
                # åˆ é™¤ä»»åŠ¡æ­¥éª¤è®°å½•
                cur.execute("DELETE FROM data_pipeline_task_steps WHERE task_id = %s", (task_id,))
                # åˆ é™¤ä»»åŠ¡ä¸»è®°å½•
                cur.execute("DELETE FROM data_pipeline_tasks WHERE task_id = %s", (task_id,))
                database_records_deleted = True
            else:
                # åªæ›´æ–°ç›®å½•çŠ¶æ€
                cur.execute("""
                    UPDATE data_pipeline_tasks 
                    SET directory_exists = FALSE, updated_at = CURRENT_TIMESTAMP 
                    WHERE task_id = %s
                """, (task_id,))
            
            conn.commit()
            cur.close()
            conn.close()
            
        except Exception as db_error:
            logger.error(f"æ•°æ®åº“æ“ä½œå¤±è´¥: {db_error}")
            # æ•°æ®åº“å¤±è´¥ä¸å½±å“æ–‡ä»¶åˆ é™¤çš„ç»“æœ
        
        # 3. æ ¼å¼åŒ–æ–‡ä»¶å¤§å°
        def format_size(size_bytes):
            if size_bytes < 1024:
                return f"{size_bytes} B"
            elif size_bytes < 1024**2:
                return f"{size_bytes/1024:.1f} KB"
            elif size_bytes < 1024**3:
                return f"{size_bytes/(1024**2):.1f} MB"
            else:
                return f"{size_bytes/(1024**3):.1f} GB"
        
        return {
            "success": True,
            "task_id": task_id,
            "directory_deleted": directory_deleted,
            "database_records_deleted": database_records_deleted,
            "deleted_files_count": deleted_files_count,
            "deleted_size": format_size(deleted_size),
            "deleted_at": datetime.now().isoformat(),
            "operation_message": operation_message  # æ–°å¢ï¼šå…·ä½“çš„æ“ä½œæ¶ˆæ¯
        }
        
    except Exception as e:
        logger.error(f"åˆ é™¤ä»»åŠ¡ç›®å½•å¤±è´¥: {task_id}, é”™è¯¯: {str(e)}")
        return {
            "success": False,
            "task_id": task_id,
            "error": str(e),
            "error_code": "DELETE_FAILED",
            "operation_message": f"åˆ é™¤æ“ä½œå¤±è´¥: {str(e)}"  # æ–°å¢ï¼šå¤±è´¥æ¶ˆæ¯
        }

@app.route('/api/v0/data_pipeline/tasks', methods=['DELETE'])
def delete_tasks():
    """åˆ é™¤ä»»åŠ¡ç›®å½•ï¼ˆæ”¯æŒå•ä¸ªå’Œæ‰¹é‡ï¼‰ï¼ˆä» citu_app.py è¿ç§»ï¼‰"""
    try:
        # æ™ºèƒ½è·å–å‚æ•°ï¼šæ”¯æŒJSON bodyå’ŒURLæŸ¥è¯¢å‚æ•°ä¸¤ç§æ–¹å¼
        def get_request_parameter(param_name, array_param_name=None):
            """ä»JSON bodyæˆ–URLæŸ¥è¯¢å‚æ•°ä¸­è·å–å‚æ•°å€¼"""
            # 1. ä¼˜å…ˆä»JSON bodyè·å–
            if request.is_json:
                try:
                    json_data = request.get_json()
                    if json_data and param_name in json_data:
                        return json_data[param_name]
                except:
                    pass
            
            # 2. ä»URLæŸ¥è¯¢å‚æ•°è·å–
            if param_name in request.args:
                value = request.args.get(param_name)
                # å¤„ç†å¸ƒå°”å€¼
                if value.lower() in ('true', '1', 'yes'):
                    return True
                elif value.lower() in ('false', '0', 'no'):
                    return False
                return value
            
            # 3. å¤„ç†æ•°ç»„å‚æ•°ï¼ˆå¦‚ task_ids[]ï¼‰
            if array_param_name and array_param_name in request.args:
                return request.args.getlist(array_param_name)
            
            return None
        
        # è·å–å‚æ•°
        task_ids = get_request_parameter('task_ids', 'task_ids[]')
        confirm = get_request_parameter('confirm')
        
        if not task_ids:
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘å¿…éœ€å‚æ•°: task_ids",
                missing_params=['task_ids']
            )), 400
        
        if not confirm:
            return jsonify(bad_request_response(
                response_text="ç¼ºå°‘å¿…éœ€å‚æ•°: confirm",
                missing_params=['confirm']
            )), 400
        
        if confirm != True:
            return jsonify(bad_request_response(
                response_text="confirmå‚æ•°å¿…é¡»ä¸ºtrueä»¥ç¡®è®¤åˆ é™¤æ“ä½œ"
            )), 400
        
        if not isinstance(task_ids, list) or len(task_ids) == 0:
            return jsonify(bad_request_response(
                response_text="task_idså¿…é¡»æ˜¯éç©ºçš„ä»»åŠ¡IDåˆ—è¡¨"
            )), 400
        
        # è·å–å¯é€‰å‚æ•°
        delete_database_records = get_request_parameter('delete_database_records') or False
        continue_on_error = get_request_parameter('continue_on_error')
        if continue_on_error is None:
            continue_on_error = True
        
        # æ‰§è¡Œæ‰¹é‡åˆ é™¤æ“ä½œ
        deleted_tasks = []
        failed_tasks = []
        total_size_freed = 0
        
        for task_id in task_ids:
            result = delete_task_directory_simple(task_id, delete_database_records)
            
            if result["success"]:
                deleted_tasks.append(result)
                # ç´¯è®¡é‡Šæ”¾çš„ç©ºé—´å¤§å°ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥è§£æsizeå­—ç¬¦ä¸²ï¼‰
            else:
                failed_tasks.append({
                    "task_id": task_id,
                    "error": result["error"],
                    "error_code": result.get("error_code", "UNKNOWN")
                })
                
                if not continue_on_error:
                    break
        
        # æ„å»ºå“åº”
        summary = {
            "total_requested": len(task_ids),
            "successfully_deleted": len(deleted_tasks),
            "failed": len(failed_tasks)
        }
        
        batch_result = {
            "deleted_tasks": deleted_tasks,
            "failed_tasks": failed_tasks,
            "summary": summary,
            "deleted_at": datetime.now().isoformat()
        }
        
        # æ„å»ºæ™ºèƒ½å“åº”æ¶ˆæ¯
        if len(task_ids) == 1:
            # å•ä¸ªåˆ é™¤ï¼šä½¿ç”¨å…·ä½“çš„æ“ä½œæ¶ˆæ¯
            if summary["failed"] == 0:
                # ä»deleted_tasksä¸­è·å–å…·ä½“çš„æ“ä½œæ¶ˆæ¯
                operation_msg = deleted_tasks[0].get('operation_message', 'ä»»åŠ¡å¤„ç†å®Œæˆ')
                message = operation_msg
            else:
                # ä»failed_tasksä¸­è·å–é”™è¯¯æ¶ˆæ¯
                error_msg = failed_tasks[0].get('error', 'åˆ é™¤å¤±è´¥')
                message = f"ä»»åŠ¡åˆ é™¤å¤±è´¥: {error_msg}"
        else:
            # æ‰¹é‡åˆ é™¤ï¼šç»Ÿè®¡å„ç§æ“ä½œç»“æœ
            directory_deleted_count = sum(1 for task in deleted_tasks if task.get('directory_deleted', False))
            directory_not_exist_count = sum(1 for task in deleted_tasks if not task.get('directory_deleted', False))
            
            if summary["failed"] == 0:
                # å…¨éƒ¨æˆåŠŸ
                if directory_deleted_count > 0 and directory_not_exist_count > 0:
                    message = f"æ‰¹é‡æ“ä½œå®Œæˆï¼š{directory_deleted_count}ä¸ªç›®å½•å·²åˆ é™¤ï¼Œ{directory_not_exist_count}ä¸ªç›®å½•ä¸å­˜åœ¨"
                elif directory_deleted_count > 0:
                    message = f"æ‰¹é‡åˆ é™¤å®Œæˆï¼šæˆåŠŸåˆ é™¤{directory_deleted_count}ä¸ªç›®å½•"
                elif directory_not_exist_count > 0:
                    message = f"æ‰¹é‡æ“ä½œå®Œæˆï¼š{directory_not_exist_count}ä¸ªç›®å½•ä¸å­˜åœ¨ï¼Œæ— éœ€åˆ é™¤"
                else:
                    message = "æ‰¹é‡æ“ä½œå®Œæˆ"
            elif summary["successfully_deleted"] == 0:
                message = f"æ‰¹é‡åˆ é™¤å¤±è´¥ï¼š{summary['failed']}ä¸ªä»»åŠ¡å¤„ç†å¤±è´¥"
            else:
                message = f"æ‰¹é‡åˆ é™¤éƒ¨åˆ†å®Œæˆï¼šæˆåŠŸ{summary['successfully_deleted']}ä¸ªï¼Œå¤±è´¥{summary['failed']}ä¸ª"
        
        return jsonify(success_response(
            response_text=message,
            data=batch_result
        )), 200
        
    except Exception as e:
        logger.error(f"åˆ é™¤ä»»åŠ¡å¤±è´¥: é”™è¯¯: {str(e)}")
        return jsonify(internal_error_response(
            response_text="åˆ é™¤ä»»åŠ¡å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500

@app.route('/api/v0/data_pipeline/tasks/<task_id>/logs/query', methods=['POST'])
def query_data_pipeline_task_logs(task_id):
    """
    é«˜çº§æŸ¥è¯¢æ•°æ®ç®¡é“ä»»åŠ¡æ—¥å¿—ï¼ˆä» citu_app.py è¿ç§»ï¼‰
    
    æ”¯æŒå¤æ‚ç­›é€‰ã€æ’åºã€åˆ†é¡µåŠŸèƒ½
    
    è¯·æ±‚ä½“:
    {
        "page": 1,                          // é¡µç ï¼Œå¿…é¡»å¤§äº0ï¼Œé»˜è®¤1
        "page_size": 50,                    // æ¯é¡µå¤§å°ï¼Œ1-500ä¹‹é—´ï¼Œé»˜è®¤50
        "level": "ERROR",                   // å¯é€‰ï¼Œæ—¥å¿—çº§åˆ«ç­›é€‰ï¼š"DEBUG"|"INFO"|"WARNING"|"ERROR"|"CRITICAL"
        "start_time": "2025-01-01 00:00:00", // å¯é€‰ï¼Œå¼€å§‹æ—¶é—´èŒƒå›´ (YYYY-MM-DD HH:MM:SS)
        "end_time": "2025-01-02 23:59:59",   // å¯é€‰ï¼Œç»“æŸæ—¶é—´èŒƒå›´ (YYYY-MM-DD HH:MM:SS)
        "keyword": "failed",                 // å¯é€‰ï¼Œå…³é”®å­—æœç´¢ï¼ˆæ¶ˆæ¯å†…å®¹æ¨¡ç³ŠåŒ¹é…ï¼‰
        "logger_name": "DDLGenerator",       // å¯é€‰ï¼Œæ—¥å¿—è®°å½•å™¨åç§°ç²¾ç¡®åŒ¹é…
        "step_name": "ddl_generation",       // å¯é€‰ï¼Œæ‰§è¡Œæ­¥éª¤åç§°ç²¾ç¡®åŒ¹é…
        "sort_by": "timestamp",              // å¯é€‰ï¼Œæ’åºå­—æ®µï¼š"timestamp"|"level"|"logger"|"step"|"line_number"ï¼Œé»˜è®¤"timestamp"
        "sort_order": "desc"                 // å¯é€‰ï¼Œæ’åºæ–¹å‘ï¼š"asc"|"desc"ï¼Œé»˜è®¤"desc"
    }
    """
    try:
        # éªŒè¯ä»»åŠ¡æ˜¯å¦å­˜åœ¨
        manager = get_data_pipeline_manager()
        task_info = manager.get_task_status(task_id)
        if not task_info:
            return jsonify(not_found_response(
                response_text=f"ä»»åŠ¡ä¸å­˜åœ¨: {task_id}"
            )), 404
        
        # è§£æè¯·æ±‚æ•°æ®
        request_data = request.get_json() or {}
        
        # å‚æ•°éªŒè¯
        def _is_valid_time_format(time_str):
            """éªŒè¯æ—¶é—´æ ¼å¼æ˜¯å¦æœ‰æ•ˆ"""
            if not time_str:
                return True
            
            # æ”¯æŒçš„æ—¶é—´æ ¼å¼
            time_formats = [
                '%Y-%m-%d %H:%M:%S',     # 2025-01-01 00:00:00
                '%Y-%m-%d',              # 2025-01-01
                '%Y-%m-%dT%H:%M:%S',     # 2025-01-01T00:00:00
                '%Y-%m-%dT%H:%M:%S.%f',  # 2025-01-01T00:00:00.123456
            ]
            
            for fmt in time_formats:
                try:
                    from datetime import datetime
                    datetime.strptime(time_str, fmt)
                    return True
                except ValueError:
                    continue
            return False
        
        # æå–å’ŒéªŒè¯å‚æ•°
        page = request_data.get('page', 1)
        page_size = request_data.get('page_size', 50)
        level = request_data.get('level')
        start_time = request_data.get('start_time')
        end_time = request_data.get('end_time')
        keyword = request_data.get('keyword')
        logger_name = request_data.get('logger_name')
        step_name = request_data.get('step_name')
        sort_by = request_data.get('sort_by', 'timestamp')
        sort_order = request_data.get('sort_order', 'desc')
        
        # å‚æ•°éªŒè¯
        if not isinstance(page, int) or page < 1:
            return jsonify(bad_request_response(
                response_text="é¡µç å¿…é¡»æ˜¯å¤§äº0çš„æ•´æ•°"
            )), 400
        
        if not isinstance(page_size, int) or page_size < 1 or page_size > 500:
            return jsonify(bad_request_response(
                response_text="æ¯é¡µå¤§å°å¿…é¡»æ˜¯1-500ä¹‹é—´çš„æ•´æ•°"
            )), 400
        
        # éªŒè¯æ—¥å¿—çº§åˆ«
        if level and level.upper() not in ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL']:
            return jsonify(bad_request_response(
                response_text="æ—¥å¿—çº§åˆ«å¿…é¡»æ˜¯DEBUGã€INFOã€WARNINGã€ERRORã€CRITICALä¹‹ä¸€"
            )), 400
        
        # éªŒè¯æ—¶é—´æ ¼å¼
        if not _is_valid_time_format(start_time):
            return jsonify(bad_request_response(
                response_text="å¼€å§‹æ—¶é—´æ ¼å¼æ— æ•ˆï¼Œæ”¯æŒæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SS æˆ– YYYY-MM-DD"
            )), 400
        
        if not _is_valid_time_format(end_time):
            return jsonify(bad_request_response(
                response_text="ç»“æŸæ—¶é—´æ ¼å¼æ— æ•ˆï¼Œæ”¯æŒæ ¼å¼ï¼šYYYY-MM-DD HH:MM:SS æˆ– YYYY-MM-DD"
            )), 400
        
        # éªŒè¯å…³é”®å­—é•¿åº¦
        if keyword and len(keyword) > 200:
            return jsonify(bad_request_response(
                response_text="å…³é”®å­—é•¿åº¦ä¸èƒ½è¶…è¿‡200ä¸ªå­—ç¬¦"
            )), 400
        
        # éªŒè¯æ’åºå­—æ®µ
        allowed_sort_fields = ['timestamp', 'level', 'logger', 'step', 'line_number']
        if sort_by not in allowed_sort_fields:
            return jsonify(bad_request_response(
                response_text=f"æ’åºå­—æ®µå¿…é¡»æ˜¯ä»¥ä¸‹ä¹‹ä¸€: {', '.join(allowed_sort_fields)}"
            )), 400
        
        # éªŒè¯æ’åºæ–¹å‘
        if sort_order.lower() not in ['asc', 'desc']:
            return jsonify(bad_request_response(
                response_text="æ’åºæ–¹å‘å¿…é¡»æ˜¯ascæˆ–desc"
            )), 400
        
        # åˆ›å»ºå·¥ä½œæµæ‰§è¡Œå™¨å¹¶æŸ¥è¯¢æ—¥å¿—
        from data_pipeline.api.simple_workflow import SimpleWorkflowExecutor
        executor = SimpleWorkflowExecutor(task_id)
        
        try:
            result = executor.query_logs_advanced(
                page=page,
                page_size=page_size,
                level=level,
                start_time=start_time,
                end_time=end_time,
                keyword=keyword,
                logger_name=logger_name,
                step_name=step_name,
                sort_by=sort_by,
                sort_order=sort_order
            )
            
            return jsonify(success_response(
                response_text="æŸ¥è¯¢ä»»åŠ¡æ—¥å¿—æˆåŠŸ",
                data=result
            ))
            
        finally:
            executor.cleanup()
        
    except Exception as e:
        logger.error(f"æŸ¥è¯¢æ•°æ®ç®¡é“ä»»åŠ¡æ—¥å¿—å¤±è´¥: {str(e)}")
        return jsonify(internal_error_response(
            response_text="æŸ¥è¯¢ä»»åŠ¡æ—¥å¿—å¤±è´¥ï¼Œè¯·ç¨åé‡è¯•"
        )), 500


# ==================== å¯åŠ¨é€»è¾‘ ====================

def signal_handler(signum, frame):
    """ä¿¡å·å¤„ç†å™¨ï¼Œä¼˜é›…é€€å‡º"""
    logger.info(f"æ¥æ”¶åˆ°ä¿¡å· {signum}ï¼Œå‡†å¤‡é€€å‡º...")
    cleanup_resources()
    sys.exit(0)

if __name__ == '__main__':
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    
    logger.info("ğŸš€ å¯åŠ¨ç»Ÿä¸€APIæœåŠ¡...")
    logger.info("ğŸ“ æœåŠ¡åœ°å€: http://localhost:8084")
    logger.info("ğŸ”— å¥åº·æ£€æŸ¥: http://localhost:8084/health")
    logger.info("ğŸ“˜ React Agent API: http://localhost:8084/api/v0/ask_react_agent")
    logger.info("ğŸ“˜ LangGraph Agent API: http://localhost:8084/api/v0/ask_agent")
    
    try:
        # å°è¯•ä½¿ç”¨ASGIæ¨¡å¼å¯åŠ¨ï¼ˆæ¨èï¼‰
        import uvicorn
        from asgiref.wsgi import WsgiToAsgi
        
        logger.info("ğŸš€ ä½¿ç”¨ASGIæ¨¡å¼å¯åŠ¨å¼‚æ­¥Flaskåº”ç”¨...")
        logger.info("   è¿™å°†è§£å†³äº‹ä»¶å¾ªç¯å†²çªé—®é¢˜ï¼Œæ”¯æŒLangGraphå¼‚æ­¥checkpointä¿å­˜")
        
        # å°†Flask WSGIåº”ç”¨è½¬æ¢ä¸ºASGIåº”ç”¨
        asgi_app = WsgiToAsgi(app)
        
        # ä½¿ç”¨uvicornå¯åŠ¨ASGIåº”ç”¨
        uvicorn.run(
            asgi_app,
            host="0.0.0.0",
            port=8084,
            log_level="info",
            access_log=True
        )
        
    except ImportError as e:
        # å¦‚æœç¼ºå°‘ASGIä¾èµ–ï¼Œfallbackåˆ°ä¼ ç»ŸFlaskæ¨¡å¼
        logger.warning("âš ï¸ ASGIä¾èµ–ç¼ºå¤±ï¼Œä½¿ç”¨ä¼ ç»ŸFlaskæ¨¡å¼å¯åŠ¨")
        logger.warning("   å»ºè®®å®‰è£…: pip install uvicorn asgiref")
        logger.warning("   ä¼ ç»Ÿæ¨¡å¼å¯èƒ½å­˜åœ¨å¼‚æ­¥äº‹ä»¶å¾ªç¯å†²çªé—®é¢˜")
        
        # å¯åŠ¨æ ‡å‡†Flaskåº”ç”¨ï¼ˆæ”¯æŒå¼‚æ­¥è·¯ç”±ï¼‰
        app.run(host="0.0.0.0", port=8084, debug=False, threaded=True)