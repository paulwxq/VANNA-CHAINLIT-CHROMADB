import asyncio
import time
import os
from typing import List, Dict, Any, Optional
from pathlib import Path

from data_pipeline.tools.base import ToolRegistry, PipelineExecutor
from data_pipeline.utils.data_structures import TableMetadata, TableProcessingContext, ProcessingResult
from data_pipeline.utils.file_manager import FileNameManager
from data_pipeline.utils.system_filter import SystemTableFilter
from data_pipeline.utils.permission_checker import DatabasePermissionChecker
from data_pipeline.utils.table_parser import TableListParser
from data_pipeline.config import SCHEMA_TOOLS_CONFIG
from data_pipeline.dp_logging import get_logger

class SchemaTrainingDataAgent:
    """Schemaè®­ç»ƒæ•°æ®ç”ŸæˆAI Agent"""
    
    def __init__(self, 
                 db_connection: str,
                 table_list_file: str,
                 business_context: str = None,
                 output_dir: str = None,
                 task_id: str = None,
                 pipeline: str = "full"):
        
        self.db_connection = db_connection
        self.table_list_file = table_list_file
        self.business_context = business_context or "æ•°æ®åº“ç®¡ç†ç³»ç»Ÿ"
        self.pipeline = pipeline
        
        # é…ç½®ç®¡ç†
        self.config = SCHEMA_TOOLS_CONFIG
        self.output_dir = output_dir or self.config["output_directory"]
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.file_manager = FileNameManager(self.output_dir)
        self.system_filter = SystemTableFilter()
        self.table_parser = TableListParser()
        self.pipeline_executor = PipelineExecutor(self.config["available_pipelines"])
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_tables': 0,
            'processed_tables': 0,
            'failed_tables': 0,
            'skipped_tables': 0,
            'start_time': None,
            'end_time': None
        }
        
        self.failed_tables = []
        self.task_id = task_id
        
        # åˆå§‹åŒ–ç‹¬ç«‹æ—¥å¿—ç³»ç»Ÿ
        if task_id:
            self.logger = get_logger("SchemaTrainingDataAgent", task_id)
        else:
            # è„šæœ¬æ¨¡å¼ä¸‹ï¼Œå¦‚æœæ²¡æœ‰ä¼ é€’task_idï¼Œç”Ÿæˆä¸€ä¸ª
            from datetime import datetime
            self.task_id = f"manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.logger = get_logger("SchemaTrainingDataAgent", self.task_id)
    
    async def generate_training_data(self) -> Dict[str, Any]:
        """ä¸»å…¥å£ï¼šç”Ÿæˆè®­ç»ƒæ•°æ®"""
        try:
            self.stats['start_time'] = time.time()
            self.logger.info("ğŸš€ å¼€å§‹ç”ŸæˆSchemaè®­ç»ƒæ•°æ®")
            
            # 1. åˆå§‹åŒ–
            await self._initialize()
            
            # 2. æ£€æŸ¥æ•°æ®åº“æƒé™
            await self._check_database_permissions()
            
            # 3. è§£æè¡¨æ¸…å•
            tables = await self._parse_table_list()
            
            # 4. è¿‡æ»¤ç³»ç»Ÿè¡¨
            user_tables = self._filter_system_tables(tables)
            
            # 5. å¹¶å‘å¤„ç†è¡¨
            results = await self._process_tables_concurrently(user_tables)
            
            # 6. è®¾ç½®ç»“æŸæ—¶é—´
            self.stats['end_time'] = time.time()
            
            # 7. ç”Ÿæˆæ€»ç»“æŠ¥å‘Š
            report = self._generate_summary_report(results)
            
            self.logger.info("âœ… Schemaè®­ç»ƒæ•°æ®ç”Ÿæˆå®Œæˆ")
            
            return report
            
        except Exception as e:
            self.stats['end_time'] = time.time()
            self.logger.exception("âŒ Schemaè®­ç»ƒæ•°æ®ç”Ÿæˆå¤±è´¥")
            raise
    
    async def _initialize(self):
        """åˆå§‹åŒ–Agent"""
        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(self.output_dir, exist_ok=True)
        if self.config["create_subdirectories"]:
            os.makedirs(os.path.join(self.output_dir, "ddl"), exist_ok=True)
            os.makedirs(os.path.join(self.output_dir, "docs"), exist_ok=True)
        
        # logsç›®å½•å§‹ç»ˆåˆ›å»º
        # os.makedirs(os.path.join(self.output_dir, "logs"), exist_ok=True)
        
        # åˆå§‹åŒ–æ•°æ®åº“å·¥å…·
        database_tool = ToolRegistry.get_tool("database_inspector", db_connection=self.db_connection)
        await database_tool._create_connection_pool()
        
        self.logger.info(f"åˆå§‹åŒ–å®Œæˆï¼Œè¾“å‡ºç›®å½•: {self.output_dir}")
    
    async def _check_database_permissions(self):
        """æ£€æŸ¥æ•°æ®åº“æƒé™"""
        if not self.config["check_permissions"]:
            return
        
        inspector = ToolRegistry.get_tool("database_inspector", db_connection=self.db_connection)
        
        # ç¡®ä¿è¿æ¥æ± å·²åˆ›å»º
        if not inspector.connection_pool:
            await inspector._create_connection_pool()
        
        # è§£æå¹¶æ‰“å°æ•°æ®åº“è¿æ¥ä¿¡æ¯
        try:
            db_info = self._parse_db_connection(self.db_connection)
            self.logger.info(f"ğŸ”— æ•°æ®åº“è¿æ¥ä¿¡æ¯: ç”¨æˆ·å={db_info['user']}, å¯†ç ={'*' * len(db_info['password'])}, ä¸»æœº={db_info['host']}:{db_info['port']}, æ•°æ®åº“={db_info['dbname']}")
        except Exception as e:
            self.logger.warning(f"æ— æ³•è§£ææ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²: {e}")
        
        checker = DatabasePermissionChecker(inspector)
        
        permissions = await checker.check_permissions()
        
        if not permissions['connect']:
            raise Exception("æ— æ³•è¿æ¥åˆ°æ•°æ®åº“")
        
        if self.config["require_select_permission"] and not permissions['select_data']:
            if not self.config["allow_readonly_database"]:
                raise Exception("æ•°æ®åº“æŸ¥è¯¢æƒé™ä¸è¶³")
            else:
                self.logger.warning("æ•°æ®åº“ä¸ºåªè¯»æˆ–æƒé™å—é™ï¼Œéƒ¨åˆ†åŠŸèƒ½å¯èƒ½å—å½±å“")
        
        self.logger.info(f"æ•°æ®åº“æƒé™æ£€æŸ¥å®Œæˆ: {permissions}")
    
    def _parse_db_connection(self, db_connection: str) -> Dict[str, str]:
        """
        è§£æPostgreSQLè¿æ¥å­—ç¬¦ä¸²
        
        Args:
            db_connection: PostgreSQLè¿æ¥å­—ç¬¦ä¸²ï¼Œæ ¼å¼ä¸º postgresql://user:password@host:port/dbname
        
        Returns:
            åŒ…å«æ•°æ®åº“è¿æ¥å‚æ•°çš„å­—å…¸
        """
        import re
        
        # è§£æè¿æ¥å­—ç¬¦ä¸²çš„æ­£åˆ™è¡¨è¾¾å¼
        pattern = r'postgresql://([^:]+):([^@]+)@([^:]+):(\d+)/(.+)'
        match = re.match(pattern, db_connection)
        
        if not match:
            raise ValueError(f"æ— æ•ˆçš„PostgreSQLè¿æ¥å­—ç¬¦ä¸²æ ¼å¼: {db_connection}")
        
        user, password, host, port, dbname = match.groups()
        
        return {
            'user': user,
            'password': password,
            'host': host,
            'port': port,
            'dbname': dbname
        }
    
    async def _parse_table_list(self) -> List[str]:
        """è§£æè¡¨æ¸…å•æ–‡ä»¶"""
        tables = self.table_parser.parse_file(self.table_list_file)
        self.stats['total_tables'] = len(tables)
        self.logger.info(f"ğŸ“‹ ä»æ¸…å•æ–‡ä»¶è¯»å–åˆ° {len(tables)} ä¸ªè¡¨")
        return tables
    
    def _filter_system_tables(self, tables: List[str]) -> List[str]:
        """è¿‡æ»¤ç³»ç»Ÿè¡¨"""
        if not self.config["filter_system_tables"]:
            return tables
        
        user_tables = self.system_filter.filter_user_tables(tables)
        filtered_count = len(tables) - len(user_tables)
        
        if filtered_count > 0:
            self.logger.info(f"ğŸ” è¿‡æ»¤äº† {filtered_count} ä¸ªç³»ç»Ÿè¡¨ï¼Œä¿ç•™ {len(user_tables)} ä¸ªç”¨æˆ·è¡¨")
            self.stats['skipped_tables'] += filtered_count
        
        return user_tables
    
    async def _process_tables_concurrently(self, tables: List[str]) -> List[Dict[str, Any]]:
        """å¹¶å‘å¤„ç†è¡¨"""
        max_concurrent = self.config["max_concurrent_tables"]
        semaphore = asyncio.Semaphore(max_concurrent)
        
        self.logger.info(f"ğŸ”„ å¼€å§‹å¹¶å‘å¤„ç† {len(tables)} ä¸ªè¡¨ (æœ€å¤§å¹¶å‘: {max_concurrent})")
        
        # åˆ›å»ºä»»åŠ¡
        tasks = [
            self._process_single_table_with_semaphore(semaphore, table_spec)
            for table_spec in tables
        ]
        
        # å¹¶å‘æ‰§è¡Œ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ç»Ÿè®¡ç»“æœ
        successful = sum(1 for r in results if isinstance(r, dict) and r.get('success', False))
        failed = len(results) - successful
        
        self.stats['processed_tables'] = successful
        self.stats['failed_tables'] = failed
        
        self.logger.info(f"ğŸ“Š å¤„ç†å®Œæˆ: æˆåŠŸ {successful} ä¸ªï¼Œå¤±è´¥ {failed} ä¸ª")
        
        return [r for r in results if isinstance(r, dict)]
    
    async def _process_single_table_with_semaphore(self, semaphore: asyncio.Semaphore, table_spec: str) -> Dict[str, Any]:
        """å¸¦ä¿¡å·é‡çš„å•è¡¨å¤„ç†"""
        async with semaphore:
            return await self._process_single_table(table_spec)
    
    async def _process_single_table(self, table_spec: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªè¡¨"""
        start_time = time.time()
        
        try:
            # è§£æè¡¨å
            if '.' in table_spec:
                schema_name, table_name = table_spec.split('.', 1)
            else:
                schema_name, table_name = 'public', table_spec
            
            full_name = f"{schema_name}.{table_name}"
            self.logger.info(f"ğŸ” å¼€å§‹å¤„ç†è¡¨: {full_name}")
            
            # åˆ›å»ºè¡¨å…ƒæ•°æ®
            table_metadata = TableMetadata(
                schema_name=schema_name,
                table_name=table_name,
                full_name=full_name
            )
            
            # åˆ›å»ºå¤„ç†ä¸Šä¸‹æ–‡
            context = TableProcessingContext(
                table_metadata=table_metadata,
                business_context=self.business_context,
                output_dir=self.output_dir,
                pipeline=self.pipeline,
                vn=None,  # å°†åœ¨å·¥å…·ä¸­æ³¨å…¥
                file_manager=self.file_manager,
                db_connection=self.db_connection,  # æ·»åŠ æ•°æ®åº“è¿æ¥å‚æ•°
                start_time=start_time
            )
            
            # æ‰§è¡Œå¤„ç†é“¾
            step_results = await self.pipeline_executor.execute_pipeline(self.pipeline, context)
            
            # è®¡ç®—æ€»ä½“æˆåŠŸçŠ¶æ€
            success = all(result.success for result in step_results.values())
            
            execution_time = time.time() - start_time
            
            if success:
                self.logger.info(f"âœ… è¡¨ {full_name} å¤„ç†æˆåŠŸï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
            else:
                self.logger.error(f"âŒ è¡¨ {full_name} å¤„ç†å¤±è´¥ï¼Œè€—æ—¶: {execution_time:.2f}ç§’")
                self.failed_tables.append(full_name)
            
            return {
                'success': success,
                'table_name': full_name,
                'execution_time': execution_time,
                'step_results': {k: v.to_dict() for k, v in step_results.items()},
                'metadata': {
                    'fields_count': len(table_metadata.fields),
                    'row_count': table_metadata.row_count,
                    'enum_fields': len([f for f in table_metadata.fields if f.is_enum])
                }
            }
            
        except Exception as e:
            execution_time = time.time() - start_time
            error_msg = f"è¡¨ {table_spec} å¤„ç†å¼‚å¸¸: {str(e)}"
            self.logger.exception(error_msg)
            self.failed_tables.append(table_spec)
            
            return {
                'success': False,
                'table_name': table_spec,
                'execution_time': execution_time,
                'error_message': error_msg,
                'step_results': {}
            }
    
    def _generate_summary_report(self, results: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ç”Ÿæˆæ€»ç»“æŠ¥å‘Š"""
        total_time = self.stats['end_time'] - self.stats['start_time']
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        successful_results = [r for r in results if r.get('success', False)]
        failed_results = [r for r in results if not r.get('success', False)]
        
        total_fields = sum(r.get('metadata', {}).get('fields_count', 0) for r in successful_results)
        total_enum_fields = sum(r.get('metadata', {}).get('enum_fields', 0) for r in successful_results)
        
        avg_execution_time = sum(r.get('execution_time', 0) for r in results) / len(results) if results else 0
        
        # è®¡ç®—ç”Ÿæˆçš„æ–‡ä»¶æ•°é‡
        successful_count = len(successful_results)
        if self.pipeline == 'full':
            md_files_generated = successful_count
            ddl_files_generated = successful_count
            total_files_generated = successful_count * 2
        elif self.pipeline == 'ddl_only':
            md_files_generated = 0
            ddl_files_generated = successful_count
            total_files_generated = successful_count
        elif self.pipeline == 'analysis_only':
            md_files_generated = successful_count
            ddl_files_generated = 0
            total_files_generated = successful_count
        else:
            md_files_generated = successful_count
            ddl_files_generated = 0
            total_files_generated = successful_count
        
        report = {
            'summary': {
                'total_tables': self.stats['total_tables'],
                'processed_successfully': len(successful_results),
                'failed': len(failed_results),
                'skipped_system_tables': self.stats['skipped_tables'],
                'total_execution_time': total_time,
                'average_table_time': avg_execution_time
            },
            'statistics': {
                'total_fields_processed': total_fields,
                'enum_fields_detected': total_enum_fields,
                'md_files_generated': md_files_generated,
                'ddl_files_generated': ddl_files_generated,
                'total_files_generated': total_files_generated
            },
            'failed_tables': self.failed_tables,
            'detailed_results': results,
            'configuration': {
                'pipeline': self.pipeline,
                'business_context': self.business_context,
                'output_directory': self.output_dir,
                'max_concurrent_tables': self.config['max_concurrent_tables']
            }
        }
        
        # è¾“å‡ºæ€»ç»“
        self.logger.info(f"ğŸ“Š å¤„ç†æ€»ç»“:")
        self.logger.info(f"  âœ… æˆåŠŸ: {report['summary']['processed_successfully']} ä¸ªè¡¨")
        self.logger.info(f"  âŒ å¤±è´¥: {report['summary']['failed']} ä¸ªè¡¨")
        self.logger.info(f"  â­ï¸  è·³è¿‡: {report['summary']['skipped_system_tables']} ä¸ªç³»ç»Ÿè¡¨")
        if md_files_generated > 0 and ddl_files_generated > 0:
            self.logger.info(f"  ğŸ“ ç”Ÿæˆæ–‡ä»¶: {md_files_generated} ä¸ªMDæ–‡ä»¶ï¼Œ{ddl_files_generated} ä¸ªDDLæ–‡ä»¶")
        elif md_files_generated > 0:
            self.logger.info(f"  ğŸ“ ç”Ÿæˆæ–‡ä»¶: {md_files_generated} ä¸ªMDæ–‡ä»¶")
        elif ddl_files_generated > 0:
            self.logger.info(f"  ğŸ“ ç”Ÿæˆæ–‡ä»¶: {ddl_files_generated} ä¸ªDDLæ–‡ä»¶")
        else:
            self.logger.info(f"  ğŸ“ ç”Ÿæˆæ–‡ä»¶: 0 ä¸ª")
        self.logger.info(f"  ğŸ• æ€»è€—æ—¶: {total_time:.2f} ç§’")
        
        if self.failed_tables:
            self.logger.warning(f"âŒ å¤±è´¥çš„è¡¨: {', '.join(self.failed_tables)}")
        
        # å†™å…¥æ–‡ä»¶åæ˜ å°„æŠ¥å‘Š
        self.file_manager.write_mapping_report()
        
        return report
    
    async def check_database_permissions(self) -> Dict[str, bool]:
        """æ£€æŸ¥æ•°æ®åº“æƒé™ï¼ˆä¾›å¤–éƒ¨è°ƒç”¨ï¼‰"""
        inspector = ToolRegistry.get_tool("database_inspector", db_connection=self.db_connection)
        await inspector._create_connection_pool()
        checker = DatabasePermissionChecker(inspector)
        return await checker.check_permissions()