import asyncio
import time
import os
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List
import psycopg2
import logging


class VectorTableManager:
    """Vectorè¡¨ç®¡ç†å™¨ï¼Œè´Ÿè´£å¤‡ä»½å’Œæ¸…ç©ºæ“ä½œ"""
    
    def __init__(self, task_output_dir: str, task_id: str = None):
        """
        Args:
            task_output_dir: ä»»åŠ¡è¾“å‡ºç›®å½•ï¼ˆç”¨äºå­˜æ”¾å¤‡ä»½æ–‡ä»¶ï¼‰
            task_id: ä»»åŠ¡IDï¼ˆç”¨äºæ—¥å¿—è®°å½•ï¼‰
        Note:
            æ•°æ®åº“è¿æ¥å°†ä»data_pipeline.config.SCHEMA_TOOLS_CONFIGè‡ªåŠ¨è·å–
        """
        self.task_output_dir = task_output_dir
        self.task_id = task_id
        
        # ä»data_pipeline.configè·å–é…ç½®
        from data_pipeline.config import SCHEMA_TOOLS_CONFIG
        self.config = SCHEMA_TOOLS_CONFIG.get("vector_table_management", {})
        
        # åˆå§‹åŒ–æ—¥å¿—
        if task_id:
            from data_pipeline.dp_logging import get_logger
            self.logger = get_logger("VectorTableManager", task_id)
        else:
            import logging
            self.logger = logging.getLogger("VectorTableManager")
    
    async def execute_vector_management(self, backup: bool, truncate: bool) -> Dict[str, Any]:
        """æ‰§è¡Œvectorè¡¨ç®¡ç†æ“ä½œçš„ä¸»æµç¨‹"""
        
        start_time = time.time()
        
        # 1. å‚æ•°éªŒè¯å’Œè‡ªåŠ¨å¯ç”¨é€»è¾‘
        if truncate and not backup:
            backup = True
            self.logger.info("ğŸ”„ å¯ç”¨truncateæ—¶è‡ªåŠ¨å¯ç”¨backup")
        
        if not backup and not truncate:
            self.logger.info("â­ï¸ æœªå¯ç”¨vectorè¡¨ç®¡ç†ï¼Œè·³è¿‡æ“ä½œ")
            return {"backup_performed": False, "truncate_performed": False}
        
        # 2. åˆå§‹åŒ–ç»“æœç»Ÿè®¡
        result = {
            "backup_performed": backup,
            "truncate_performed": truncate,
            "tables_backed_up": {},
            "truncate_results": {},
            "errors": [],
            "backup_directory": None,
            "duration": 0
        }
        
        try:
            # 3. åˆ›å»ºå¤‡ä»½ç›®å½•
            backup_dir = Path(self.task_output_dir) / self.config.get("backup_directory", "vector_bak")
            if backup:
                backup_dir.mkdir(parents=True, exist_ok=True)
                result["backup_directory"] = str(backup_dir)
                self.logger.info(f"ğŸ“ å¤‡ä»½ç›®å½•: {backup_dir}")
            
            # 4. æ‰§è¡Œå¤‡ä»½æ“ä½œ
            if backup:
                self.logger.info("ğŸ—‚ï¸ å¼€å§‹å¤‡ä»½vectorè¡¨...")
                backup_results = await self.backup_vector_tables()
                result["tables_backed_up"] = backup_results
                
                # æ£€æŸ¥å¤‡ä»½æ˜¯å¦å…¨éƒ¨æˆåŠŸ
                backup_failed = any(not r.get("success", False) for r in backup_results.values())
                if backup_failed:
                    result["errors"].append("éƒ¨åˆ†è¡¨å¤‡ä»½å¤±è´¥")
                    if truncate:
                        self.logger.error("âŒ å¤‡ä»½å¤±è´¥ï¼Œå–æ¶ˆæ¸…ç©ºæ“ä½œ")
                        result["truncate_performed"] = False
                        truncate = False
            
            # 5. æ‰§è¡Œæ¸…ç©ºæ“ä½œï¼ˆä»…åœ¨å¤‡ä»½æˆåŠŸæ—¶ï¼‰
            if truncate:
                self.logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºvectorè¡¨...")
                truncate_results = await self.truncate_vector_tables()
                result["truncate_results"] = truncate_results
                
                # æ£€æŸ¥æ¸…ç©ºæ˜¯å¦æˆåŠŸ
                truncate_failed = any(not r.get("success", False) for r in truncate_results.values())
                if truncate_failed:
                    result["errors"].append("éƒ¨åˆ†è¡¨æ¸…ç©ºå¤±è´¥")
            
            # 6. ç”Ÿæˆå¤‡ä»½æ—¥å¿—æ–‡ä»¶
            if backup and backup_dir.exists():
                self._write_backup_log(backup_dir, result)
            
            # 7. è®¡ç®—æ€»è€—æ—¶
            result["duration"] = time.time() - start_time
            
            # 8. è®°å½•æœ€ç»ˆçŠ¶æ€
            if result["errors"]:
                self.logger.warning(f"âš ï¸ Vectorè¡¨ç®¡ç†å®Œæˆï¼Œä½†æœ‰é”™è¯¯: {'; '.join(result['errors'])}")
            else:
                self.logger.info(f"âœ… Vectorè¡¨ç®¡ç†å®Œæˆï¼Œè€—æ—¶: {result['duration']:.2f}ç§’")
            
            return result
            
        except Exception as e:
            result["duration"] = time.time() - start_time
            result["errors"].append(f"æ‰§è¡Œå¤±è´¥: {str(e)}")
            self.logger.error(f"âŒ Vectorè¡¨ç®¡ç†å¤±è´¥: {e}")
            raise
    
    async def backup_vector_tables(self) -> Dict[str, Any]:
        """å¤‡ä»½vectorè¡¨æ•°æ®"""
        
        # 1. åˆ›å»ºå¤‡ä»½ç›®å½•
        backup_dir = Path(self.task_output_dir) / self.config.get("backup_directory", "vector_bak")
        backup_dir.mkdir(parents=True, exist_ok=True)
        
        # 2. ç”Ÿæˆæ—¶é—´æˆ³
        timestamp = datetime.now().strftime(self.config.get("timestamp_format", "%Y%m%d_%H%M%S"))
        
        # 3. æ‰§è¡Œå¤‡ä»½ï¼ˆæ¯ä¸ªè¡¨åˆ†åˆ«å¤„ç†ï¼‰
        results = {}
        supported_tables = self.config.get("supported_tables", ["langchain_pg_collection", "langchain_pg_embedding"])
        
        for table_name in supported_tables:
            try:
                # 3.1 å®šä¹‰æ–‡ä»¶è·¯å¾„ï¼ˆ.tmpä¸´æ—¶æ–‡ä»¶ï¼‰
                temp_file = backup_dir / f"{table_name}_{timestamp}.csv.tmp"
                final_file = backup_dir / f"{table_name}_{timestamp}.csv"
                
                # ç¡®ä¿ä½¿ç”¨ç»å¯¹è·¯å¾„ï¼ˆPostgreSQL COPYå‘½ä»¤è¦æ±‚ï¼‰
                temp_file_abs = temp_file.resolve()
                
                # 3.2 é€šè¿‡psycopg2ä½¿ç”¨æµå¼å®¢æˆ·ç«¯å¯¼å‡ºï¼ˆæ”¯æŒå¤§æ•°æ®é‡ï¼‰
                start_time = time.time()
                row_count = 0
                batch_size = 10000  # æ¯æ‰¹å¤„ç†1ä¸‡æ¡è®°å½•
                
                with self.get_connection() as conn:
                    # ä¸´æ—¶å…³é—­autocommitä»¥æ”¯æŒæµå¼å¤„ç†
                    old_autocommit = conn.autocommit
                    conn.autocommit = False
                    
                    try:
                        with conn.cursor() as cursor:
                            # è®¾ç½®æ¸¸æ ‡ä¸ºæµå¼æ¨¡å¼
                            cursor.itersize = batch_size
                            
                            # æ‰§è¡Œç¼–ç è®¾ç½®
                            cursor.execute("SET client_encoding TO 'UTF8'")
                            
                            # æ‰§è¡ŒæŸ¥è¯¢
                            cursor.execute(f"SELECT * FROM {table_name}")
                            
                            # è·å–åˆ—å
                            colnames = [desc[0] for desc in cursor.description]
                            
                            # ä½¿ç”¨æµå¼æ–¹å¼å†™å…¥CSVæ–‡ä»¶
                            import csv
                            with open(temp_file_abs, 'w', newline='', encoding='utf-8') as csvfile:
                                writer = csv.writer(csvfile)
                                
                                # å†™å…¥è¡¨å¤´
                                writer.writerow(colnames)
                                
                                # æµå¼è¯»å–å’Œå†™å…¥æ•°æ®
                                while True:
                                    rows = cursor.fetchmany(batch_size)
                                    if not rows:
                                        break
                                        
                                    # æ‰¹é‡å†™å…¥å½“å‰æ‰¹æ¬¡çš„æ•°æ®
                                    for row in rows:
                                        writer.writerow(row)
                                        row_count += 1
                                    
                                    # è®°å½•è¿›åº¦ï¼ˆå¤§æ•°æ®é‡æ—¶æœ‰ç”¨ï¼‰
                                    if row_count % (batch_size * 5) == 0:  # æ¯5ä¸‡æ¡è®°å½•è®°å½•ä¸€æ¬¡
                                        self.logger.info(f"ğŸ“Š {table_name} å·²å¯¼å‡º {row_count} è¡Œæ•°æ®...")
                        
                        # æäº¤äº‹åŠ¡
                        conn.commit()
                        
                    finally:
                        # æ¢å¤åŸæ¥çš„autocommitè®¾ç½®
                        conn.autocommit = old_autocommit
                
                self.logger.info(f"ğŸ“Š {table_name} æµå¼å¯¼å‡ºå®Œæˆï¼Œæ€»è®¡ {row_count} è¡Œ")
                
                # 3.3 å¯¼å‡ºå®Œæˆåï¼Œé‡å‘½åæ–‡ä»¶ (.tmp -> .csv)
                if temp_file.exists():
                    temp_file.rename(final_file)
                    
                    # 3.4 è·å–æ–‡ä»¶ä¿¡æ¯
                    file_stat = final_file.stat()
                    duration = time.time() - start_time
                    
                    results[table_name] = {
                        "success": True,
                        "row_count": row_count,
                        "file_size": self._format_file_size(file_stat.st_size),
                        "backup_file": final_file.name,
                        "duration": duration
                    }
                    
                    self.logger.info(f"âœ… {table_name} å¤‡ä»½æˆåŠŸ: {row_count}è¡Œ -> {final_file.name}")
                else:
                    raise Exception(f"ä¸´æ—¶æ–‡ä»¶ {temp_file} æœªç”Ÿæˆ")
                    
            except Exception as e:
                results[table_name] = {
                    "success": False,
                    "error": str(e)
                }
                self.logger.error(f"âŒ {table_name} å¤‡ä»½å¤±è´¥: {e}")
                
                # æ¸…ç†å¯èƒ½çš„ä¸´æ—¶æ–‡ä»¶
                if temp_file.exists():
                    temp_file.unlink()
        
        return results
    
    async def truncate_vector_tables(self) -> Dict[str, Any]:
        """æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆåªæ¸…ç©ºlangchain_pg_embeddingï¼‰"""
        
        results = {}
        
        # åªæ¸…ç©ºé…ç½®ä¸­æŒ‡å®šçš„è¡¨ï¼ˆé€šå¸¸åªæœ‰langchain_pg_embeddingï¼‰
        truncate_tables = self.config.get("truncate_tables", ["langchain_pg_embedding"])
        
        for table_name in truncate_tables:
            try:
                # è®°å½•æ¸…ç©ºå‰çš„è¡Œæ•°ï¼ˆç”¨äºç»Ÿè®¡ï¼‰
                count_sql = f"SELECT COUNT(*) FROM {table_name}"
                
                start_time = time.time()
                with self.get_connection() as conn:
                    with conn.cursor() as cursor:
                        # 1. è·å–æ¸…ç©ºå‰çš„è¡Œæ•°
                        cursor.execute(count_sql)
                        rows_before = cursor.fetchone()[0]
                        
                        # 2. æ‰§è¡ŒTRUNCATE
                        cursor.execute(f"TRUNCATE TABLE {table_name}")
                        
                        # 3. éªŒè¯æ¸…ç©ºç»“æœ
                        cursor.execute(count_sql)
                        rows_after = cursor.fetchone()[0]
                
                duration = time.time() - start_time
                
                if rows_after == 0:
                    results[table_name] = {
                        "success": True,
                        "rows_before": rows_before,
                        "rows_after": rows_after,
                        "duration": duration
                    }
                    self.logger.info(f"âœ… {table_name} æ¸…ç©ºæˆåŠŸ: {rows_before}è¡Œ -> 0è¡Œ")
                else:
                    raise Exception(f"æ¸…ç©ºå¤±è´¥ï¼Œè¡¨ä¸­ä»æœ‰ {rows_after} è¡Œæ•°æ®")
                    
            except Exception as e:
                results[table_name] = {
                    "success": False,
                    "error": str(e)
                }
                self.logger.error(f"âŒ {table_name} æ¸…ç©ºå¤±è´¥: {e}")
        
        return results
    
    def get_connection(self):
        """è·å–pgvectoræ•°æ®åº“è¿æ¥ï¼ˆä»data_pipeline.configè·å–é…ç½®ï¼‰"""
        import psycopg2
        
        try:
            # æ–¹æ³•1ï¼šå¦‚æœSCHEMA_TOOLS_CONFIGä¸­æœ‰è¿æ¥å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
            from data_pipeline.config import SCHEMA_TOOLS_CONFIG
            connection_string = SCHEMA_TOOLS_CONFIG.get("default_db_connection")
            if connection_string:
                conn = psycopg2.connect(connection_string)
            else:
                # æ–¹æ³•2ï¼šä»app_configè·å–pgvectoræ•°æ®åº“é…ç½®
                import app_config
                pgvector_config = app_config.PGVECTOR_CONFIG
                conn = psycopg2.connect(
                    host=pgvector_config.get('host'),
                    port=pgvector_config.get('port'),
                    database=pgvector_config.get('dbname'),
                    user=pgvector_config.get('user'),
                    password=pgvector_config.get('password')
                )
            
            # è®¾ç½®è‡ªåŠ¨æäº¤ï¼Œé¿å…äº‹åŠ¡é—®é¢˜
            conn.autocommit = True
            return conn
            
        except Exception as e:
            self.logger.error(f"pgvectoræ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise

    def _write_backup_log(self, backup_dir: Path, result: Dict[str, Any]):
        """å†™å…¥è¯¦ç»†çš„å¤‡ä»½æ—¥å¿—"""
        log_file = backup_dir / "vector_backup_log.txt"
        
        try:
            with open(log_file, 'w', encoding='utf-8') as f:
                f.write("=== Vector Table Backup Log ===\n")
                f.write(f"Backup Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
                f.write(f"Task ID: {self.task_id or 'Unknown'}\n")
                f.write(f"Duration: {result.get('duration', 0):.2f}s\n\n")
                
                # å¤‡ä»½çŠ¶æ€
                f.write("Tables Backup Status:\n")
                for table_name, info in result.get("tables_backed_up", {}).items():
                    if info.get("success", False):
                        f.write(f"âœ“ {table_name}: {info['row_count']} rows -> {info['backup_file']} ({info['file_size']})\n")
                    else:
                        f.write(f"âœ— {table_name}: FAILED - {info.get('error', 'Unknown error')}\n")
                
                # æ¸…ç©ºçŠ¶æ€
                if result.get("truncate_performed", False):
                    f.write("\nTruncate Status:\n")
                    for table_name, info in result.get("truncate_results", {}).items():
                        if info.get("success", False):
                            f.write(f"âœ“ {table_name}: TRUNCATED ({info['rows_before']} rows removed)\n")
                        else:
                            f.write(f"âœ— {table_name}: FAILED - {info.get('error', 'Unknown error')}\n")
                else:
                    f.write("\nTruncate Status:\n- Not performed\n")
                
                # é”™è¯¯æ±‡æ€»
                if result.get("errors"):
                    f.write(f"\nErrors: {'; '.join(result['errors'])}\n")
                    
        except Exception as e:
            self.logger.warning(f"å†™å…¥å¤‡ä»½æ—¥å¿—å¤±è´¥: {e}")
    
    def _format_file_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º"""
        if size_bytes == 0:
            return "0 B"
        
        size_names = ["B", "KB", "MB", "GB"]
        i = 0
        size = float(size_bytes)
        
        while size >= 1024.0 and i < len(size_names) - 1:
            size /= 1024.0
            i += 1
        
        return f"{size:.1f} {size_names[i]}" 