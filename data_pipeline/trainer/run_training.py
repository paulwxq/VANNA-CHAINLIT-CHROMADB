# run_training.py
import os
import time
import re
import json
import sys
import requests
import pandas as pd
import argparse
from pathlib import Path
from sqlalchemy import create_engine


from .vanna_trainer import (
    train_ddl,
    train_documentation,
    train_sql_example,
    train_question_sql_pair,
    flush_training,
    shutdown_trainer
)

def check_embedding_model_connection():
    """æ£€æŸ¥åµŒå…¥æ¨¡å‹è¿æ¥æ˜¯å¦å¯ç”¨    
    å¦‚æœæ— æ³•è¿æ¥åˆ°åµŒå…¥æ¨¡å‹ï¼Œåˆ™ç»ˆæ­¢ç¨‹åºæ‰§è¡Œ    
    Returns:
        bool: è¿æ¥æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™ç»ˆæ­¢ç¨‹åº
    """
    from core.embedding_function import test_embedding_connection

    print("æ­£åœ¨æ£€æŸ¥åµŒå…¥æ¨¡å‹è¿æ¥...")
    
    # ä½¿ç”¨ä¸“é—¨çš„æµ‹è¯•å‡½æ•°è¿›è¡Œè¿æ¥æµ‹è¯•
    test_result = test_embedding_connection()
    
    if test_result["success"]:
        print(f"å¯ä»¥ç»§ç»­è®­ç»ƒè¿‡ç¨‹ã€‚")
        return True
    else:
        print(f"\né”™è¯¯: æ— æ³•è¿æ¥åˆ°åµŒå…¥æ¨¡å‹: {test_result['message']}")
        print("è®­ç»ƒè¿‡ç¨‹ç»ˆæ­¢ã€‚è¯·æ£€æŸ¥é…ç½®å’ŒAPIæœåŠ¡å¯ç”¨æ€§ã€‚")
        sys.exit(1)

def read_file_by_delimiter(filepath, delimiter="---"):
    """é€šç”¨è¯»å–ï¼šå°†æ–‡ä»¶æŒ‰åˆ†éš”ç¬¦åˆ‡ç‰‡ä¸ºå¤šä¸ªæ®µè½"""
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    blocks = [block.strip() for block in content.split(delimiter) if block.strip()]
    return blocks

def read_markdown_file_by_sections(filepath):
    """ä¸“é—¨ç”¨äºMarkdownæ–‡ä»¶ï¼šæŒ‰æ ‡é¢˜(#ã€##ã€###)åˆ†å‰²æ–‡æ¡£
    
    Args:
        filepath (str): Markdownæ–‡ä»¶è·¯å¾„
        
    Returns:
        list: åˆ†å‰²åçš„Markdownç« èŠ‚åˆ—è¡¨
    """
    with open(filepath, "r", encoding="utf-8") as f:
        content = f.read()
    
    # ç¡®å®šæ–‡ä»¶æ˜¯å¦ä¸ºMarkdown
    is_markdown = filepath.lower().endswith('.md') or filepath.lower().endswith('.markdown')
    
    if not is_markdown:
        # éMarkdownæ–‡ä»¶ä½¿ç”¨é»˜è®¤çš„---åˆ†éš”
        return read_file_by_delimiter(filepath, "---")
    
    # ç›´æ¥æŒ‰ç…§æ ‡é¢˜çº§åˆ«åˆ†å‰²å†…å®¹ï¼Œå¤„ç†#ã€##å’Œ###
    sections = []
    
    # åŒ¹é…æ‰€æœ‰çº§åˆ«çš„æ ‡é¢˜ï¼ˆ#ã€##æˆ–###å¼€å¤´ï¼‰
    header_pattern = r'(?:^|\n)((?:#|##|###)[^#].*?)(?=\n(?:#|##|###)[^#]|\Z)'
    all_sections = re.findall(header_pattern, content, re.DOTALL)
    
    for section in all_sections:
        section = section.strip()
        if section:
            sections.append(section)
    
    # å¤„ç†æ²¡æœ‰åŒ¹é…åˆ°æ ‡é¢˜çš„æƒ…å†µ
    if not sections and content.strip():
        sections = [content.strip()]
        
    return sections

def train_ddl_statements(ddl_file):
    """è®­ç»ƒDDLè¯­å¥
    Args:
        ddl_file (str): DDLæ–‡ä»¶è·¯å¾„
    """
    print(f"å¼€å§‹è®­ç»ƒ DDL: {ddl_file}")
    if not os.path.exists(ddl_file):
        print(f"DDL æ–‡ä»¶ä¸å­˜åœ¨: {ddl_file}")
        return
    for idx, ddl in enumerate(read_file_by_delimiter(ddl_file, ";"), start=1):
        try:
            print(f"\n DDL è®­ç»ƒ {idx}")
            train_ddl(ddl)
        except Exception as e:
            print(f"é”™è¯¯ï¼šDDL #{idx} - {e}")

def train_documentation_blocks(doc_file):
    """è®­ç»ƒæ–‡æ¡£å—
    Args:
        doc_file (str): æ–‡æ¡£æ–‡ä»¶è·¯å¾„
    """
    print(f"å¼€å§‹è®­ç»ƒ æ–‡æ¡£: {doc_file}")
    if not os.path.exists(doc_file):
        print(f"æ–‡æ¡£æ–‡ä»¶ä¸å­˜åœ¨: {doc_file}")
        return
    
    # æ£€æŸ¥æ˜¯å¦ä¸ºMarkdownæ–‡ä»¶
    is_markdown = doc_file.lower().endswith('.md') or doc_file.lower().endswith('.markdown')
    
    if is_markdown:
        # ä½¿ç”¨Markdownä¸“ç”¨åˆ†å‰²å™¨
        sections = read_markdown_file_by_sections(doc_file)
        print(f" Markdownæ–‡æ¡£å·²åˆ†å‰²ä¸º {len(sections)} ä¸ªç« èŠ‚")
        
        for idx, section in enumerate(sections, start=1):
            try:
                section_title = section.split('\n', 1)[0].strip()
                print(f"\n Markdownç« èŠ‚è®­ç»ƒ {idx}: {section_title}")
                
                # æ£€æŸ¥éƒ¨åˆ†é•¿åº¦å¹¶æä¾›è­¦å‘Š
                if len(section) > 2000:
                    print(f" ç« èŠ‚ {idx} é•¿åº¦ä¸º {len(section)} å­—ç¬¦ï¼Œæ¥è¿‘APIé™åˆ¶(2048)")
                
                train_documentation(section)
            except Exception as e:
                print(f" é”™è¯¯ï¼šç« èŠ‚ #{idx} - {e}")
    else:
        # éMarkdownæ–‡ä»¶ä½¿ç”¨ä¼ ç»Ÿçš„---åˆ†éš”
        for idx, doc in enumerate(read_file_by_delimiter(doc_file, "---"), start=1):
            try:
                print(f"\n æ–‡æ¡£è®­ç»ƒ {idx}")
                train_documentation(doc)
            except Exception as e:
                print(f" é”™è¯¯ï¼šæ–‡æ¡£ #{idx} - {e}")

def train_sql_examples(sql_file):
    """è®­ç»ƒSQLç¤ºä¾‹
    Args:
        sql_file (str): SQLç¤ºä¾‹æ–‡ä»¶è·¯å¾„
    """
    print(f" å¼€å§‹è®­ç»ƒ SQL ç¤ºä¾‹: {sql_file}")
    if not os.path.exists(sql_file):
        print(f" SQL ç¤ºä¾‹æ–‡ä»¶ä¸å­˜åœ¨: {sql_file}")
        return
    for idx, sql in enumerate(read_file_by_delimiter(sql_file, ";"), start=1):
        try:
            print(f"\n SQL ç¤ºä¾‹è®­ç»ƒ {idx}")
            train_sql_example(sql)
        except Exception as e:
            print(f" é”™è¯¯ï¼šSQL #{idx} - {e}")

def train_question_sql_pairs(qs_file):
    """è®­ç»ƒé—®ç­”å¯¹
    Args:
        qs_file (str): é—®ç­”å¯¹æ–‡ä»¶è·¯å¾„
    """
    print(f" å¼€å§‹è®­ç»ƒ é—®ç­”å¯¹: {qs_file}")
    if not os.path.exists(qs_file):
        print(f" é—®ç­”æ–‡ä»¶ä¸å­˜åœ¨: {qs_file}")
        return
    try:
        with open(qs_file, "r", encoding="utf-8") as f:
            lines = f.readlines()
        for idx, line in enumerate(lines, start=1):
            if "::" not in line:
                continue
            question, sql = line.strip().split("::", 1)
            print(f"\n é—®ç­”è®­ç»ƒ {idx}")
            train_question_sql_pair(question.strip(), sql.strip())
    except Exception as e:
        print(f" é”™è¯¯ï¼šé—®ç­”è®­ç»ƒ - {e}")

def train_formatted_question_sql_pairs(formatted_file):
    """è®­ç»ƒæ ¼å¼åŒ–çš„é—®ç­”å¯¹æ–‡ä»¶
    æ”¯æŒä¸¤ç§æ ¼å¼ï¼š
    1. Question: xxx\nSQL: xxx (å•è¡ŒSQL)
    2. Question: xxx\nSQL:\nxxx\nxxx (å¤šè¡ŒSQL)
    
    Args:
        formatted_file (str): æ ¼å¼åŒ–é—®ç­”å¯¹æ–‡ä»¶è·¯å¾„
    """
    print(f" å¼€å§‹è®­ç»ƒ æ ¼å¼åŒ–é—®ç­”å¯¹: {formatted_file}")
    if not os.path.exists(formatted_file):
        print(f" æ ¼å¼åŒ–é—®ç­”æ–‡ä»¶ä¸å­˜åœ¨: {formatted_file}")
        return
    
    # è¯»å–æ•´ä¸ªæ–‡ä»¶å†…å®¹
    with open(formatted_file, "r", encoding="utf-8") as f:
        content = f.read()
    
    # æŒ‰åŒç©ºè¡Œåˆ†å‰²ä¸åŒçš„é—®ç­”å¯¹
    # ä½¿ç”¨æ›´ç²¾ç¡®çš„åˆ†éš”ç¬¦ï¼Œé¿å…è¯¯è¯†åˆ«
    pairs = []
    # ä½¿ç”¨å¤§å°å†™ä¸æ•æ„Ÿçš„æ­£åˆ™è¡¨è¾¾å¼æ¥åˆ†å‰²
    import re
    blocks = re.split(r'\n\n(?=question\s*:)', content, flags=re.IGNORECASE)
    
    # å¤„ç†ç¬¬ä¸€å—ï¼ˆå¯èƒ½æ²¡æœ‰å‰å¯¼çš„"\n\nQuestion:"ï¼‰
    first_block = blocks[0]
    if re.search(r'^\s*question\s*:', first_block.strip(), re.IGNORECASE):
        pairs.append(first_block.strip())
    elif re.search(r'question\s*:', first_block, re.IGNORECASE):
        # å¤„ç†æ–‡ä»¶å¼€å¤´æ²¡æœ‰Question:çš„æƒ…å†µ
        question_match = re.search(r'question\s*:', first_block, re.IGNORECASE)
        pairs.append(first_block[question_match.start():].strip())
    
    # å¤„ç†å…¶ä½™å—
    for block in blocks[1:]:
        pairs.append(block.strip())
    
    # å¤„ç†æ¯ä¸ªé—®ç­”å¯¹
    successfully_processed = 0
    for idx, pair in enumerate(pairs, start=1):
        try:
            # ä½¿ç”¨å¤§å°å†™ä¸æ•æ„Ÿçš„åŒ¹é…
            question_match = re.search(r'question\s*:', pair, re.IGNORECASE)
            sql_match = re.search(r'sql\s*:', pair, re.IGNORECASE)
            
            if not question_match or not sql_match:
                print(f" è·³è¿‡ä¸ç¬¦åˆæ ¼å¼çš„å¯¹ #{idx}")
                continue
            
            # ç¡®ä¿SQLåœ¨Questionä¹‹å
            if sql_match.start() <= question_match.end():
                print(f" SQLéƒ¨åˆ†æœªæ‰¾åˆ°ï¼Œè·³è¿‡å¯¹ #{idx}")
                continue
                
            # æå–é—®é¢˜éƒ¨åˆ†
            question_start = question_match.end()
            sql_start = sql_match.start()
            
            question = pair[question_start:sql_start].strip()
            
            # æå–SQLéƒ¨åˆ†ï¼ˆæ”¯æŒå¤šè¡Œï¼‰
            sql_part = pair[sql_match.end():].strip()
            
            # æ£€æŸ¥æ˜¯å¦å­˜åœ¨ä¸‹ä¸€ä¸ªQuestionæ ‡è®°ï¼ˆé˜²æ­¢è§£æé”™è¯¯ï¼‰
            next_question_match = re.search(r'question\s*:', pair[sql_match.end():], re.IGNORECASE)
            if next_question_match:
                sql_part = pair[sql_match.end():sql_match.end() + next_question_match.start()].strip()
            
            if not question or not sql_part:
                print(f" é—®é¢˜æˆ–SQLä¸ºç©ºï¼Œè·³è¿‡å¯¹ #{idx}")
                continue
            
            # è®­ç»ƒé—®ç­”å¯¹
            print(f"\næ ¼å¼åŒ–é—®ç­”è®­ç»ƒ {idx}")
            print(f"é—®é¢˜: {question}")
            print(f"SQL: {sql_part}")
            train_question_sql_pair(question, sql_part)
            successfully_processed += 1
            
        except Exception as e:
            print(f" é”™è¯¯ï¼šæ ¼å¼åŒ–é—®ç­”è®­ç»ƒå¯¹ #{idx} - {e}")
    
    print(f"æ ¼å¼åŒ–é—®ç­”è®­ç»ƒå®Œæˆï¼Œå…±æˆåŠŸå¤„ç† {successfully_processed} å¯¹é—®ç­”ï¼ˆæ€»è®¡ {len(pairs)} å¯¹ï¼‰")

def train_json_question_sql_pairs(json_file):
    """è®­ç»ƒJSONæ ¼å¼çš„é—®ç­”å¯¹
    
    Args:
        json_file (str): JSONæ ¼å¼é—®ç­”å¯¹æ–‡ä»¶è·¯å¾„
    """
    print(f" å¼€å§‹è®­ç»ƒ JSONæ ¼å¼é—®ç­”å¯¹: {json_file}")
    if not os.path.exists(json_file):
        print(f" JSONé—®ç­”æ–‡ä»¶ä¸å­˜åœ¨: {json_file}")
        return
    
    try:
        # è¯»å–JSONæ–‡ä»¶
        with open(json_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        # ç¡®ä¿æ•°æ®æ˜¯åˆ—è¡¨æ ¼å¼
        if not isinstance(data, list):
            print(f" é”™è¯¯: JSONæ–‡ä»¶æ ¼å¼ä¸æ­£ç¡®ï¼Œåº”ä¸ºé—®ç­”å¯¹åˆ—è¡¨")
            return
            
        successfully_processed = 0
        for idx, pair in enumerate(data, start=1):
            try:
                # æ£€æŸ¥é—®ç­”å¯¹æ ¼å¼
                if not isinstance(pair, dict):
                    print(f" è·³è¿‡ä¸ç¬¦åˆæ ¼å¼çš„å¯¹ #{idx}")
                    continue
                
                # å¤§å°å†™ä¸æ•æ„Ÿåœ°æŸ¥æ‰¾questionå’Œsqlé”®
                question_key = None
                sql_key = None
                question_value = None
                sql_value = None
                
                for key, value in pair.items():
                    if key.lower() == "question":
                        question_key = key
                        question_value = value
                    elif key.lower() == "sql":
                        sql_key = key
                        sql_value = value
                
                if question_key is None or sql_key is None:
                    print(f" è·³è¿‡ä¸ç¬¦åˆæ ¼å¼çš„å¯¹ #{idx}")
                    continue
                
                question = str(question_value).strip()
                sql = str(sql_value).strip()
                
                if not question or not sql:
                    print(f" é—®é¢˜æˆ–SQLä¸ºç©ºï¼Œè·³è¿‡å¯¹ #{idx}")
                    continue
                
                # è®­ç»ƒé—®ç­”å¯¹
                print(f"\n JSONæ ¼å¼é—®ç­”è®­ç»ƒ {idx}")
                print(f"é—®é¢˜: {question}")
                print(f"SQL: {sql}")
                train_question_sql_pair(question, sql)
                successfully_processed += 1
                
            except Exception as e:
                print(f" é”™è¯¯ï¼šJSONé—®ç­”è®­ç»ƒå¯¹ #{idx} - {e}")
        
        print(f"JSONæ ¼å¼é—®ç­”è®­ç»ƒå®Œæˆï¼Œå…±æˆåŠŸå¤„ç† {successfully_processed} å¯¹é—®ç­”ï¼ˆæ€»è®¡ {len(data)} å¯¹ï¼‰")
        
    except json.JSONDecodeError as e:
        print(f" é”™è¯¯ï¼šJSONè§£æå¤±è´¥ - {e}")
    except Exception as e:
        print(f" é”™è¯¯ï¼šå¤„ç†JSONé—®ç­”è®­ç»ƒ - {e}")

def process_training_files(data_path, task_id=None, backup_vector_tables=False, truncate_vector_tables=False):
    """å¤„ç†æŒ‡å®šè·¯å¾„ä¸‹çš„æ‰€æœ‰è®­ç»ƒæ–‡ä»¶
    
    Args:
        data_path (str): è®­ç»ƒæ•°æ®ç›®å½•è·¯å¾„
        task_id (str): ä»»åŠ¡IDï¼Œç”¨äºæ—¥å¿—è®°å½•
        backup_vector_tables (bool): æ˜¯å¦å¤‡ä»½vectorè¡¨æ•°æ®
        truncate_vector_tables (bool): æ˜¯å¦æ¸…ç©ºvectorè¡¨æ•°æ®
    """
    # åˆå§‹åŒ–æ—¥å¿—
    if task_id:
        from data_pipeline.dp_logging import get_logger
        logger = get_logger("TrainingDataLoader", task_id)
        logger.info(f"æ‰«æè®­ç»ƒæ•°æ®ç›®å½•: {os.path.abspath(data_path)}")
    else:
        # å…¼å®¹åŸæœ‰è°ƒç”¨æ–¹å¼
        print(f"\n===== æ‰«æè®­ç»ƒæ•°æ®ç›®å½•: {os.path.abspath(data_path)} =====")
        logger = None
    
    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not os.path.exists(data_path):
        error_msg = f"é”™è¯¯: è®­ç»ƒæ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_path}"
        if logger:
            logger.error(error_msg)
        else:
            print(error_msg)
        return False
    
    # æ—¥å¿—è¾“å‡ºè¾…åŠ©å‡½æ•°
    def log_message(message, level="info"):
        if logger:
            getattr(logger, level)(message)
        else:
            print(message)
    
    # Vectorè¡¨ç®¡ç†ï¼ˆå‰ç½®æ­¥éª¤ï¼‰
    vector_stats = None
    if backup_vector_tables or truncate_vector_tables:
        # å‚æ•°éªŒè¯å’Œè‡ªåŠ¨å¯ç”¨é€»è¾‘
        if truncate_vector_tables:
            backup_vector_tables = True
        
        try:
            import asyncio
            from data_pipeline.trainer.vector_table_manager import VectorTableManager
            
            log_message("ğŸ—‚ï¸ å¼€å§‹æ‰§è¡ŒVectorè¡¨ç®¡ç†...")
            
            vector_manager = VectorTableManager(data_path, task_id)
            vector_stats = asyncio.run(vector_manager.execute_vector_management(backup_vector_tables, truncate_vector_tables))
            
            log_message("âœ… Vectorè¡¨ç®¡ç†å®Œæˆ")
            
        except Exception as e:
            log_message(f"âŒ Vectorè¡¨ç®¡ç†å¤±è´¥: {e}", "error")
            return False
    
    # åˆå§‹åŒ–ç»Ÿè®¡è®¡æ•°å™¨
    stats = {
        "ddl": 0,
        "documentation": 0,
        "sql_example": 0,
        "question_sql_formatted": 0,
        "question_sql_json": 0
    }
    
    # åªæ‰«ææŒ‡å®šç›®å½•ä¸‹çš„ç›´æ¥æ–‡ä»¶ï¼Œä¸æ‰«æå­ç›®å½•
    try:
        items = os.listdir(data_path)
        for item in items:
            item_path = os.path.join(data_path, item)
            
            # åªå¤„ç†æ–‡ä»¶ï¼Œè·³è¿‡ç›®å½•
            if not os.path.isfile(item_path):
                log_message(f"è·³è¿‡å­ç›®å½•: {item}")
                continue
                
            file_lower = item.lower()
            
            # æ ¹æ®æ–‡ä»¶ç±»å‹è°ƒç”¨ç›¸åº”çš„å¤„ç†å‡½æ•°
            try:
                if file_lower.endswith(".ddl"):
                    log_message(f"å¤„ç†DDLæ–‡ä»¶: {item_path}")
                    train_ddl_statements(item_path)
                    stats["ddl"] += 1
                    
                elif file_lower.endswith(".md") or file_lower.endswith(".markdown"):
                    log_message(f"å¤„ç†æ–‡æ¡£æ–‡ä»¶: {item_path}")
                    train_documentation_blocks(item_path)
                    stats["documentation"] += 1
                    
                elif file_lower.endswith("_pair.json") or file_lower.endswith("_pairs.json"):
                    log_message(f"å¤„ç†JSONé—®ç­”å¯¹æ–‡ä»¶: {item_path}")
                    train_json_question_sql_pairs(item_path)
                    stats["question_sql_json"] += 1
                    
                elif file_lower.endswith("_pair.sql") or file_lower.endswith("_pairs.sql"):
                    log_message(f"å¤„ç†æ ¼å¼åŒ–é—®ç­”å¯¹æ–‡ä»¶: {item_path}")
                    train_formatted_question_sql_pairs(item_path)
                    stats["question_sql_formatted"] += 1
                    
                elif file_lower.endswith(".sql") and not (file_lower.endswith("_pair.sql") or file_lower.endswith("_pairs.sql")):
                    log_message(f"å¤„ç†SQLç¤ºä¾‹æ–‡ä»¶: {item_path}")
                    train_sql_examples(item_path)
                    stats["sql_example"] += 1
                else:
                    log_message(f"è·³è¿‡ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {item}")
            except Exception as e:
                log_message(f"å¤„ç†æ–‡ä»¶ {item_path} æ—¶å‡ºé”™: {e}", "error")
                
    except OSError as e:
        log_message(f"è¯»å–ç›®å½•å¤±è´¥: {e}", "error")
        return False
    
    # æ‰“å°å¤„ç†ç»Ÿè®¡
    log_message("è®­ç»ƒæ–‡ä»¶å¤„ç†ç»Ÿè®¡:")
    log_message(f"DDLæ–‡ä»¶: {stats['ddl']}ä¸ª")
    log_message(f"æ–‡æ¡£æ–‡ä»¶: {stats['documentation']}ä¸ª")
    log_message(f"SQLç¤ºä¾‹æ–‡ä»¶: {stats['sql_example']}ä¸ª")
    log_message(f"æ ¼å¼åŒ–é—®ç­”å¯¹æ–‡ä»¶: {stats['question_sql_formatted']}ä¸ª")
    log_message(f"JSONé—®ç­”å¯¹æ–‡ä»¶: {stats['question_sql_json']}ä¸ª")
    
    total_files = sum(stats.values())
    if total_files == 0:
        log_message(f"è­¦å‘Š: åœ¨ç›®å½• {data_path} ä¸­æœªæ‰¾åˆ°ä»»ä½•å¯è®­ç»ƒçš„æ–‡ä»¶", "warning")
        return False, vector_stats
        
    return True, vector_stats

def check_pgvector_connection():
    """æ£€æŸ¥ PgVector æ•°æ®åº“è¿æ¥æ˜¯å¦å¯ç”¨
    
    Returns:
        bool: è¿æ¥æˆåŠŸè¿”å›Trueï¼Œå¦åˆ™è¿”å›False
    """
    import app_config
    from sqlalchemy import create_engine, text
    
    try:
        # æ„å»ºè¿æ¥å­—ç¬¦ä¸²
        pg_config = app_config.PGVECTOR_CONFIG
        connection_string = f"postgresql://{pg_config['user']}:{pg_config['password']}@{pg_config['host']}:{pg_config['port']}/{pg_config['dbname']}"
        
        print(f"æ­£åœ¨æµ‹è¯• PgVector æ•°æ®åº“è¿æ¥...")
        print(f"è¿æ¥åœ°å€: {pg_config['host']}:{pg_config['port']}/{pg_config['dbname']}")
        
        # åˆ›å»ºæ•°æ®åº“å¼•æ“å¹¶æµ‹è¯•è¿æ¥
        engine = create_engine(connection_string)
        
        with engine.connect() as connection:
            # æµ‹è¯•åŸºæœ¬è¿æ¥
            result = connection.execute(text("SELECT 1"))
            result.fetchone()
            
            # æ£€æŸ¥æ˜¯å¦å®‰è£…äº† pgvector æ‰©å±•
            try:
                result = connection.execute(text("SELECT extname FROM pg_extension WHERE extname = 'vector'"))
                extension_exists = result.fetchone() is not None
                
                if extension_exists:
                    print("âœ“ PgVector æ‰©å±•å·²å®‰è£…")
                else:
                    print("âš  è­¦å‘Š: PgVector æ‰©å±•æœªå®‰è£…ï¼Œè¯·ç¡®ä¿å·²å®‰è£… pgvector æ‰©å±•")
                    
            except Exception as ext_e:
                print(f"âš  æ— æ³•æ£€æŸ¥ pgvector æ‰©å±•çŠ¶æ€: {ext_e}")
            
            # æ£€æŸ¥è®­ç»ƒæ•°æ®è¡¨æ˜¯å¦å­˜åœ¨
            try:
                result = connection.execute(text("SELECT tablename FROM pg_tables WHERE tablename = 'langchain_pg_embedding'"))
                table_exists = result.fetchone() is not None
                
                if table_exists:
                    # è·å–è¡¨ä¸­çš„è®°å½•æ•°
                    result = connection.execute(text("SELECT COUNT(*) FROM langchain_pg_embedding"))
                    count = result.fetchone()[0]
                    print(f"âœ“ è®­ç»ƒæ•°æ®è¡¨å­˜åœ¨ï¼Œå½“å‰åŒ…å« {count} æ¡è®°å½•")
                else:
                    print("â„¹ è®­ç»ƒæ•°æ®è¡¨å°šæœªåˆ›å»ºï¼ˆé¦–æ¬¡è®­ç»ƒæ—¶ä¼šè‡ªåŠ¨åˆ›å»ºï¼‰")
                    
            except Exception as table_e:
                print(f"âš  æ— æ³•æ£€æŸ¥è®­ç»ƒæ•°æ®è¡¨çŠ¶æ€: {table_e}")
        
        print("âœ“ PgVector æ•°æ®åº“è¿æ¥æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âœ— PgVector æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return False

def main():
    """ä¸»å‡½æ•°ï¼šé…ç½®å’Œè¿è¡Œè®­ç»ƒæµç¨‹"""
    
    # å…ˆå¯¼å…¥æ‰€éœ€æ¨¡å—
    import os
    import app_config
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description='è®­ç»ƒVanna NL2SQLæ¨¡å‹')
    
    # è·å–é»˜è®¤è·¯å¾„å¹¶è¿›è¡Œæ™ºèƒ½å¤„ç†
    def resolve_training_data_path():
        """æ™ºèƒ½è§£æè®­ç»ƒæ•°æ®è·¯å¾„"""
        # ä½¿ç”¨data_pipelineç»Ÿä¸€é…ç½®
        try:
            from data_pipeline.config import SCHEMA_TOOLS_CONFIG
            config_path = SCHEMA_TOOLS_CONFIG.get("output_directory", './data_pipeline/training_data/')
        except ImportError:
            # å¦‚æœæ— æ³•å¯¼å…¥data_pipelineé…ç½®ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
            config_path = './data_pipeline/training_data/'
        
        # å¦‚æœæ˜¯ç»å¯¹è·¯å¾„ï¼Œç›´æ¥è¿”å›
        if os.path.isabs(config_path):
            return config_path
        
        # å¦‚æœä»¥ . å¼€å¤´ï¼Œç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•è§£æ
        if config_path.startswith('./') or config_path.startswith('../'):
            project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            return os.path.join(project_root, config_path)
        
        # å…¶ä»–æƒ…å†µï¼Œç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        return os.path.join(project_root, config_path)
    
    default_path = resolve_training_data_path()
    
    parser.add_argument('--data_path', type=str, default=default_path,
                        help='è®­ç»ƒæ•°æ®ç›®å½•è·¯å¾„ (é»˜è®¤: ä»data_pipeline.config.SCHEMA_TOOLS_CONFIG)')
    
    parser.add_argument('--backup-vector-tables', action='store_true',
                        help='å¤‡ä»½vectorè¡¨æ•°æ®')
    
    parser.add_argument('--truncate-vector-tables', action='store_true',
                        help='æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰')
    
    args = parser.parse_args()
    
    # ä½¿ç”¨Pathå¯¹è±¡å¤„ç†è·¯å¾„ä»¥ç¡®ä¿è·¨å¹³å°å…¼å®¹æ€§
    data_path = Path(args.data_path)
    
    # æ˜¾ç¤ºè·¯å¾„è§£æç»“æœ
    print(f"\n===== è®­ç»ƒæ•°æ®è·¯å¾„é…ç½® =====")
    try:
        from data_pipeline.config import SCHEMA_TOOLS_CONFIG
        config_value = SCHEMA_TOOLS_CONFIG.get("output_directory", "æœªé…ç½®")
        print(f"data_pipelineé…ç½®è·¯å¾„: {config_value}")
    except ImportError:
        print(f"data_pipelineé…ç½®: æ— æ³•å¯¼å…¥")
    print(f"è§£æåçš„ç»å¯¹è·¯å¾„: {os.path.abspath(data_path)}")
    print("==============================")
    
    # è®¾ç½®æ­£ç¡®çš„é¡¹ç›®æ ¹ç›®å½•è·¯å¾„
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

    # æ£€æŸ¥åµŒå…¥æ¨¡å‹è¿æ¥
    check_embedding_model_connection()
    
    # æ ¹æ®é…ç½®çš„å‘é‡æ•°æ®åº“ç±»å‹æ˜¾ç¤ºç›¸åº”ä¿¡æ¯
    vector_db_type = app_config.VECTOR_DB_TYPE.lower()
    
    if vector_db_type == "chromadb":
        # æ‰“å°ChromaDBç›¸å…³ä¿¡æ¯
        try:
            try:
                import chromadb
                chroma_version = chromadb.__version__
            except ImportError:
                chroma_version = "æœªçŸ¥"
            
            # å°è¯•æŸ¥çœ‹å½“å‰ä½¿ç”¨çš„ChromaDBæ–‡ä»¶
            chroma_file = "chroma.sqlite3"  # é»˜è®¤æ–‡ä»¶å
            
            # ä½¿ç”¨é¡¹ç›®æ ¹ç›®å½•ä½œä¸ºChromaDBæ–‡ä»¶è·¯å¾„
            db_file_path = os.path.join(project_root, chroma_file)

            if os.path.exists(db_file_path):
                file_size = os.path.getsize(db_file_path) / 1024  # KB
                print(f"\n===== ChromaDBæ•°æ®åº“: {os.path.abspath(db_file_path)} (å¤§å°: {file_size:.2f} KB) =====")
            else:
                print(f"\n===== æœªæ‰¾åˆ°ChromaDBæ•°æ®åº“æ–‡ä»¶äº: {os.path.abspath(db_file_path)} =====")
                
            # æ‰“å°ChromaDBç‰ˆæœ¬
            print(f"===== ChromaDBå®¢æˆ·ç«¯åº“ç‰ˆæœ¬: {chroma_version} =====\n")
        except Exception as e:
            print(f"\n===== æ— æ³•è·å–ChromaDBä¿¡æ¯: {e} =====\n")
            
    elif vector_db_type == "pgvector":
        # æ‰“å°PgVectorç›¸å…³ä¿¡æ¯å¹¶æµ‹è¯•è¿æ¥
        print(f"\n===== PgVectoræ•°æ®åº“é…ç½® =====")
        pg_config = app_config.PGVECTOR_CONFIG
        print(f"æ•°æ®åº“åœ°å€: {pg_config['host']}:{pg_config['port']}")
        print(f"æ•°æ®åº“åç§°: {pg_config['dbname']}")
        print(f"ç”¨æˆ·å: {pg_config['user']}")
        print("==============================\n")
        
        # æµ‹è¯•PgVectorè¿æ¥
        if not check_pgvector_connection():
            print("PgVector æ•°æ®åº“è¿æ¥å¤±è´¥ï¼Œè®­ç»ƒè¿‡ç¨‹ç»ˆæ­¢ã€‚")
            sys.exit(1)
    else:
        print(f"\n===== æœªçŸ¥çš„å‘é‡æ•°æ®åº“ç±»å‹: {vector_db_type} =====\n")
    
    # å¤„ç†è®­ç»ƒæ–‡ä»¶
    process_successful, vector_stats = process_training_files(data_path, None, 
                                                             args.backup_vector_tables, 
                                                             args.truncate_vector_tables)
    
    if process_successful:
        # è®­ç»ƒç»“æŸï¼Œåˆ·æ–°å’Œå…³é—­æ‰¹å¤„ç†å™¨
        print("\n===== è®­ç»ƒå®Œæˆï¼Œå¤„ç†å‰©ä½™æ‰¹æ¬¡ =====")
        flush_training()
        shutdown_trainer()
        
        # éªŒè¯æ•°æ®æ˜¯å¦æˆåŠŸå†™å…¥
        print("\n===== éªŒè¯è®­ç»ƒæ•°æ® =====")
        from core.vanna_llm_factory import create_vanna_instance
        vn = create_vanna_instance()
        
        # æ ¹æ®å‘é‡æ•°æ®åº“ç±»å‹æ‰§è¡Œä¸åŒçš„éªŒè¯é€»è¾‘
        try:
            training_data = vn.get_training_data()
            if training_data is not None and not training_data.empty:
                print(f"âœ“ å·²ä»{vector_db_type.upper()}ä¸­æ£€ç´¢åˆ° {len(training_data)} æ¡è®­ç»ƒæ•°æ®è¿›è¡ŒéªŒè¯ã€‚")
                
                # æ˜¾ç¤ºè®­ç»ƒæ•°æ®ç±»å‹ç»Ÿè®¡
                if 'training_data_type' in training_data.columns:
                    type_counts = training_data['training_data_type'].value_counts()
                    print("è®­ç»ƒæ•°æ®ç±»å‹ç»Ÿè®¡:")
                    for data_type, count in type_counts.items():
                        print(f"  {data_type}: {count} æ¡")
                        
            elif training_data is not None and training_data.empty:
                print(f"âš  åœ¨{vector_db_type.upper()}ä¸­æœªæ‰¾åˆ°ä»»ä½•è®­ç»ƒæ•°æ®ã€‚")
            else: # training_data is None
                print(f"âš  æ— æ³•ä»Vannaè·å–è®­ç»ƒæ•°æ® (å¯èƒ½è¿”å›äº†None)ã€‚è¯·æ£€æŸ¥{vector_db_type.upper()}è¿æ¥å’ŒVannaå®ç°ã€‚")

        except Exception as e:
            print(f"âœ— éªŒè¯è®­ç»ƒæ•°æ®å¤±è´¥: {e}")
            print(f"è¯·æ£€æŸ¥{vector_db_type.upper()}è¿æ¥å’Œè¡¨ç»“æ„ã€‚")
    else:
        print("\n===== æœªèƒ½æ‰¾åˆ°æˆ–å¤„ç†ä»»ä½•è®­ç»ƒæ–‡ä»¶ï¼Œè®­ç»ƒè¿‡ç¨‹ç»ˆæ­¢ =====")
    
    # Vectorè¡¨ç®¡ç†æ€»ç»“
    print("\n===== Vectorè¡¨ç®¡ç†ç»Ÿè®¡ =====")
    if vector_stats:
        if vector_stats.get("backup_performed", False):
            tables_info = vector_stats.get("tables_backed_up", {})
            print(f"âœ“ å¤‡ä»½æ‰§è¡Œ: æˆåŠŸå¤‡ä»½ {len(tables_info)} ä¸ªè¡¨")
            for table_name, info in tables_info.items():
                if info.get("success", False):
                    print(f"  - {table_name}: {info['row_count']}è¡Œ -> {info['backup_file']} ({info['file_size']})")
                else:
                    print(f"  - {table_name}: å¤‡ä»½å¤±è´¥ - {info.get('error', 'æœªçŸ¥é”™è¯¯')}")
        else:
            print("- å¤‡ä»½æ‰§è¡Œ: æœªæ‰§è¡Œ")
            
        if vector_stats.get("truncate_performed", False):
            truncate_info = vector_stats.get("truncate_results", {})
            print("âœ“ æ¸…ç©ºæ‰§è¡Œ: langchain_pg_embeddingè¡¨å·²æ¸…ç©º")
            for table_name, info in truncate_info.items():
                if info.get("success", False):
                    print(f"  - {table_name}: {info['rows_before']}è¡Œ -> 0è¡Œ")
                else:
                    print(f"  - {table_name}: æ¸…ç©ºå¤±è´¥ - {info.get('error', 'æœªçŸ¥é”™è¯¯')}")
        else:
            print("- æ¸…ç©ºæ‰§è¡Œ: æœªæ‰§è¡Œ")
            
        print(f"âœ“ æ€»è€—æ—¶: {vector_stats.get('duration', 0):.1f}ç§’")
        
        if vector_stats.get("errors"):
            print(f"âš  é”™è¯¯: {'; '.join(vector_stats['errors'])}")
    else:
        print("- æœªæ‰§è¡Œvectorè¡¨ç®¡ç†æ“ä½œ")
    print("===========================")
    
    # è¾“å‡ºembeddingæ¨¡å‹ä¿¡æ¯
    print("\n===== Embeddingæ¨¡å‹ä¿¡æ¯ =====")
    try:
        from common.utils import get_current_embedding_config, get_current_model_info
        
        embedding_config = get_current_embedding_config()
        model_info = get_current_model_info()
        
        print(f"æ¨¡å‹ç±»å‹: {model_info['embedding_type']}")
        print(f"æ¨¡å‹åç§°: {model_info['embedding_model']}")
        print(f"å‘é‡ç»´åº¦: {embedding_config.get('embedding_dimension', 'æœªçŸ¥')}")
        if 'base_url' in embedding_config:
            print(f"APIæœåŠ¡: {embedding_config['base_url']}")
    except ImportError as e:
        print(f"è­¦å‘Š: æ— æ³•å¯¼å…¥é…ç½®å·¥å…·å‡½æ•°: {e}")
        # å›é€€åˆ°æ—§çš„é…ç½®è®¿é—®æ–¹å¼
        embedding_config = getattr(app_config, 'API_EMBEDDING_CONFIG', {})
        print(f"æ¨¡å‹åç§°: {embedding_config.get('model_name', 'æœªçŸ¥')}")
        print(f"å‘é‡ç»´åº¦: {embedding_config.get('embedding_dimension', 'æœªçŸ¥')}")
        print(f"APIæœåŠ¡: {embedding_config.get('base_url', 'æœªçŸ¥')}")
    
    # æ ¹æ®é…ç½®æ˜¾ç¤ºå‘é‡æ•°æ®åº“ä¿¡æ¯
    if vector_db_type == "chromadb":
        chroma_display_path = os.path.abspath(project_root)
        print(f"å‘é‡æ•°æ®åº“: ChromaDB ({chroma_display_path})")
    elif vector_db_type == "pgvector":
        pg_config = app_config.PGVECTOR_CONFIG
        print(f"å‘é‡æ•°æ®åº“: PgVector ({pg_config['host']}:{pg_config['port']}/{pg_config['dbname']})")
    
    print("===== è®­ç»ƒæµç¨‹å®Œæˆ =====\n")

if __name__ == "__main__":
    main() 