"""
Schemaå·¥ä½œæµç¼–æ’å™¨
ç»Ÿä¸€ç®¡ç†å®Œæ•´çš„æ•°æ®åº“Schemaå¤„ç†æµç¨‹
"""

import asyncio
import time
import logging
from typing import Dict, Any, List, Optional
from pathlib import Path
from datetime import datetime

from data_pipeline.ddl_generation.training_data_agent import SchemaTrainingDataAgent
from data_pipeline.qa_generation.qs_agent import QuestionSQLGenerationAgent
from data_pipeline.validators.sql_validation_agent import SQLValidationAgent
from data_pipeline.config import SCHEMA_TOOLS_CONFIG
from data_pipeline.dp_logging import get_logger


class SchemaWorkflowOrchestrator:
    """ç«¯åˆ°ç«¯çš„Schemaå¤„ç†ç¼–æ’å™¨ - å®Œæ•´å·¥ä½œæµç¨‹"""
    
    def __init__(self, 
                 db_connection: str,
                 table_list_file: str,
                 business_context: str,
                 output_dir: str = None,
                 task_id: str = None,
                 enable_sql_validation: bool = True,
                 enable_llm_repair: bool = True,
                 modify_original_file: bool = True,
                 enable_training_data_load: bool = True):
        """
        åˆå§‹åŒ–Schemaå·¥ä½œæµç¼–æ’å™¨
        
        Args:
            db_connection: æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸² (postgresql://user:pass@host:port/dbname)
            table_list_file: è¡¨æ¸…å•æ–‡ä»¶è·¯å¾„
            business_context: ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿°
            output_dir: è¾“å‡ºç›®å½•
            task_id: ä»»åŠ¡ID (APIæ¨¡å¼ä¼ é€’ï¼Œè„šæœ¬æ¨¡å¼è‡ªåŠ¨ç”Ÿæˆ)
            enable_sql_validation: æ˜¯å¦å¯ç”¨SQLéªŒè¯
            enable_llm_repair: æ˜¯å¦å¯ç”¨LLMä¿®å¤åŠŸèƒ½
            modify_original_file: æ˜¯å¦ä¿®æ”¹åŸå§‹JSONæ–‡ä»¶
            enable_training_data_load: æ˜¯å¦å¯ç”¨è®­ç»ƒæ•°æ®åŠ è½½
        """
        self.db_connection = db_connection
        self.table_list_file = table_list_file
        self.business_context = business_context
        self.db_name = self._extract_db_name_from_connection(db_connection)
        self.enable_sql_validation = enable_sql_validation
        self.enable_llm_repair = enable_llm_repair
        self.modify_original_file = modify_original_file
        self.enable_training_data_load = enable_training_data_load
        
        # å¤„ç†task_id
        if task_id is None:
            # è„šæœ¬æ¨¡å¼ï¼šè‡ªåŠ¨ç”Ÿæˆmanualå¼€å¤´çš„task_id
            self.task_id = f"manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        else:
            # APIæ¨¡å¼ï¼šä½¿ç”¨ä¼ é€’çš„task_id
            self.task_id = task_id
        
        # è®¾ç½®è¾“å‡ºç›®å½•
        if output_dir is None:
            # è„šæœ¬æ¨¡å¼æˆ–æœªæŒ‡å®šè¾“å‡ºç›®å½•æ—¶ï¼Œä½¿ç”¨ä»»åŠ¡ç›®å½•
            # è·å–é¡¹ç›®æ ¹ç›®å½•çš„ç»å¯¹è·¯å¾„
            project_root = Path(__file__).parent.parent
            self.output_dir = project_root / "data_pipeline" / "training_data" / self.task_id
        else:
            # APIæ¨¡å¼æˆ–æ˜ç¡®æŒ‡å®šè¾“å‡ºç›®å½•æ—¶ï¼Œä½¿ç”¨æŒ‡å®šçš„ç›®å½•
            self.output_dir = Path(output_dir)
            
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
            
        # åˆå§‹åŒ–ç‹¬ç«‹æ—¥å¿—ç³»ç»Ÿ
        self.logger = get_logger("SchemaWorkflowOrchestrator", self.task_id)
        
        # å·¥ä½œæµç¨‹çŠ¶æ€
        self.workflow_state = {
            "start_time": None,
            "end_time": None,
            "current_step": None,
            "completed_steps": [],
            "failed_steps": [],
            "artifacts": {},  # å­˜å‚¨å„æ­¥éª¤äº§ç”Ÿçš„æ–‡ä»¶
            "statistics": {}
        }
    
    def _extract_db_name_from_connection(self, connection_string: str) -> str:
        """
        ä»æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸²ä¸­æå–æ•°æ®åº“åç§°
        
        Args:
            connection_string: PostgreSQLè¿æ¥å­—ç¬¦ä¸²
            
        Returns:
            str: æ•°æ®åº“åç§°
        """
        try:
            # å¤„ç†æ ‡å‡†çš„PostgreSQLè¿æ¥å­—ç¬¦ä¸²: postgresql://user:pass@host:port/dbname
            if '/' in connection_string:
                # å–æœ€åä¸€ä¸ª '/' åé¢çš„éƒ¨åˆ†ä½œä¸ºæ•°æ®åº“å
                db_name = connection_string.split('/')[-1]
                # ç§»é™¤å¯èƒ½çš„æŸ¥è¯¢å‚æ•°
                if '?' in db_name:
                    db_name = db_name.split('?')[0]
                return db_name if db_name else "database"
            else:
                return "database"
        except Exception:
            return "database"
    
    async def execute_complete_workflow(self) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„Schemaå¤„ç†å·¥ä½œæµç¨‹
        
        Returns:
            å®Œæ•´çš„å·¥ä½œæµç¨‹æŠ¥å‘Š
        """
        self.workflow_state["start_time"] = time.time()
        self.logger.info("ğŸš€ å¼€å§‹æ‰§è¡ŒSchemaå·¥ä½œæµç¼–æ’")
        self.logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {self.output_dir}")
        self.logger.info(f"ğŸ¢ ä¸šåŠ¡èƒŒæ™¯: {self.business_context}")
        self.logger.info(f"ğŸ’¾ æ•°æ®åº“: {self.db_name}")
        
        try:
            # æ­¥éª¤1: ç”ŸæˆDDLå’ŒMDæ–‡ä»¶
            await self._execute_step_1_ddl_md_generation()
            
            # æ­¥éª¤2: ç”ŸæˆQuestion-SQLå¯¹
            await self._execute_step_2_question_sql_generation()
            
            # æ­¥éª¤3: éªŒè¯å’Œä¿®æ­£SQLï¼ˆå¯é€‰ï¼‰
            if self.enable_sql_validation:
                await self._execute_step_3_sql_validation()
            else:
                self.logger.info("â­ï¸ è·³è¿‡SQLéªŒè¯æ­¥éª¤")
            
            # æ­¥éª¤4: è®­ç»ƒæ•°æ®åŠ è½½ï¼ˆå¯é€‰ï¼‰
            if self.enable_training_data_load:
                await self._execute_step_4_training_data_load()
            else:
                self.logger.info("â­ï¸ è·³è¿‡è®­ç»ƒæ•°æ®åŠ è½½æ­¥éª¤")
            
            # è®¾ç½®ç»“æŸæ—¶é—´
            self.workflow_state["end_time"] = time.time()
            
            # ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
            final_report = await self._generate_final_report()
            
            self.logger.info("âœ… Schemaå·¥ä½œæµç¼–æ’å®Œæˆ")
            
            return final_report
            
        except Exception as e:
            self.workflow_state["end_time"] = time.time()
            self.logger.exception(f"âŒ å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥: {str(e)}")
            
            error_report = await self._generate_error_report(e)
            return error_report
    
    async def _execute_step_1_ddl_md_generation(self):
        """æ­¥éª¤1: ç”ŸæˆDDLå’ŒMDæ–‡ä»¶"""
        self.workflow_state["current_step"] = "ddl_md_generation"
        self.logger.info("=" * 60)
        self.logger.info("ğŸ“ æ­¥éª¤1: å¼€å§‹ç”ŸæˆDDLå’ŒMDæ–‡ä»¶")
        self.logger.info("=" * 60)
        
        step_start_time = time.time()
        
        try:
            # åˆ›å»ºDDL/MDç”ŸæˆAgent
            ddl_md_agent = SchemaTrainingDataAgent(
                db_connection=self.db_connection,
                table_list_file=self.table_list_file,
                business_context=self.business_context,
                output_dir=str(self.output_dir),
                task_id=self.task_id,  # ä¼ é€’task_id
                pipeline="full"
            )
            
            # æ‰§è¡ŒDDL/MDç”Ÿæˆ
            ddl_md_result = await ddl_md_agent.generate_training_data()
            
            step_duration = time.time() - step_start_time
            
            # è®°å½•ç»“æœ
            self.workflow_state["completed_steps"].append("ddl_md_generation")
            self.workflow_state["artifacts"]["ddl_md_generation"] = {
                "total_tables": ddl_md_result.get("summary", {}).get("total_tables", 0),
                "processed_successfully": ddl_md_result.get("summary", {}).get("processed_successfully", 0),
                "failed": ddl_md_result.get("summary", {}).get("failed", 0),
                "files_generated": ddl_md_result.get("statistics", {}).get("files_generated", 0),
                "duration": step_duration
            }
            self.workflow_state["statistics"]["step1_duration"] = step_duration
            
            processed_tables = ddl_md_result.get("summary", {}).get("processed_successfully", 0)
            self.logger.info(f"âœ… æ­¥éª¤1å®Œæˆ: æˆåŠŸå¤„ç† {processed_tables} ä¸ªè¡¨ï¼Œè€—æ—¶ {step_duration:.2f}ç§’")
            
        except Exception as e:
            self.workflow_state["failed_steps"].append("ddl_md_generation")
            self.logger.error(f"âŒ æ­¥éª¤1å¤±è´¥: {str(e)}")
            raise
    
    async def _execute_step_2_question_sql_generation(self):
        """æ­¥éª¤2: ç”ŸæˆQuestion-SQLå¯¹"""
        self.workflow_state["current_step"] = "question_sql_generation"
        self.logger.info("=" * 60)
        self.logger.info("ğŸ¤– æ­¥éª¤2: å¼€å§‹ç”ŸæˆQuestion-SQLå¯¹")
        self.logger.info("=" * 60)
        
        step_start_time = time.time()
        
        try:
            # åˆ›å»ºQuestion-SQLç”ŸæˆAgent
            qs_agent = QuestionSQLGenerationAgent(
                output_dir=str(self.output_dir),
                table_list_file=self.table_list_file,
                business_context=self.business_context,
                db_name=self.db_name,
                task_id=self.task_id  # ä¼ é€’task_id
            )
            
            # æ‰§è¡ŒQuestion-SQLç”Ÿæˆ
            qs_result = await qs_agent.generate()
            
            step_duration = time.time() - step_start_time
            
            # è®°å½•ç»“æœ
            self.workflow_state["completed_steps"].append("question_sql_generation")
            self.workflow_state["artifacts"]["question_sql_generation"] = {
                "output_file": str(qs_result.get("output_file", "")),
                "total_questions": qs_result.get("total_questions", 0),
                "total_themes": qs_result.get("total_themes", 0),
                "successful_themes": qs_result.get("successful_themes", 0),
                "failed_themes": qs_result.get("failed_themes", []),
                "duration": step_duration
            }
            self.workflow_state["statistics"]["step2_duration"] = step_duration
            
            total_questions = qs_result.get("total_questions", 0)
            self.logger.info(f"âœ… æ­¥éª¤2å®Œæˆ: ç”Ÿæˆäº† {total_questions} ä¸ªé—®ç­”å¯¹ï¼Œè€—æ—¶ {step_duration:.2f}ç§’")
            
        except Exception as e:
            self.workflow_state["failed_steps"].append("question_sql_generation")
            self.logger.error(f"âŒ æ­¥éª¤2å¤±è´¥: {str(e)}")
            raise
    
    async def _execute_step_3_sql_validation(self):
        """æ­¥éª¤3: éªŒè¯å’Œä¿®æ­£SQL"""
        self.workflow_state["current_step"] = "sql_validation"
        self.logger.info("=" * 60)
        self.logger.info("ğŸ” æ­¥éª¤3: å¼€å§‹éªŒè¯å’Œä¿®æ­£SQL")
        self.logger.info("=" * 60)
        
        step_start_time = time.time()
        
        try:
            # è·å–æ­¥éª¤2ç”Ÿæˆçš„æ–‡ä»¶
            qs_artifacts = self.workflow_state["artifacts"].get("question_sql_generation", {})
            qs_file = qs_artifacts.get("output_file")
            
            if not qs_file or not Path(qs_file).exists():
                raise FileNotFoundError(f"æ‰¾ä¸åˆ°Question-SQLæ–‡ä»¶: {qs_file}")
            
            self.logger.info(f"ğŸ“„ éªŒè¯æ–‡ä»¶: {qs_file}")
            
            # åˆ›å»ºSQLéªŒè¯Agentï¼Œé€šè¿‡å‚æ•°ä¼ é€’é…ç½®è€Œéä¿®æ”¹å…¨å±€é…ç½®
            sql_validator = SQLValidationAgent(
                db_connection=self.db_connection,
                input_file=str(qs_file),
                output_dir=str(self.output_dir),
                task_id=self.task_id,  # ä¼ é€’task_id
                enable_sql_repair=self.enable_llm_repair,
                modify_original_file=self.modify_original_file
            )
            
            # æ‰§è¡ŒSQLéªŒè¯å’Œä¿®æ­£
            validation_result = await sql_validator.validate()
            
            step_duration = time.time() - step_start_time
            
            # è®°å½•ç»“æœ
            self.workflow_state["completed_steps"].append("sql_validation")
            
            summary = validation_result.get("summary", {})
            self.workflow_state["artifacts"]["sql_validation"] = {
                "original_sql_count": summary.get("total_questions", 0),
                "valid_sql_count": summary.get("valid_sqls", 0),
                "invalid_sql_count": summary.get("invalid_sqls", 0),
                "success_rate": summary.get("success_rate", 0),
                "repair_stats": summary.get("repair_stats", {}),
                "file_modification_stats": summary.get("file_modification_stats", {}),
                "average_execution_time": summary.get("average_execution_time", 0),
                "total_retries": summary.get("total_retries", 0),
                "duration": step_duration
            }
            self.workflow_state["statistics"]["step3_duration"] = step_duration
            
            success_rate = summary.get("success_rate", 0)
            valid_count = summary.get("valid_sqls", 0)
            total_count = summary.get("total_questions", 0)
            
            self.logger.info(f"âœ… æ­¥éª¤3å®Œæˆ: SQLéªŒè¯æˆåŠŸç‡ {success_rate:.1%} ({valid_count}/{total_count})ï¼Œè€—æ—¶ {step_duration:.2f}ç§’")
            
            # æ˜¾ç¤ºä¿®å¤ç»Ÿè®¡
            repair_stats = summary.get("repair_stats", {})
            if repair_stats.get("attempted", 0) > 0:
                self.logger.info(f"ğŸ”§ ä¿®å¤ç»Ÿè®¡: å°è¯• {repair_stats['attempted']}ï¼ŒæˆåŠŸ {repair_stats['successful']}ï¼Œå¤±è´¥ {repair_stats['failed']}")
            
            # æ˜¾ç¤ºæ–‡ä»¶ä¿®æ”¹ç»Ÿè®¡
            file_stats = summary.get("file_modification_stats", {})
            if file_stats.get("modified", 0) > 0 or file_stats.get("deleted", 0) > 0:
                self.logger.info(f"ğŸ“ æ–‡ä»¶ä¿®æ”¹: æ›´æ–° {file_stats.get('modified', 0)} ä¸ªSQLï¼Œåˆ é™¤ {file_stats.get('deleted', 0)} ä¸ªæ— æ•ˆé¡¹")
            
        except Exception as e:
            self.workflow_state["failed_steps"].append("sql_validation")
            self.logger.error(f"âŒ æ­¥éª¤3å¤±è´¥: {str(e)}")
            raise
    
    async def _execute_step_4_training_data_load(self):
        """æ­¥éª¤4: è®­ç»ƒæ•°æ®åŠ è½½"""
        self.workflow_state["current_step"] = "training_data_load"
        self.logger.info("=" * 60)
        self.logger.info("ğŸ¯ æ­¥éª¤4: å¼€å§‹åŠ è½½è®­ç»ƒæ•°æ®")
        self.logger.info("=" * 60)
        
        step_start_time = time.time()
        
        try:
            # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨æ‰€éœ€çš„è®­ç»ƒæ•°æ®
            training_data_dir = str(self.output_dir)
            self.logger.info(f"ğŸ“ è®­ç»ƒæ•°æ®ç›®å½•: {training_data_dir}")
            
            # å¯¼å…¥è®­ç»ƒå™¨æ¨¡å—
            import sys
            import os
            sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
            
            from data_pipeline.trainer.run_training import process_training_files
            
            # æ‰§è¡Œè®­ç»ƒæ•°æ®åŠ è½½
            self.logger.info("ğŸ”„ å¼€å§‹å¤„ç†è®­ç»ƒæ–‡ä»¶...")
            load_successful = process_training_files(training_data_dir, self.task_id)
            
            step_duration = time.time() - step_start_time
            
            if load_successful:
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                from data_pipeline.trainer.vanna_trainer import flush_training, shutdown_trainer
                
                # åˆ·æ–°æ‰¹å¤„ç†å™¨
                self.logger.info("ğŸ”„ åˆ·æ–°æ‰¹å¤„ç†å™¨...")
                flush_training()
                shutdown_trainer()
                
                # éªŒè¯åŠ è½½ç»“æœ
                try:
                    from core.vanna_llm_factory import create_vanna_instance
                    vn = create_vanna_instance()
                    training_data = vn.get_training_data()
                    
                    if training_data is not None and not training_data.empty:
                        total_records = len(training_data)
                        self.logger.info(f"âœ… æˆåŠŸåŠ è½½ {total_records} æ¡è®­ç»ƒæ•°æ®")
                        
                        # ç»Ÿè®¡æ•°æ®ç±»å‹
                        if 'training_data_type' in training_data.columns:
                            type_counts = training_data['training_data_type'].value_counts().to_dict()
                        else:
                            type_counts = {}
                    else:
                        total_records = 0
                        type_counts = {}
                        self.logger.warning("âš ï¸ æœªèƒ½éªŒè¯è®­ç»ƒæ•°æ®åŠ è½½ç»“æœ")
                        
                except Exception as e:
                    self.logger.warning(f"âš ï¸ éªŒè¯è®­ç»ƒæ•°æ®æ—¶å‡ºé”™: {e}")
                    total_records = 0
                    type_counts = {}
                
                # è®°å½•ç»“æœ
                self.workflow_state["completed_steps"].append("training_data_load")
                self.workflow_state["artifacts"]["training_data_load"] = {
                    "training_data_dir": training_data_dir,
                    "load_successful": True,
                    "total_records": total_records,
                    "data_type_counts": type_counts,
                    "duration": step_duration
                }
                self.workflow_state["statistics"]["step4_duration"] = step_duration
                
                self.logger.info(f"âœ… æ­¥éª¤4å®Œæˆ: æˆåŠŸåŠ è½½è®­ç»ƒæ•°æ®ï¼Œè€—æ—¶ {step_duration:.2f}ç§’")
                
            else:
                raise Exception("è®­ç»ƒæ•°æ®åŠ è½½å¤±è´¥ï¼šæœªæ‰¾åˆ°å¯å¤„ç†çš„è®­ç»ƒæ–‡ä»¶")
                
        except Exception as e:
            self.workflow_state["failed_steps"].append("training_data_load")
            self.logger.error(f"âŒ æ­¥éª¤4å¤±è´¥: {str(e)}")
            raise
    
    async def _generate_final_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆå·¥ä½œæµç¨‹æŠ¥å‘Š"""
        total_duration = self.workflow_state["end_time"] - self.workflow_state["start_time"]
        
        # è®¡ç®—æœ€ç»ˆè¾“å‡ºæ–‡ä»¶
        qs_artifacts = self.workflow_state["artifacts"].get("question_sql_generation", {})
        final_output_file = qs_artifacts.get("output_file", "")
        
        # è®¡ç®—æœ€ç»ˆé—®é¢˜æ•°é‡
        if "sql_validation" in self.workflow_state["artifacts"]:
            # å¦‚æœæœ‰éªŒè¯æ­¥éª¤ï¼Œä½¿ç”¨éªŒè¯åçš„æ•°é‡
            validation_artifacts = self.workflow_state["artifacts"]["sql_validation"]
            final_question_count = validation_artifacts.get("valid_sql_count", 0)
        else:
            # å¦åˆ™ä½¿ç”¨ç”Ÿæˆçš„æ•°é‡
            final_question_count = qs_artifacts.get("total_questions", 0)
        
        report = {
            "success": True,
            "workflow_summary": {
                "total_duration": round(total_duration, 2),
                "completed_steps": self.workflow_state["completed_steps"],
                "failed_steps": self.workflow_state["failed_steps"],
                "total_steps": len(self.workflow_state["completed_steps"]),
                "workflow_started": datetime.fromtimestamp(self.workflow_state["start_time"]).isoformat(),
                "workflow_completed": datetime.fromtimestamp(self.workflow_state["end_time"]).isoformat()
            },
            "input_parameters": {
                "db_connection": self._mask_connection_string(self.db_connection),
                "table_list_file": self.table_list_file,
                "business_context": self.business_context,
                "db_name": self.db_name,
                "output_directory": str(self.output_dir),
                "enable_sql_validation": self.enable_sql_validation,
                "enable_llm_repair": self.enable_llm_repair,
                "modify_original_file": self.modify_original_file,
                "enable_training_data_load": self.enable_training_data_load
            },
            "processing_results": {
                "ddl_md_generation": self.workflow_state["artifacts"].get("ddl_md_generation", {}),
                "question_sql_generation": self.workflow_state["artifacts"].get("question_sql_generation", {}),
                "sql_validation": self.workflow_state["artifacts"].get("sql_validation", {}),
                "training_data_load": self.workflow_state["artifacts"].get("training_data_load", {})
            },
            "final_outputs": {
                "primary_output_file": final_output_file,
                "output_directory": str(self.output_dir),
                "final_question_count": final_question_count,
                "backup_files_created": self.modify_original_file
            },
            "performance_metrics": {
                "step1_duration": round(self.workflow_state["statistics"].get("step1_duration", 0), 2),
                "step2_duration": round(self.workflow_state["statistics"].get("step2_duration", 0), 2),
                "step3_duration": round(self.workflow_state["statistics"].get("step3_duration", 0), 2),
                "step4_duration": round(self.workflow_state["statistics"].get("step4_duration", 0), 2),
                "total_duration": round(total_duration, 2)
            }
        }
        
        return report
    
    async def _generate_error_report(self, error: Exception) -> Dict[str, Any]:
        """ç”Ÿæˆé”™è¯¯æŠ¥å‘Š"""
        total_duration = self.workflow_state["end_time"] - self.workflow_state["start_time"]
        
        return {
            "success": False,
            "error": {
                "message": str(error),
                "type": type(error).__name__,
                "failed_step": self.workflow_state["current_step"]
            },
            "workflow_summary": {
                "total_duration": round(total_duration, 2),
                "completed_steps": self.workflow_state["completed_steps"],
                "failed_steps": self.workflow_state["failed_steps"],
                "workflow_started": datetime.fromtimestamp(self.workflow_state["start_time"]).isoformat() if self.workflow_state["start_time"] else None,
                "workflow_failed": datetime.fromtimestamp(self.workflow_state["end_time"]).isoformat() if self.workflow_state["end_time"] else None
            },
            "partial_results": self.workflow_state["artifacts"],
            "input_parameters": {
                "db_connection": self._mask_connection_string(self.db_connection),
                "table_list_file": self.table_list_file,
                "business_context": self.business_context,
                "db_name": self.db_name,
                "output_directory": str(self.output_dir)
            }
        }
    
    def _mask_connection_string(self, conn_str: str) -> str:
        """éšè—è¿æ¥å­—ç¬¦ä¸²ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
        import re
        return re.sub(r':[^:@]+@', ':***@', conn_str)
    
    def print_final_summary(self, report: Dict[str, Any]):
        """æ‰“å°æœ€ç»ˆæ‘˜è¦"""
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š å·¥ä½œæµç¨‹æ‰§è¡Œæ‘˜è¦")
        self.logger.info("=" * 80)
        
        if report["success"]:
            summary = report["workflow_summary"]
            results = report["processing_results"]
            outputs = report["final_outputs"]
            metrics = report["performance_metrics"]
            
            self.logger.info(f"âœ… å·¥ä½œæµç¨‹æ‰§è¡ŒæˆåŠŸ")
            self.logger.info(f"â±ï¸  æ€»è€—æ—¶: {summary['total_duration']} ç§’")
            self.logger.info(f"ğŸ“ å®Œæˆæ­¥éª¤: {len(summary['completed_steps'])}/{summary['total_steps']}")
            
            # DDL/MDç”Ÿæˆç»“æœ
            if "ddl_md_generation" in results:
                ddl_md = results["ddl_md_generation"]
                self.logger.info(f"ğŸ“‹ DDL/MDç”Ÿæˆ: {ddl_md.get('processed_successfully', 0)} ä¸ªè¡¨æˆåŠŸå¤„ç†")
            
            # Question-SQLç”Ÿæˆç»“æœ
            if "question_sql_generation" in results:
                qs = results["question_sql_generation"]
                self.logger.info(f"ğŸ¤– Question-SQLç”Ÿæˆ: {qs.get('total_questions', 0)} ä¸ªé—®ç­”å¯¹")
            
            # SQLéªŒè¯ç»“æœ
            if "sql_validation" in results:
                validation = results["sql_validation"]
                success_rate = validation.get('success_rate', 0)
                self.logger.info(f"ğŸ” SQLéªŒè¯: {success_rate:.1%} æˆåŠŸç‡ ({validation.get('valid_sql_count', 0)}/{validation.get('original_sql_count', 0)})")
            
            self.logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {outputs['output_directory']}")
            self.logger.info(f"ğŸ“„ ä¸»è¦è¾“å‡ºæ–‡ä»¶: {outputs['primary_output_file']}")
            self.logger.info(f"â“ æœ€ç»ˆé—®é¢˜æ•°é‡: {outputs['final_question_count']}")
            
        else:
            error = report["error"]
            summary = report["workflow_summary"]
            
            self.logger.error(f"âŒ å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥")
            self.logger.error(f"ğŸ’¥ å¤±è´¥åŸå› : {error['message']}")
            self.logger.error(f"ğŸ’¥ å¤±è´¥æ­¥éª¤: {error['failed_step']}")
            self.logger.error(f"â±ï¸  æ‰§è¡Œè€—æ—¶: {summary['total_duration']} ç§’")
            self.logger.error(f"âœ… å·²å®Œæˆæ­¥éª¤: {', '.join(summary['completed_steps']) if summary['completed_steps'] else 'æ— '}")
        
        self.logger.info("=" * 80)


# ä¾¿æ·çš„å‘½ä»¤è¡Œæ¥å£
def setup_argument_parser():
    """è®¾ç½®å‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="Schemaå·¥ä½œæµç¼–æ’å™¨ - ç«¯åˆ°ç«¯çš„Schemaå¤„ç†æµç¨‹",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å®Œæ•´å·¥ä½œæµç¨‹
  python -m data_pipeline.schema_workflow \\
    --db-connection "postgresql://user:pass@localhost:5432/highway_db" \\
    --table-list tables.txt \\
    --business-context "é«˜é€Ÿå…¬è·¯æœåŠ¡åŒºç®¡ç†ç³»ç»Ÿ" \\
    --output-dir ./data_pipeline/training_data/
  
  # è·³è¿‡SQLéªŒè¯
  python -m data_pipeline.schema_workflow \\
    --db-connection "postgresql://user:pass@localhost:5432/ecommerce_db" \\
    --table-list tables.txt \\
    --business-context "ç”µå•†ç³»ç»Ÿ" \\
    --skip-validation
  
  # ç¦ç”¨LLMä¿®å¤
  python -m data_pipeline.schema_workflow \\
    --db-connection "postgresql://user:pass@localhost:5432/management_db" \\
    --table-list tables.txt \\
    --business-context "ç®¡ç†ç³»ç»Ÿ" \\
    --disable-llm-repair
  
  # è·³è¿‡è®­ç»ƒæ•°æ®åŠ è½½
  python -m data_pipeline.schema_workflow \\
    --db-connection "postgresql://user:pass@localhost:5432/management_db" \\
    --table-list tables.txt \\
    --business-context "ç®¡ç†ç³»ç»Ÿ" \\
    --skip-training-load
        """
    )
    
    # å¿…éœ€å‚æ•°
    parser.add_argument(
        "--db-connection",
        required=True,
        help="æ•°æ®åº“è¿æ¥å­—ç¬¦ä¸² (postgresql://user:pass@host:port/dbname)"
    )
    
    parser.add_argument(
        "--table-list",
        required=True,
        help="è¡¨æ¸…å•æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--business-context",
        required=True,
        help="ä¸šåŠ¡ä¸Šä¸‹æ–‡æè¿°"
    )
    
    
    # å¯é€‰å‚æ•°
    parser.add_argument(
        "--output-dir",
        default="./data_pipeline/training_data/",
        help="è¾“å‡ºç›®å½•ï¼ˆé»˜è®¤ï¼š./data_pipeline/training_data/ï¼‰"
    )
    
    parser.add_argument(
        "--skip-validation",
        action="store_true",
        help="è·³è¿‡SQLéªŒè¯æ­¥éª¤"
    )
    
    parser.add_argument(
        "--disable-llm-repair",
        action="store_true",
        help="ç¦ç”¨LLMä¿®å¤åŠŸèƒ½"
    )
    
    parser.add_argument(
        "--no-modify-file",
        action="store_true",
        help="ä¸ä¿®æ”¹åŸå§‹JSONæ–‡ä»¶ï¼ˆä»…ç”ŸæˆæŠ¥å‘Šï¼‰"
    )
    
    parser.add_argument(
        "--skip-training-load",
        action="store_true",
        help="è·³è¿‡è®­ç»ƒæ•°æ®åŠ è½½æ­¥éª¤"
    )
    
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="å¯ç”¨è¯¦ç»†æ—¥å¿—è¾“å‡º"
    )
    
    parser.add_argument(
        "--log-file",
        help="æ—¥å¿—æ–‡ä»¶è·¯å¾„"
    )
    
    return parser


async def main():
    """å‘½ä»¤è¡Œå…¥å£ç‚¹"""
    import sys
    import os
    
    parser = setup_argument_parser()
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(
        verbose=args.verbose,
        log_file=args.log_file
    )
    
    # éªŒè¯è¾“å…¥æ–‡ä»¶
    if not os.path.exists(args.table_list):
        # ä¸ºè„šæœ¬æ¨¡å¼ç”Ÿæˆtask_id
        from datetime import datetime
        script_task_id = f"manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        # ä½¿ç”¨ç‹¬ç«‹æ—¥å¿—ç³»ç»Ÿ
        from data_pipeline.dp_logging import get_logger
        logger = get_logger("SchemaWorkflow", script_task_id)
        logger.error(f"é”™è¯¯: è¡¨æ¸…å•æ–‡ä»¶ä¸å­˜åœ¨: {args.table_list}")
        sys.exit(1)
    
    try:
        # åˆ›å»ºå¹¶æ‰§è¡Œå·¥ä½œæµç¼–æ’å™¨
        orchestrator = SchemaWorkflowOrchestrator(
            db_connection=args.db_connection,
            table_list_file=args.table_list,
            business_context=args.business_context,
            output_dir=args.output_dir,
            enable_sql_validation=not args.skip_validation,
            enable_llm_repair=not args.disable_llm_repair,
            modify_original_file=not args.no_modify_file,
            enable_training_data_load=not args.skip_training_load
        )
        
        # è·å–loggerç”¨äºå¯åŠ¨ä¿¡æ¯
        # ä¸ºè„šæœ¬æ¨¡å¼ç”Ÿæˆtask_id
        from datetime import datetime
        script_task_id = f"manual_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        # ä½¿ç”¨ç‹¬ç«‹æ—¥å¿—ç³»ç»Ÿ
        from data_pipeline.dp_logging import get_logger
        logger = get_logger("SchemaWorkflow", script_task_id)
        logger.info(f"ğŸš€ å¼€å§‹æ‰§è¡ŒSchemaå·¥ä½œæµç¼–æ’...")
        logger.info(f"ğŸ“ è¾“å‡ºç›®å½•: {args.output_dir}")
        logger.info(f"ğŸ“‹ è¡¨æ¸…å•: {args.table_list}")
        logger.info(f"ğŸ¢ ä¸šåŠ¡èƒŒæ™¯: {args.business_context}")
        logger.info(f"ğŸ’¾ æ•°æ®åº“: {orchestrator.db_name}")
        logger.info(f"ğŸ” SQLéªŒè¯: {'å¯ç”¨' if not args.skip_validation else 'ç¦ç”¨'}")
        logger.info(f"ğŸ”§ LLMä¿®å¤: {'å¯ç”¨' if not args.disable_llm_repair else 'ç¦ç”¨'}")
        logger.info(f"ğŸ¯ è®­ç»ƒæ•°æ®åŠ è½½: {'å¯ç”¨' if not args.skip_training_load else 'ç¦ç”¨'}")
        
        # æ‰§è¡Œå®Œæ•´å·¥ä½œæµç¨‹
        report = await orchestrator.execute_complete_workflow()
        
        # æ‰“å°è¯¦ç»†æ‘˜è¦
        orchestrator.print_final_summary(report)
        
        # è¾“å‡ºç»“æœå¹¶è®¾ç½®é€€å‡ºç 
        if report["success"]:
            if report["processing_results"].get("sql_validation", {}).get("success_rate", 1.0) >= 0.8:
                logger.info(f"\nğŸ‰ å·¥ä½œæµç¨‹æ‰§è¡ŒæˆåŠŸ!")
                exit_code = 0  # å®Œå…¨æˆåŠŸ
            else:
                logger.warning(f"\nâš ï¸  å·¥ä½œæµç¨‹æ‰§è¡Œå®Œæˆï¼Œä½†SQLéªŒè¯æˆåŠŸç‡è¾ƒä½")
                exit_code = 1  # éƒ¨åˆ†æˆåŠŸ
        else:
            logger.error(f"\nâŒ å·¥ä½œæµç¨‹æ‰§è¡Œå¤±è´¥")
            exit_code = 2  # å¤±è´¥
        
        logger.info(f"ğŸ“„ ä¸»è¦è¾“å‡ºæ–‡ä»¶: {report['final_outputs']['primary_output_file']}")
        sys.exit(exit_code)
        
    except KeyboardInterrupt:
        logger.info("\n\nâ¹ï¸  ç”¨æˆ·ä¸­æ–­ï¼Œç¨‹åºé€€å‡º")
        sys.exit(130)
    except Exception as e:
        logger.error(f"\nâŒ ç¨‹åºæ‰§è¡Œå¤±è´¥: {e}")
        if args.verbose:
            import traceback
            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    asyncio.run(main()) 