"""
Data Pipeline API ç®€åŒ–ä»»åŠ¡å·¥ä½œæµ

é›†æˆç®€åŒ–åçš„æ•°æ®åº“ç®¡ç†å™¨å’Œæ–‡ä»¶ç®¡ç†å™¨ï¼Œæä¾›ä»»åŠ¡æ‰§è¡ŒåŠŸèƒ½
"""

import asyncio
import json
import os
import logging
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List
from contextlib import contextmanager

from data_pipeline.schema_workflow import SchemaWorkflowOrchestrator
from data_pipeline.api.simple_db_manager import SimpleTaskManager
from data_pipeline.api.simple_file_manager import SimpleFileManager
from data_pipeline.dp_logging import get_logger


class SimpleWorkflowExecutor:
    """ç®€åŒ–çš„ä»»åŠ¡å·¥ä½œæµæ‰§è¡Œå™¨"""
    
    def __init__(self, task_id: str, backup_vector_tables: bool = False, truncate_vector_tables: bool = False, skip_training: bool = False):
        """
        åˆå§‹åŒ–å·¥ä½œæµæ‰§è¡Œå™¨
        
        Args:
            task_id: ä»»åŠ¡ID
            backup_vector_tables: æ˜¯å¦å¤‡ä»½vectorè¡¨æ•°æ®
            truncate_vector_tables: æ˜¯å¦æ¸…ç©ºvectorè¡¨æ•°æ®ï¼ˆè‡ªåŠ¨å¯ç”¨å¤‡ä»½ï¼‰
            skip_training: æ˜¯å¦è·³è¿‡è®­ç»ƒæ–‡ä»¶å¤„ç†ï¼Œä»…æ‰§è¡ŒVectorè¡¨ç®¡ç†
        """
        self.task_id = task_id
        self.backup_vector_tables = backup_vector_tables
        self.truncate_vector_tables = truncate_vector_tables
        self.skip_training = skip_training
        
        # å‚æ•°é€»è¾‘ï¼štruncateè‡ªåŠ¨å¯ç”¨backup
        if self.truncate_vector_tables:
            self.backup_vector_tables = True
        
        self.logger = get_logger("SimpleWorkflowExecutor", task_id)
        
        # è®°å½•Vectorè¡¨ç®¡ç†å‚æ•°çŠ¶æ€
        if self.backup_vector_tables or self.truncate_vector_tables:
            self.logger.info(f"ğŸ—‚ï¸ Vectorè¡¨ç®¡ç†å·²å¯ç”¨: backup={self.backup_vector_tables}, truncate={self.truncate_vector_tables}")
        
        # åˆå§‹åŒ–ç®¡ç†å™¨
        self.task_manager = SimpleTaskManager()
        self.file_manager = SimpleFileManager()
        
        # ä»»åŠ¡ç›®å½•æ—¥å¿—è®°å½•å™¨
        self.task_dir_logger = None
        
        # åŠ è½½ä»»åŠ¡ä¿¡æ¯
        self.task_info = None
        self.task_params = None
        self._load_task_info()
    
    def _load_task_info(self):
        """åŠ è½½ä»»åŠ¡ä¿¡æ¯"""
        try:
            self.task_info = self.task_manager.get_task(self.task_id)
            if self.task_info:
                self.task_params = self.task_info.get('parameters', {})
            else:
                raise ValueError(f"ä»»åŠ¡ä¸å­˜åœ¨: {self.task_id}")
        except Exception as e:
            self.logger.error(f"åŠ è½½ä»»åŠ¡ä¿¡æ¯å¤±è´¥: {e}")
            raise
    
    def _ensure_task_directory(self) -> bool:
        """ç¡®ä¿ä»»åŠ¡ç›®å½•å­˜åœ¨"""
        try:
            success = self.file_manager.create_task_directory(self.task_id)
            if success:
                # å†™å…¥ä»»åŠ¡é…ç½®æ–‡ä»¶
                self._write_task_config()
                # åˆå§‹åŒ–ä»»åŠ¡ç›®å½•æ—¥å¿—è®°å½•å™¨
                self._setup_task_directory_logger()
            return success
        except Exception as e:
            self.logger.error(f"åˆ›å»ºä»»åŠ¡ç›®å½•å¤±è´¥: {e}")
            return False
    
    def _write_task_config(self):
        """å†™å…¥ä»»åŠ¡é…ç½®æ–‡ä»¶"""
        try:
            task_dir = self.file_manager.get_task_directory(self.task_id)
            config_file = task_dir / "task_config.json"
            
            config_data = {
                "task_id": self.task_id,
                "created_at": self.task_info.get('created_at').isoformat() if self.task_info.get('created_at') else None,
                "parameters": self.task_params,
                "output_directory": str(task_dir)
            }
            
            with open(config_file, 'w', encoding='utf-8') as f:
                json.dump(config_data, f, ensure_ascii=False, indent=2, default=str)
                
        except Exception as e:
            self.logger.error(f"å†™å…¥ä»»åŠ¡é…ç½®å¤±è´¥: {e}")
    
    def _setup_task_directory_logger(self):
        """è®¾ç½®ä»»åŠ¡ç›®å½•æ—¥å¿—è®°å½•å™¨"""
        try:
            task_dir = self.file_manager.get_task_directory(self.task_id)
            log_file = task_dir / "data_pipeline.log"
            
            # åˆ›å»ºä¸“é—¨çš„ä»»åŠ¡ç›®å½•æ—¥å¿—è®°å½•å™¨
            self.task_dir_logger = logging.getLogger(f"TaskDir_{self.task_id}")
            self.task_dir_logger.setLevel(logging.DEBUG)
            
            # æ¸…é™¤å·²æœ‰å¤„ç†å™¨
            self.task_dir_logger.handlers.clear()
            self.task_dir_logger.propagate = False
            
            # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
            file_handler = logging.FileHandler(log_file, encoding='utf-8')
            file_handler.setLevel(logging.DEBUG)
            
            # è®¾ç½®è¯¦ç»†çš„æ—¥å¿—æ ¼å¼
            formatter = logging.Formatter(
                '%(asctime)s [%(levelname)s] %(name)s: %(message)s',
                datefmt='%Y-%m-%d %H:%M:%S'
            )
            file_handler.setFormatter(formatter)
            
            self.task_dir_logger.addHandler(file_handler)
            
            # è®°å½•åˆå§‹åŒ–ä¿¡æ¯
            self.task_dir_logger.info(f"ä»»åŠ¡ç›®å½•æ—¥å¿—åˆå§‹åŒ–å®Œæˆ - ä»»åŠ¡ID: {self.task_id}")
            self.task_dir_logger.info(f"ä»»åŠ¡å‚æ•°: {json.dumps(self.task_params, ensure_ascii=False, default=str)}")
            
        except Exception as e:
            self.logger.error(f"è®¾ç½®ä»»åŠ¡ç›®å½•æ—¥å¿—è®°å½•å™¨å¤±è´¥: {e}")
    
    def _log_to_task_directory(self, level: str, message: str, step_name: str = None):
        """è®°å½•æ—¥å¿—åˆ°ä»»åŠ¡ç›®å½•"""
        if self.task_dir_logger:
            try:
                if step_name:
                    message = f"[{step_name}] {message}"
                
                log_level = getattr(logging, level.upper(), logging.INFO)
                self.task_dir_logger.log(log_level, message)
            except Exception as e:
                self.logger.error(f"è®°å½•ä»»åŠ¡ç›®å½•æ—¥å¿—å¤±è´¥: {e}")
    
    def _backup_existing_files_if_needed(self):
        """å¦‚æœéœ€è¦ï¼Œå¤‡ä»½ç°æœ‰æ–‡ä»¶ï¼ˆä»…å¤‡ä»½æ–‡ä»¶ï¼Œä¸åŒ…æ‹¬å­ç›®å½•ï¼‰"""
        try:
            task_dir = self.file_manager.get_task_directory(self.task_id)
            
            # ä¸¥æ ¼æ£€æŸ¥ï¼šåªå…è®¸ä¿ç•™æŒ‡å®šæ–‡ä»¶
            allowed_files = {"table_list.txt", "data_pipeline.log"}
            
            # æ‰«æä»»åŠ¡ç›®å½•ä¸­çš„æ–‡ä»¶ï¼ˆæ’é™¤å­ç›®å½•å’Œå…è®¸çš„æ–‡ä»¶ï¼‰
            files_to_backup = []
            for item in task_dir.iterdir():
                if item.is_file() and item.name not in allowed_files:
                    files_to_backup.append(item)
            
            # å¦‚æœæ²¡æœ‰æ–‡ä»¶éœ€è¦å¤‡ä»½ï¼Œç›´æ¥è¿”å›
            if not files_to_backup:
                self._log_to_task_directory("INFO", "ä»»åŠ¡ç›®å½•ä¸­æ²¡æœ‰éœ€è¦å¤‡ä»½çš„æ–‡ä»¶")
                return
            
            # åˆ›å»ºå¤‡ä»½ç›®å½•
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_dir_name = f"file_bak_{timestamp}"
            backup_dir = task_dir / backup_dir_name
            
            # å¤„ç†å¤‡ä»½ç›®å½•åå†²çª
            counter = 1
            while backup_dir.exists():
                backup_dir = task_dir / f"{backup_dir_name}_{counter}"
                counter += 1
            
            backup_dir.mkdir(parents=True)
            
            # ç§»åŠ¨æ–‡ä»¶åˆ°å¤‡ä»½ç›®å½•
            moved_files = []
            failed_files = []
            
            for file_path in files_to_backup:
                try:
                    target_path = backup_dir / file_path.name
                    shutil.move(str(file_path), str(target_path))
                    moved_files.append(file_path.name)
                    self._log_to_task_directory("DEBUG", f"æ–‡ä»¶å·²å¤‡ä»½: {file_path.name}")
                except Exception as e:
                    failed_files.append({"file": file_path.name, "error": str(e)})
                    self._log_to_task_directory("WARNING", f"æ–‡ä»¶å¤‡ä»½å¤±è´¥: {file_path.name} - {e}")
            
            # ç”Ÿæˆå¤‡ä»½è®°å½•æ–‡ä»¶
            backup_info = {
                "backup_time": datetime.now().isoformat(),
                "backup_directory": backup_dir.name,
                "moved_files": moved_files,
                "failed_files": failed_files,
                "task_id": self.task_id
            }
            
            backup_info_file = backup_dir / "backup_info.json"
            with open(backup_info_file, 'w', encoding='utf-8') as f:
                json.dump(backup_info, f, ensure_ascii=False, indent=2)
            
            # è®°å½•å¤‡ä»½å®Œæˆ
            self._log_to_task_directory("INFO", 
                f"æ–‡ä»¶å¤‡ä»½å®Œæˆ: {len(moved_files)} ä¸ªæ–‡ä»¶å·²ç§»åŠ¨åˆ° {backup_dir.name}")
            
            # å¦‚æœæœ‰æ–‡ä»¶å¤‡ä»½å¤±è´¥ï¼Œä¸­æ–­ä½œä¸š
            if failed_files:
                error_msg = f"âŒ æ— æ³•æ¸…ç†å·¥ä½œç›®å½•ï¼Œä»¥ä¸‹æ–‡ä»¶ç§»åŠ¨å¤±è´¥: {[f['file'] for f in failed_files]}"
                self._log_to_task_directory("ERROR", error_msg)
                raise Exception(error_msg)
        
        except Exception as e:
            # å¤‡ä»½å¤±è´¥å¿…é¡»ä¸­æ–­ä½œä¸š
            error_msg = f"âŒ æ–‡ä»¶å¤‡ä»½è¿‡ç¨‹å¤±è´¥ï¼Œä½œä¸šä¸­æ–­: {e}"
            self._log_to_task_directory("ERROR", error_msg)
            raise Exception(error_msg)
    
    def _resolve_table_list_file_path(self) -> str:
        """è§£æè¡¨æ¸…å•æ–‡ä»¶è·¯å¾„"""
        table_list_file = self.task_params['table_list_file']
        
        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨æ–‡ä»¶ä¸Šä¼ æ¨¡å¼
        if self.task_params.get('file_upload_mode', False) or '{task_directory}' in table_list_file:
            # æ–‡ä»¶ä¸Šä¼ æ¨¡å¼ï¼šæ£€æŸ¥ä»»åŠ¡ç›®å½•ä¸­çš„æ–‡ä»¶
            task_dir = self.file_manager.get_task_directory(self.task_id)
            
            # æ›¿æ¢å ä½ç¬¦
            if '{task_directory}' in table_list_file:
                resolved_path = table_list_file.replace('{task_directory}', str(task_dir))
            else:
                from data_pipeline.config import SCHEMA_TOOLS_CONFIG
                upload_config = SCHEMA_TOOLS_CONFIG.get("file_upload", {})
                target_filename = upload_config.get("target_filename", "table_list.txt")
                resolved_path = str(task_dir / target_filename)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not Path(resolved_path).exists():
                raise FileNotFoundError(
                    f"è¡¨æ¸…å•æ–‡ä»¶ä¸å­˜åœ¨: {resolved_path}ã€‚"
                    f"è¯·å…ˆä¸Šä¼ è¡¨æ¸…å•æ–‡ä»¶åˆ°ä»»åŠ¡ {self.task_id}ï¼Œç„¶åå†æ‰§è¡Œå·¥ä½œæµã€‚"
                )
            
            return resolved_path
        else:
            # ä¼ ç»Ÿæ¨¡å¼ï¼šä½¿ç”¨æŒ‡å®šçš„æ–‡ä»¶è·¯å¾„
            if not Path(table_list_file).exists():
                raise FileNotFoundError(f"è¡¨æ¸…å•æ–‡ä»¶ä¸å­˜åœ¨: {table_list_file}")
            return table_list_file
    
    def _create_orchestrator(self) -> SchemaWorkflowOrchestrator:
        """åˆ›å»ºå·¥ä½œæµç¼–æ’å™¨"""
        task_dir = self.file_manager.get_task_directory(self.task_id)
        
        # è§£æè¡¨æ¸…å•æ–‡ä»¶è·¯å¾„
        table_list_file = self._resolve_table_list_file_path()
        
        return SchemaWorkflowOrchestrator(
            db_connection=self.task_params['db_connection'],
            table_list_file=table_list_file,
            business_context=self.task_params['business_context'],
            output_dir=str(task_dir),
            task_id=self.task_id,  # ä¼ é€’task_idç»™ç¼–æ’å™¨
            enable_sql_validation=self.task_params.get('enable_sql_validation', True),
            enable_llm_repair=self.task_params.get('enable_llm_repair', True),
            modify_original_file=self.task_params.get('modify_original_file', True),
            enable_training_data_load=self.task_params.get('enable_training_data_load', True),
            # æ–°å¢ï¼šVectorè¡¨ç®¡ç†å‚æ•°
            backup_vector_tables=self.backup_vector_tables,
            truncate_vector_tables=self.truncate_vector_tables,
            skip_training=self.skip_training
        )
    
    @contextmanager
    def _step_execution(self, step_name: str):
        """æ­¥éª¤æ‰§è¡Œä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        execution_id = None
        
        try:
            # å¼€å§‹æ‰§è¡Œ
            execution_id = self.task_manager.start_step(self.task_id, step_name)
            
            # è®°å½•åˆ°ä»»åŠ¡ç›®å½•æ—¥å¿—
            self._log_to_task_directory("INFO", f"å¼€å§‹æ‰§è¡Œæ­¥éª¤: {step_name}", step_name)
            
            yield execution_id
            
            # æˆåŠŸå®Œæˆ
            self.task_manager.complete_step(self.task_id, step_name, 'completed')
            
            # è®°å½•åˆ°ä»»åŠ¡ç›®å½•æ—¥å¿—
            self._log_to_task_directory("INFO", f"æ­¥éª¤æ‰§è¡Œå®Œæˆ: {step_name}", step_name)
            
        except Exception as e:
            # æ‰§è¡Œå¤±è´¥
            error_msg = str(e)
            
            self.task_manager.complete_step(self.task_id, step_name, 'failed', error_msg)
            
            # è®°å½•åˆ°ä»»åŠ¡ç›®å½•æ—¥å¿—
            self._log_to_task_directory("ERROR", f"æ­¥éª¤æ‰§è¡Œå¤±è´¥: {step_name} - {error_msg}", step_name)
            raise
    
    async def execute_complete_workflow(self) -> Dict[str, Any]:
        """æ‰§è¡Œå®Œæ•´å·¥ä½œæµ"""
        try:
            # ğŸ†• æ–°å¢ï¼šå…ˆå¤‡ä»½ç°æœ‰æ–‡ä»¶ï¼ˆæ¸…ç†ç¯å¢ƒï¼‰
            self._backup_existing_files_if_needed()
            
            # ç¡®ä¿ä»»åŠ¡ç›®å½•å­˜åœ¨å¹¶å†™å…¥æ–°é…ç½®
            if not self._ensure_task_directory():
                raise Exception("æ— æ³•åˆ›å»ºä»»åŠ¡ç›®å½•")
            
            # å¼€å§‹ä»»åŠ¡
            self.task_manager.update_task_status(self.task_id, 'in_progress')
            
            # è®°å½•åˆ°ä»»åŠ¡ç›®å½•æ—¥å¿—
            self._log_to_task_directory("INFO", "å®Œæ•´å·¥ä½œæµä»»åŠ¡å¼€å§‹æ‰§è¡Œ")
            
            # åˆ›å»ºå·¥ä½œæµç¼–æ’å™¨
            orchestrator = self._create_orchestrator()
            
            # é‡å®šå‘SchemaWorkflowOrchestratorçš„æ—¥å¿—åˆ°ä»»åŠ¡ç›®å½•
            self._redirect_orchestrator_logs(orchestrator)
            
            # åˆ†åˆ«æ‰§è¡Œå„ä¸ªæ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤éƒ½ç”¨_step_executionåŒ…è£…
            try:
                # æ­¥éª¤1: DDL/MDç”Ÿæˆ
                with self._step_execution("ddl_generation") as execution_id:
                    self._log_to_task_directory("INFO", "å¼€å§‹æ‰§è¡ŒDDL/MDç”Ÿæˆæ­¥éª¤", "ddl_generation")
                    await orchestrator._execute_step_1_ddl_md_generation()
                    self._log_to_task_directory("INFO", "DDL/MDç”Ÿæˆæ­¥éª¤å®Œæˆ", "ddl_generation")
                
                # æ­¥éª¤2: Question-SQLç”Ÿæˆ  
                with self._step_execution("qa_generation") as execution_id:
                    self._log_to_task_directory("INFO", "å¼€å§‹æ‰§è¡ŒQuestion-SQLç”Ÿæˆæ­¥éª¤", "qa_generation")
                    await orchestrator._execute_step_2_question_sql_generation()
                    self._log_to_task_directory("INFO", "Question-SQLç”Ÿæˆæ­¥éª¤å®Œæˆ", "qa_generation")
                
                # æ­¥éª¤3: SQLéªŒè¯ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if orchestrator.enable_sql_validation:
                    with self._step_execution("sql_validation") as execution_id:
                        self._log_to_task_directory("INFO", "å¼€å§‹æ‰§è¡ŒSQLéªŒè¯æ­¥éª¤", "sql_validation")
                        await orchestrator._execute_step_3_sql_validation()
                        self._log_to_task_directory("INFO", "SQLéªŒè¯æ­¥éª¤å®Œæˆ", "sql_validation")
                else:
                    self._log_to_task_directory("INFO", "è·³è¿‡SQLéªŒè¯æ­¥éª¤ï¼ˆæœªå¯ç”¨ï¼‰", "sql_validation")
                
                # æ­¥éª¤4: è®­ç»ƒæ•°æ®åŠ è½½ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                if orchestrator.enable_training_data_load:
                    with self._step_execution("training_load") as execution_id:
                        self._log_to_task_directory("INFO", "å¼€å§‹æ‰§è¡Œè®­ç»ƒæ•°æ®åŠ è½½æ­¥éª¤", "training_load")
                        await orchestrator._execute_step_4_training_data_load()
                        self._log_to_task_directory("INFO", "è®­ç»ƒæ•°æ®åŠ è½½æ­¥éª¤å®Œæˆ", "training_load")
                else:
                    self._log_to_task_directory("INFO", "è·³è¿‡è®­ç»ƒæ•°æ®åŠ è½½æ­¥éª¤ï¼ˆæœªå¯ç”¨ï¼‰", "training_load")
                
                # è·å–å·¥ä½œæµç»“æœ
                result = {
                    "success": True,
                    "workflow_state": orchestrator.workflow_state,
                    "artifacts": orchestrator.workflow_state.get("artifacts", {})
                }
                
                # å†™å…¥ç»“æœæ–‡ä»¶
                self._write_result_file(result)
                
            except Exception as step_error:
                self.logger.error(f"å·¥ä½œæµæ­¥éª¤æ‰§è¡Œå¤±è´¥: {step_error}")
                # è®°å½•åˆ°ä»»åŠ¡ç›®å½•æ—¥å¿—
                self._log_to_task_directory("ERROR", f"å·¥ä½œæµæ­¥éª¤æ‰§è¡Œå¤±è´¥: {step_error}")
                raise
            
            # å®Œæˆä»»åŠ¡
            self.task_manager.update_task_status(self.task_id, 'completed')
            
            # è®°å½•åˆ°ä»»åŠ¡ç›®å½•æ—¥å¿—
            self._log_to_task_directory("INFO", "å®Œæ•´å·¥ä½œæµä»»åŠ¡æ‰§è¡Œå®Œæˆ")
            
            return {
                "success": True,
                "task_id": self.task_id,
                "execution_mode": "complete",
                "result": result
            }
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            error_msg = str(e)
            self.task_manager.update_task_status(self.task_id, 'failed', error_msg)
            
            # è®°å½•åˆ°ä»»åŠ¡ç›®å½•æ—¥å¿—
            self._log_to_task_directory("ERROR", f"å®Œæ•´å·¥ä½œæµä»»åŠ¡æ‰§è¡Œå¤±è´¥: {error_msg}")
            
            return {
                "success": False,
                "task_id": self.task_id,
                "execution_mode": "complete",
                "error": error_msg
            }
    
    async def execute_single_step(self, step_name: str) -> Dict[str, Any]:
        """æ‰§è¡Œå•ä¸ªæ­¥éª¤"""
        try:
            # æ–°å¢ï¼šétraining_loadæ­¥éª¤çš„Vectorè¡¨ç®¡ç†å‚æ•°è­¦å‘Š
            if step_name != 'training_load' and (self.backup_vector_tables or self.truncate_vector_tables or self.skip_training):
                self.logger.warning(
                    f"âš ï¸ Vectorè¡¨ç®¡ç†å‚æ•°ä»…åœ¨training_loadæ­¥éª¤æœ‰æ•ˆï¼Œå½“å‰æ­¥éª¤: {step_name}ï¼Œå¿½ç•¥å‚æ•°"
                )
                # ä¸´æ—¶ç¦ç”¨Vectorè¡¨ç®¡ç†å‚æ•°
                temp_backup = self.backup_vector_tables
                temp_truncate = self.truncate_vector_tables
                temp_skip = self.skip_training
                self.backup_vector_tables = False
                self.truncate_vector_tables = False
                self.skip_training = False
            
            # ç¡®ä¿ä»»åŠ¡ç›®å½•å­˜åœ¨
            if not self._ensure_task_directory():
                raise Exception("æ— æ³•åˆ›å»ºä»»åŠ¡ç›®å½•")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.task_manager.update_task_status(self.task_id, 'in_progress')
            
            # åˆ›å»ºå·¥ä½œæµç¼–æ’å™¨ï¼ˆä¼šæ ¹æ®å½“å‰å‚æ•°çŠ¶æ€åˆ›å»ºï¼‰
            orchestrator = self._create_orchestrator()
            
            # é‡å®šå‘SchemaWorkflowOrchestratorçš„æ—¥å¿—åˆ°ä»»åŠ¡ç›®å½•
            self._redirect_orchestrator_logs(orchestrator)
            
            # æ‰§è¡ŒæŒ‡å®šæ­¥éª¤
            result = None
            with self._step_execution(step_name) as execution_id:
                if step_name == "ddl_generation":
                    await orchestrator._execute_step_1_ddl_md_generation()
                    result = orchestrator.workflow_state["artifacts"].get("ddl_md_generation", {})
                    
                elif step_name == "qa_generation":
                    await orchestrator._execute_step_2_question_sql_generation()
                    result = orchestrator.workflow_state["artifacts"].get("question_sql_generation", {})
                    
                elif step_name == "sql_validation":
                    await orchestrator._execute_step_3_sql_validation()
                    result = orchestrator.workflow_state["artifacts"].get("sql_validation", {})
                    
                elif step_name == "training_load":
                    await orchestrator._execute_step_4_training_data_load()
                    result = orchestrator.workflow_state["artifacts"].get("training_data_load", {})
                    
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„æ­¥éª¤: {step_name}")
                
                # å†™å…¥æ­¥éª¤ç»“æœæ–‡ä»¶
                self._write_step_result_file(step_name, result)
            
            # æ¢å¤åŸå§‹å‚æ•°çŠ¶æ€ï¼ˆå¦‚æœè¢«ä¸´æ—¶ä¿®æ”¹ï¼‰
            if step_name != 'training_load' and 'temp_backup' in locals():
                self.backup_vector_tables = temp_backup
                self.truncate_vector_tables = temp_truncate
                self.skip_training = temp_skip
            
            # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ­¥éª¤éƒ½å·²å®Œæˆ
            self._update_overall_task_status()
            
            return {
                "success": True,
                "task_id": self.task_id,
                "execution_mode": "step",
                "step_name": step_name,
                "result": result
            }
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            error_msg = str(e)
            self.task_manager.update_task_status(self.task_id, 'failed', error_msg)
            
            # è®°å½•åˆ°ä»»åŠ¡ç›®å½•æ—¥å¿—
            self._log_to_task_directory("ERROR", f"æ­¥éª¤æ‰§è¡Œå¤±è´¥: {step_name} - {error_msg}", step_name)
            
            return {
                "success": False,
                "task_id": self.task_id,
                "execution_mode": "step",
                "step_name": step_name,
                "error": error_msg
            }
    
    def _write_result_file(self, result: Dict[str, Any]):
        """å†™å…¥å®Œæ•´ç»“æœæ–‡ä»¶"""
        try:
            task_dir = self.file_manager.get_task_directory(self.task_id)
            result_file = task_dir / "task_result.json"
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2, default=str)
                
        except Exception as e:
            self.logger.error(f"å†™å…¥ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
    
    def _write_step_result_file(self, step_name: str, result: Dict[str, Any]):
        """å†™å…¥æ­¥éª¤ç»“æœæ–‡ä»¶"""
        try:
            task_dir = self.file_manager.get_task_directory(self.task_id)
            result_file = task_dir / f"{step_name}_result.json"
            
            with open(result_file, 'w', encoding='utf-8') as f:
                json.dump(result, f, ensure_ascii=False, indent=2, default=str)
                
        except Exception as e:
            self.logger.error(f"å†™å…¥æ­¥éª¤ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
    
    def _update_overall_task_status(self):
        """æ›´æ–°æ•´ä½“ä»»åŠ¡çŠ¶æ€"""
        try:
            # æ£€æŸ¥æ‰€æœ‰æ­¥éª¤çš„å®Œæˆæƒ…å†µ
            steps = self.task_manager.get_task_steps(self.task_id)
            
            completed_steps = set()
            failed_steps = set()
            
            for step in steps:
                if step['step_status'] == 'completed':
                    completed_steps.add(step['step_name'])
                elif step['step_status'] == 'failed':
                    failed_steps.add(step['step_name'])
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥çš„æ­¥éª¤
            if failed_steps:
                self.task_manager.update_task_status(self.task_id, 'failed')
                return
            
            # æ£€æŸ¥æ˜¯å¦å®Œæˆäº†å¿…è¦æ­¥éª¤
            required_steps = {"ddl_generation", "qa_generation"}
            if required_steps.issubset(completed_steps):
                # æ£€æŸ¥æ˜¯å¦æœ‰å¯é€‰æ­¥éª¤å®Œæˆ
                optional_steps = {"sql_validation", "training_load"}
                if completed_steps.intersection(optional_steps):
                    if len(completed_steps) >= 3:
                        self.task_manager.update_task_status(self.task_id, 'completed')
                    else:
                        self.task_manager.update_task_status(self.task_id, 'partial_completed')
                else:
                    self.task_manager.update_task_status(self.task_id, 'partial_completed')
            
        except Exception as e:
            self.logger.error(f"æ›´æ–°ä»»åŠ¡çŠ¶æ€å¤±è´¥: {e}")
    
    def _redirect_orchestrator_logs(self, orchestrator):
        """é‡å®šå‘SchemaWorkflowOrchestratorçš„æ—¥å¿—åˆ°ä»»åŠ¡ç›®å½•"""
        if self.task_dir_logger and hasattr(orchestrator, 'logger'):
            try:
                # ä¸ºorchestratorçš„loggeræ·»åŠ ä»»åŠ¡ç›®å½•æ–‡ä»¶å¤„ç†å™¨
                for handler in self.task_dir_logger.handlers:
                    if isinstance(handler, logging.FileHandler):
                        orchestrator.logger.addHandler(handler)
                        break
            except Exception as e:
                self.logger.error(f"é‡å®šå‘orchestratoræ—¥å¿—å¤±è´¥: {e}")
    
    def query_logs_advanced(self,
                           page: int = 1,
                           page_size: int = 50,
                           level: str = None,
                           start_time: str = None,
                           end_time: str = None,
                           keyword: str = None,
                           logger_name: str = None,
                           step_name: str = None,
                           sort_by: str = "timestamp",
                           sort_order: str = "desc") -> dict:
        """
        é«˜çº§æ—¥å¿—æŸ¥è¯¢ï¼ˆå·¥ä½œæµå±‚ï¼‰
        
        Args:
            page: é¡µç ï¼Œå¿…é¡»å¤§äº0ï¼Œé»˜è®¤1
            page_size: æ¯é¡µå¤§å°ï¼Œ1-500ä¹‹é—´ï¼Œé»˜è®¤50
            level: å¯é€‰ï¼Œæ—¥å¿—çº§åˆ«ç­›é€‰
            start_time: å¯é€‰ï¼Œå¼€å§‹æ—¶é—´èŒƒå›´
            end_time: å¯é€‰ï¼Œç»“æŸæ—¶é—´èŒƒå›´
            keyword: å¯é€‰ï¼Œå…³é”®å­—æœç´¢
            logger_name: å¯é€‰ï¼Œæ—¥å¿—è®°å½•å™¨åç§°
            step_name: å¯é€‰ï¼Œæ‰§è¡Œæ­¥éª¤åç§°
            sort_by: å¯é€‰ï¼Œæ’åºå­—æ®µ
            sort_order: å¯é€‰ï¼Œæ’åºæ–¹å‘
            
        Returns:
            æ—¥å¿—æŸ¥è¯¢ç»“æœ
        """
        try:
            # è°ƒç”¨æ•°æ®åº“å±‚æ–¹æ³•
            result = self.task_manager.query_logs_advanced(
                task_id=self.task_id,
                page=page,
                page_size=page_size,
                level=level,
                start_time=start_time,
                end_time=end_time,
                keyword=keyword,
                logger_name=logger_name,
                step_name=step_name,
                sort_by=sort_by,
                sort_order=sort_order
            )
            
            # è®°å½•æŸ¥è¯¢æ“ä½œ
            self.logger.info(f"æ—¥å¿—æŸ¥è¯¢å®Œæˆ: {self.task_id}, é¡µç : {page}, ç»“æœæ•°: {len(result.get('logs', []))}")
            
            return result
            
        except Exception as e:
            self.logger.error(f"æ—¥å¿—æŸ¥è¯¢å¤±è´¥: {e}")
            return {
                "logs": [],
                "pagination": {
                    "page": page,
                    "page_size": page_size,
                    "total": 0,
                    "total_pages": 0,
                    "has_next": False,
                    "has_prev": False
                },
                "log_file_info": {
                    "exists": False,
                    "error": str(e)
                },
                "query_time": "0.000s"
            }
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            # æ¸…ç†ä»»åŠ¡ç›®å½•æ—¥å¿—è®°å½•å™¨
            if self.task_dir_logger:
                for handler in self.task_dir_logger.handlers:
                    handler.close()
                self.task_dir_logger.handlers.clear()
                
            self.task_manager.close_connection()
        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")


class SimpleWorkflowManager:
    """ç®€åŒ–çš„ä»»åŠ¡å·¥ä½œæµç®¡ç†å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å·¥ä½œæµç®¡ç†å™¨"""
        self.task_manager = SimpleTaskManager()
        self.file_manager = SimpleFileManager()
        # ä½¿ç”¨ç®€å•çš„æ§åˆ¶å°æ—¥å¿—ï¼Œä¸ä½¿ç”¨æ–‡ä»¶æ—¥å¿—
        self.logger = logging.getLogger("SimpleWorkflowManager")
        self.logger.setLevel(logging.INFO)
    
    def create_task(self, 
                   table_list_file: str = None,
                   business_context: str = None,
                   db_name: str = None,
                   **kwargs) -> str:
        """åˆ›å»ºæ–°ä»»åŠ¡"""
        try:
            # å¦‚æœæä¾›äº†table_list_fileï¼ŒéªŒè¯æ–‡ä»¶å­˜åœ¨
            if table_list_file and not os.path.exists(table_list_file):
                raise FileNotFoundError(f"è¡¨æ¸…å•æ–‡ä»¶ä¸å­˜åœ¨: {table_list_file}")
            
            # åˆ›å»ºä»»åŠ¡ï¼ˆä½¿ç”¨app_configä¸­çš„æ•°æ®åº“é…ç½®ï¼‰
            task_id = self.task_manager.create_task(
                table_list_file=table_list_file,
                business_context=business_context,
                db_name=db_name,
                **kwargs
            )
            
            return task_id
            
        except Exception as e:
            self.logger.error(f"åˆ›å»ºä»»åŠ¡å¤±è´¥: {e}")
            raise
    
    async def execute_task(self, 
                          task_id: str,
                          execution_mode: str = "complete",
                          step_name: Optional[str] = None) -> Dict[str, Any]:
        """æ‰§è¡Œä»»åŠ¡"""
        executor = None
        try:
            executor = SimpleWorkflowExecutor(task_id)
            
            if execution_mode == "complete":
                return await executor.execute_complete_workflow()
            elif execution_mode == "step":
                if not step_name:
                    raise ValueError("æ­¥éª¤æ‰§è¡Œæ¨¡å¼éœ€è¦æŒ‡å®šstep_name")
                return await executor.execute_single_step(step_name)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ‰§è¡Œæ¨¡å¼: {execution_mode}")
                
        finally:
            if executor:
                executor.cleanup()
    
    def get_task_status(self, task_id: str) -> Optional[Dict[str, Any]]:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        return self.task_manager.get_task(task_id)
    
    def get_task_files(self, task_id: str) -> List[Dict[str, Any]]:
        """è·å–ä»»åŠ¡æ–‡ä»¶åˆ—è¡¨"""
        return self.file_manager.get_task_files(task_id)
    
    def get_task_steps(self, task_id: str) -> List[Dict[str, Any]]:
        """è·å–ä»»åŠ¡æ­¥éª¤çŠ¶æ€"""
        return self.task_manager.get_task_steps(task_id)
    
    def get_tasks_list(self, **kwargs) -> List[Dict[str, Any]]:
        """è·å–ä»»åŠ¡åˆ—è¡¨"""
        return self.task_manager.get_tasks_list(**kwargs)
    
    def query_tasks_advanced(self, **kwargs) -> dict:
        """
        é«˜çº§ä»»åŠ¡æŸ¥è¯¢ï¼Œæ”¯æŒå¤æ‚ç­›é€‰ã€æ’åºã€åˆ†é¡µ
        
        Args:
            **kwargs: ä¼ é€’ç»™æ•°æ®åº“å±‚çš„æŸ¥è¯¢å‚æ•°
        
        Returns:
            åŒ…å«ä»»åŠ¡åˆ—è¡¨å’Œåˆ†é¡µä¿¡æ¯çš„å­—å…¸
        """
        return self.task_manager.query_tasks_advanced(**kwargs)
    
    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        try:
            self.task_manager.close_connection()
        except Exception as e:
            self.logger.error(f"æ¸…ç†èµ„æºå¤±è´¥: {e}")