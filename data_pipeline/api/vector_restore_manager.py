"""
Vectorè¡¨æ¢å¤ç®¡ç†å™¨

æä¾›pgvectorè¡¨å¤‡ä»½æ–‡ä»¶æ‰«æå’Œæ•°æ®æ¢å¤åŠŸèƒ½ï¼Œä¸VectorTableManagerå½¢æˆå®Œæ•´çš„å¤‡ä»½æ¢å¤è§£å†³æ–¹æ¡ˆ
"""

import os
import re
import time
import glob
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, List, Optional
import psycopg2
import logging


class VectorRestoreManager:
    """Vectorè¡¨æ¢å¤ç®¡ç†å™¨ - ä»¿ç…§VectorTableManagerè®¾è®¡"""
    
    def __init__(self, base_output_dir: str = None):
        """
        åˆå§‹åŒ–æ¢å¤ç®¡ç†å™¨ï¼Œå¤ç”¨ç°æœ‰é…ç½®æœºåˆ¶
        
        Args:
            base_output_dir: åŸºç¡€è¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä»data_pipeline.configè·å–
        """
        if base_output_dir is None:
            # ä»é…ç½®æ–‡ä»¶è·å–é»˜è®¤ç›®å½•
            from data_pipeline.config import SCHEMA_TOOLS_CONFIG
            base_output_dir = SCHEMA_TOOLS_CONFIG.get("output_directory", "./data_pipeline/training_data/")
        
        self.base_output_dir = Path(base_output_dir)
        
        # ä»data_pipeline.configè·å–é…ç½®
        from data_pipeline.config import SCHEMA_TOOLS_CONFIG
        self.config = SCHEMA_TOOLS_CONFIG.get("vector_table_management", {})
        
        # åˆå§‹åŒ–æ—¥å¿—
        self.logger = logging.getLogger("VectorRestoreManager")
        
        # æ”¯æŒçš„è¡¨å
        self.supported_tables = self.config.get("supported_tables", [
            "langchain_pg_collection",
            "langchain_pg_embedding"
        ])
    
    def scan_backup_files(self, global_only: bool = False, task_id: str = None) -> Dict[str, Any]:
        """
        æ‰«æå¯ç”¨çš„å¤‡ä»½æ–‡ä»¶
        
        Args:
            global_only: ä»…æŸ¥è¯¢å…¨å±€å¤‡ä»½ç›®å½•ï¼ˆtraining_data/vector_bak/ï¼‰
            task_id: æŒ‡å®štask_idï¼Œä»…æŸ¥è¯¢è¯¥ä»»åŠ¡ä¸‹çš„å¤‡ä»½æ–‡ä»¶
            
        Returns:
            åŒ…å«å¤‡ä»½æ–‡ä»¶ä¿¡æ¯çš„å­—å…¸
        """
        scan_start_time = datetime.now()
        backup_locations = []
        
        try:
            # ç¡®å®šæ‰«æèŒƒå›´
            if task_id:
                # ä»…æ‰«ææŒ‡å®šä»»åŠ¡
                directories_to_scan = [self.base_output_dir / task_id / "vector_bak"]
            elif global_only:
                # ä»…æ‰«æå…¨å±€ç›®å½•
                directories_to_scan = [self.base_output_dir / "vector_bak"]
            else:
                # æ‰«ææ‰€æœ‰ç›®å½•
                directories_to_scan = self._get_all_vector_bak_directories()
            
            # æ‰«ææ¯ä¸ªç›®å½•
            for backup_dir in directories_to_scan:
                if not backup_dir.exists():
                    continue
                    
                # æŸ¥æ‰¾æœ‰æ•ˆçš„å¤‡ä»½é›†
                backup_sets = self._find_backup_sets(backup_dir)
                if not backup_sets:
                    continue
                
                # æ„å»ºå¤‡ä»½ä½ç½®ä¿¡æ¯
                location_info = self._build_location_info(backup_dir, backup_sets)
                if location_info:
                    backup_locations.append(location_info)
            
            # æ„å»ºæ±‡æ€»ä¿¡æ¯
            summary = self._build_summary(backup_locations, scan_start_time)
            
            return {
                "backup_locations": backup_locations,
                "summary": summary
            }
            
        except Exception as e:
            self.logger.error(f"æ‰«æå¤‡ä»½æ–‡ä»¶å¤±è´¥: {e}")
            raise
    
    def restore_from_backup(self, backup_path: str, timestamp: str, 
                          tables: List[str] = None, pg_conn: str = None,
                          truncate_before_restore: bool = False) -> Dict[str, Any]:
        """
        ä»å¤‡ä»½æ–‡ä»¶æ¢å¤æ•°æ®
        
        Args:
            backup_path: å¤‡ä»½æ–‡ä»¶æ‰€åœ¨çš„ç›®å½•è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
            timestamp: å¤‡ä»½æ–‡ä»¶çš„æ—¶é—´æˆ³
            tables: è¦æ¢å¤çš„è¡¨ååˆ—è¡¨ï¼ŒNoneè¡¨ç¤ºæ¢å¤æ‰€æœ‰è¡¨
            pg_conn: PostgreSQLè¿æ¥å­—ç¬¦ä¸²ï¼ŒNoneåˆ™ä»configè·å–
            truncate_before_restore: æ¢å¤å‰æ˜¯å¦æ¸…ç©ºç›®æ ‡è¡¨
            
        Returns:
            æ¢å¤æ“ä½œçš„è¯¦ç»†ç»“æœ
        """
        start_time = time.time()
        
        # è®¾ç½®é»˜è®¤è¡¨åˆ—è¡¨
        if tables is None:
            tables = self.supported_tables.copy()
        
        # éªŒè¯è¡¨å
        invalid_tables = [t for t in tables if t not in self.supported_tables]
        if invalid_tables:
            raise ValueError(f"ä¸æ”¯æŒçš„è¡¨å: {invalid_tables}")
        
        # è§£æå¤‡ä»½è·¯å¾„
        backup_dir = Path(backup_path)
        if not backup_dir.is_absolute():
            # ç›¸å¯¹è·¯å¾„ï¼Œç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•
            project_root = Path(__file__).parent.parent.parent
            backup_dir = project_root / backup_path
        
        if not backup_dir.exists():
            raise FileNotFoundError(f"å¤‡ä»½ç›®å½•ä¸å­˜åœ¨: {backup_path}")
        
        # éªŒè¯å¤‡ä»½æ–‡ä»¶å­˜åœ¨
        missing_files = []
        backup_files = {}
        for table_name in tables:
            csv_file = backup_dir / f"{table_name}_{timestamp}.csv"
            if not csv_file.exists():
                missing_files.append(csv_file.name)
            else:
                backup_files[table_name] = csv_file
        
        if missing_files:
            raise FileNotFoundError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {', '.join(missing_files)}")
        
        # åˆå§‹åŒ–ç»“æœ
        result = {
            "restore_performed": True,
            "truncate_performed": truncate_before_restore,
            "backup_info": {
                "backup_path": backup_path,
                "timestamp": timestamp,
                "backup_date": self._parse_timestamp_to_date(timestamp)
            },
            "truncate_results": {},
            "restore_results": {},
            "errors": [],
            "duration": 0
        }
        
        # ä¸´æ—¶ä¿®æ”¹æ•°æ®åº“è¿æ¥é…ç½®
        original_config = None
        if pg_conn:
            from data_pipeline.config import SCHEMA_TOOLS_CONFIG
            original_config = SCHEMA_TOOLS_CONFIG.get("default_db_connection")
            SCHEMA_TOOLS_CONFIG["default_db_connection"] = pg_conn
        
        try:
            # æ‰§è¡Œæ¸…ç©ºæ“ä½œï¼ˆå¦‚æœéœ€è¦ï¼‰
            if truncate_before_restore:
                self.logger.info("ğŸ—‘ï¸ å¼€å§‹æ¸…ç©ºç›®æ ‡è¡¨...")
                for table_name in tables:
                    truncate_result = self._truncate_table(table_name)
                    result["truncate_results"][table_name] = truncate_result
                    if not truncate_result.get("success", False):
                        result["errors"].append(f"{table_name}è¡¨æ¸…ç©ºå¤±è´¥")
            
            # æ‰§è¡Œæ¢å¤æ“ä½œ
            self.logger.info("ğŸ“¥ å¼€å§‹æ¢å¤è¡¨æ•°æ®...")
            for table_name in tables:
                csv_file = backup_files[table_name]
                restore_result = self._restore_table_from_csv(table_name, csv_file)
                result["restore_results"][table_name] = restore_result
                if not restore_result.get("success", False):
                    result["errors"].append(f"{table_name}è¡¨æ¢å¤å¤±è´¥")
            
            # è®¡ç®—æ€»è€—æ—¶
            result["duration"] = time.time() - start_time
            
            # è®°å½•æœ€ç»ˆçŠ¶æ€
            if result["errors"]:
                self.logger.warning(f"âš ï¸ Vectorè¡¨æ¢å¤å®Œæˆï¼Œä½†æœ‰é”™è¯¯: {'; '.join(result['errors'])}")
            else:
                self.logger.info(f"âœ… Vectorè¡¨æ¢å¤å®Œæˆï¼Œè€—æ—¶: {result['duration']:.2f}ç§’")
            
            return result
            
        finally:
            # æ¢å¤åŸå§‹é…ç½®
            if original_config is not None:
                SCHEMA_TOOLS_CONFIG["default_db_connection"] = original_config
    
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥ - å®Œå…¨å¤ç”¨VectorTableManagerçš„è¿æ¥é€»è¾‘"""
        try:
            # æ–¹æ³•1ï¼šå¦‚æœSCHEMA_TOOLS_CONFIGä¸­æœ‰è¿æ¥å­—ç¬¦ä¸²ï¼Œç›´æ¥ä½¿ç”¨
            from data_pipeline.config import SCHEMA_TOOLS_CONFIG
            connection_string = SCHEMA_TOOLS_CONFIG.get("default_db_connection")
            if connection_string:
                conn = psycopg2.connect(connection_string)
            else:
                # æ–¹æ³•2ï¼šä»app_configè·å–pgvectoræ•°æ®åº“é…ç½®
                import app_config
                pgvector_config = app_config.PGVECTOR_CONFIG
                conn = psycopg2.connect(
                    host=pgvector_config.get('host'),
                    port=pgvector_config.get('port'),
                    database=pgvector_config.get('dbname'),
                    user=pgvector_config.get('user'),
                    password=pgvector_config.get('password')
                )
            
            # è®¾ç½®è‡ªåŠ¨æäº¤
            conn.autocommit = True
            return conn
            
        except Exception as e:
            self.logger.error(f"pgvectoræ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
            raise
    
    def _get_all_vector_bak_directories(self) -> List[Path]:
        """è·å–æ‰€æœ‰vector_bakç›®å½•"""
        directories = []
        
        # å…¨å±€å¤‡ä»½ç›®å½•
        global_backup_dir = self.base_output_dir / "vector_bak"
        if global_backup_dir.exists():
            directories.append(global_backup_dir)
        
        # ä»»åŠ¡å¤‡ä»½ç›®å½• (task_* å’Œ manual_*)
        for pattern in ["task_*", "manual_*"]:
            for task_dir in self.base_output_dir.glob(pattern):
                if task_dir.is_dir():
                    vector_bak_dir = task_dir / "vector_bak"
                    if vector_bak_dir.exists():
                        directories.append(vector_bak_dir)
        
        return directories
    
    def _find_backup_sets(self, backup_dir: Path) -> List[str]:
        """æŸ¥æ‰¾å¤‡ä»½ç›®å½•ä¸­çš„æœ‰æ•ˆå¤‡ä»½é›†"""
        # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
        collection_files = list(backup_dir.glob("langchain_pg_collection_*.csv"))
        embedding_files = list(backup_dir.glob("langchain_pg_embedding_*.csv"))
        
        # æå–æ—¶é—´æˆ³
        collection_timestamps = set()
        embedding_timestamps = set()
        
        for file in collection_files:
            timestamp = self._extract_timestamp_from_filename(file.name)
            if timestamp:
                collection_timestamps.add(timestamp)
        
        for file in embedding_files:
            timestamp = self._extract_timestamp_from_filename(file.name)
            if timestamp:
                embedding_timestamps.add(timestamp)
        
        # æ‰¾åˆ°åŒæ—¶å­˜åœ¨ä¸¤ä¸ªæ–‡ä»¶çš„æ—¶é—´æˆ³
        valid_timestamps = collection_timestamps & embedding_timestamps
        
        # æŒ‰æ—¶é—´æˆ³é™åºæ’åˆ—ï¼ˆæœ€æ–°çš„åœ¨å‰ï¼‰
        return sorted(valid_timestamps, reverse=True)
    
    def _extract_timestamp_from_filename(self, filename: str) -> Optional[str]:
        """ä»æ–‡ä»¶åä¸­æå–æ—¶é—´æˆ³"""
        # åŒ¹é…æ ¼å¼ï¼šlangchain_pg_collection_20250722_010318.csv
        pattern = r'langchain_pg_(?:collection|embedding)_(\d{8}_\d{6})\.csv'
        match = re.search(pattern, filename)
        return match.group(1) if match else None
    
    def _build_location_info(self, backup_dir: Path, backup_sets: List[str]) -> Optional[Dict[str, Any]]:
        """æ„å»ºå¤‡ä»½ä½ç½®ä¿¡æ¯"""
        if not backup_sets:
            return None
        
        # ç¡®å®šä½ç½®ç±»å‹å’Œç›¸å…³ä¿¡æ¯
        relative_path = self._get_relative_path(backup_dir)
        location_type, task_id = self._determine_location_type(backup_dir)
        
        # æ„å»ºå¤‡ä»½ä¿¡æ¯åˆ—è¡¨
        backups = []
        for timestamp in backup_sets:
            backup_info = self._build_backup_info(backup_dir, timestamp)
            if backup_info:
                backups.append(backup_info)
        
        location_info = {
            "type": location_type,
            "relative_path": relative_path,
            "backups": backups
        }
        
        if task_id:
            location_info["task_id"] = task_id
        
        return location_info
    
    def _get_relative_path(self, backup_dir: Path) -> str:
        """è·å–ç›¸å¯¹è·¯å¾„ï¼ˆUnixé£æ ¼ï¼‰"""
        try:
            # è®¡ç®—ç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•çš„è·¯å¾„
            project_root = Path(__file__).parent.parent.parent
            relative_path = backup_dir.relative_to(project_root)
            # è½¬æ¢ä¸ºUnixé£æ ¼è·¯å¾„
            return "./" + str(relative_path).replace("\\", "/")
        except ValueError:
            # å¦‚æœæ— æ³•è®¡ç®—ç›¸å¯¹è·¯å¾„ï¼Œç›´æ¥è½¬æ¢
            return str(backup_dir).replace("\\", "/")
    
    def _determine_location_type(self, backup_dir: Path) -> tuple:
        """ç¡®å®šä½ç½®ç±»å‹å’Œtask_id"""
        backup_dir_str = str(backup_dir)
        
        if "/vector_bak" in backup_dir_str.replace("\\", "/"):
            parent = backup_dir.parent.name
            if parent.startswith(("task_", "manual_")):
                return "task", parent
            else:
                return "global", None
        
        return "unknown", None
    
    def _build_backup_info(self, backup_dir: Path, timestamp: str) -> Optional[Dict[str, Any]]:
        """æ„å»ºå•ä¸ªå¤‡ä»½ä¿¡æ¯"""
        try:
            collection_file = backup_dir / f"langchain_pg_collection_{timestamp}.csv"
            embedding_file = backup_dir / f"langchain_pg_embedding_{timestamp}.csv"
            log_file = backup_dir / "vector_backup_log.txt"
            
            # æ£€æŸ¥æ–‡ä»¶å­˜åœ¨æ€§
            if not (collection_file.exists() and embedding_file.exists()):
                return None
            
            # è·å–æ–‡ä»¶å¤§å°
            collection_size = self._format_file_size(collection_file.stat().st_size)
            embedding_size = self._format_file_size(embedding_file.stat().st_size)
            
            # è§£æå¤‡ä»½æ—¥æœŸ
            backup_date = self._parse_timestamp_to_date(timestamp)
            
            return {
                "timestamp": timestamp,
                "collection_file": collection_file.name,
                "embedding_file": embedding_file.name,
                "collection_size": collection_size,
                "embedding_size": embedding_size,
                "backup_date": backup_date,
                "has_log": log_file.exists(),
                "log_file": log_file.name if log_file.exists() else None
            }
            
        except Exception as e:
            self.logger.warning(f"æ„å»ºå¤‡ä»½ä¿¡æ¯å¤±è´¥: {e}")
            return None
    
    def _parse_timestamp_to_date(self, timestamp: str) -> str:
        """å°†æ—¶é—´æˆ³è½¬æ¢ä¸ºå¯è¯»æ—¥æœŸæ ¼å¼"""
        try:
            # è§£ææ ¼å¼ï¼š20250722_010318
            dt = datetime.strptime(timestamp, "%Y%m%d_%H%M%S")
            return dt.strftime("%Y-%m-%d %H:%M:%S")
        except Exception:
            return timestamp
    
    def _build_summary(self, backup_locations: List[Dict], scan_start_time: datetime) -> Dict[str, Any]:
        """æ„å»ºæ±‡æ€»ä¿¡æ¯"""
        total_backup_sets = sum(len(loc["backups"]) for loc in backup_locations)
        global_backups = sum(len(loc["backups"]) for loc in backup_locations if loc["type"] == "global")
        task_backups = total_backup_sets - global_backups
        
        return {
            "total_locations": len(backup_locations),
            "total_backup_sets": total_backup_sets,
            "global_backups": global_backups,
            "task_backups": task_backups,
            "scan_time": scan_start_time.isoformat()
        }
    
    def _restore_table_from_csv(self, table_name: str, csv_file: Path) -> Dict[str, Any]:
        """ä»CSVæ–‡ä»¶æ¢å¤å•ä¸ªè¡¨ - ä½¿ç”¨COPY FROM STDIN"""
        try:
            start_time = time.time()
            
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯embeddingè¡¨ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†JSONæ ¼å¼
                    if table_name == "langchain_pg_embedding":
                        self._restore_embedding_table_with_json_fix(cursor, csv_file)
                    else:
                        # å…¶ä»–è¡¨ç›´æ¥ä½¿ç”¨COPY FROM STDIN
                        with open(csv_file, 'r', encoding='utf-8') as f:
                            # ä½¿ç”¨CSV HEADERé€‰é¡¹è‡ªåŠ¨è·³è¿‡è¡¨å¤´ï¼Œæ— éœ€æ‰‹åŠ¨next(f)
                            cursor.copy_expert(
                                f"COPY {table_name} FROM STDIN WITH (FORMAT CSV, HEADER)",
                                f
                            )
                    
                    # éªŒè¯å¯¼å…¥ç»“æœ
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    rows_restored = cursor.fetchone()[0]
            
            duration = time.time() - start_time
            file_size = csv_file.stat().st_size
            
            return {
                "success": True,
                "source_file": csv_file.name,
                "rows_restored": rows_restored,
                "file_size": self._format_file_size(file_size),
                "duration": duration
            }
            
        except Exception as e:
            return {
                "success": False,
                "source_file": csv_file.name,
                "error": str(e)
            }
    
    def _truncate_table(self, table_name: str) -> Dict[str, Any]:
        """æ¸…ç©ºæŒ‡å®šè¡¨"""
        try:
            start_time = time.time()
            
            with self.get_connection() as conn:
                with conn.cursor() as cursor:
                    # è·å–æ¸…ç©ºå‰çš„è¡Œæ•°
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    rows_before = cursor.fetchone()[0]
                    
                    # æ‰§è¡ŒTRUNCATE
                    cursor.execute(f"TRUNCATE TABLE {table_name}")
                    
                    # éªŒè¯æ¸…ç©ºç»“æœ
                    cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
                    rows_after = cursor.fetchone()[0]
            
            duration = time.time() - start_time
            
            if rows_after == 0:
                return {
                    "success": True,
                    "rows_before": rows_before,
                    "rows_after": rows_after,
                    "duration": duration
                }
            else:
                raise Exception(f"æ¸…ç©ºå¤±è´¥ï¼Œè¡¨ä¸­ä»æœ‰ {rows_after} è¡Œæ•°æ®")
                
        except Exception as e:
            return {
                "success": False,
                "error": str(e)
            }
    
    def _format_file_size(self, size_bytes: int) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶å¤§å°æ˜¾ç¤º"""
        if size_bytes == 0:
            return "0 B"
        
        size_names = ["B", "KB", "MB", "GB"]
        i = 0
        size = float(size_bytes)
        
        while size >= 1024.0 and i < len(size_names) - 1:
            size /= 1024.0
            i += 1
        
        return f"{size:.1f} {size_names[i]}" 
    
    def _restore_embedding_table_with_json_fix(self, cursor, csv_file: Path):
        """æ¢å¤embeddingè¡¨ï¼Œä¿®å¤cmetadataåˆ—çš„JSONæ ¼å¼é—®é¢˜"""
        import csv
        import json
        import ast
        import io
        
        # è¯»å–CSVå¹¶ä¿®å¤JSONæ ¼å¼
        corrected_data = io.StringIO()
        
        with open(csv_file, 'r', encoding='utf-8') as f:
            reader = csv.reader(f)
            writer = csv.writer(corrected_data)
            
            # å¤„ç†è¡¨å¤´
            header = next(reader)
            writer.writerow(header)
            
            # æ‰¾åˆ°cmetadataåˆ—çš„ç´¢å¼•
            try:
                cmetadata_index = header.index('cmetadata')
            except ValueError:
                # å¦‚æœæ²¡æœ‰cmetadataåˆ—ï¼Œç›´æ¥ä½¿ç”¨åŸå§‹CSV
                corrected_data.seek(0)
                corrected_data.truncate(0)
                f.seek(0)
                corrected_data.write(f.read())
                corrected_data.seek(0)
                cursor.copy_expert(
                    "COPY langchain_pg_embedding FROM STDIN WITH (FORMAT CSV, HEADER)",
                    corrected_data
                )
                return
            
            # å¤„ç†æ•°æ®è¡Œ
            for row in reader:
                if len(row) > cmetadata_index and row[cmetadata_index]:
                    try:
                        # å°è¯•å°†Pythonå­—å…¸æ ¼å¼è½¬æ¢ä¸ºJSONæ ¼å¼
                        # å¦‚æœå·²ç»æ˜¯JSONæ ¼å¼ï¼Œjson.loadsä¼šæˆåŠŸ
                        if row[cmetadata_index].startswith('{') and row[cmetadata_index].endswith('}'):
                            try:
                                # å…ˆå°è¯•ä½œä¸ºJSONè§£æ
                                json.loads(row[cmetadata_index])
                                # å·²ç»æ˜¯æœ‰æ•ˆJSONï¼Œä¸éœ€è¦è½¬æ¢
                            except json.JSONDecodeError:
                                # ä¸æ˜¯æœ‰æ•ˆJSONï¼Œå°è¯•ä½œä¸ºPythonå­—å…¸è§£æå¹¶è½¬æ¢
                                try:
                                    python_dict = ast.literal_eval(row[cmetadata_index])
                                    row[cmetadata_index] = json.dumps(python_dict, ensure_ascii=False)
                                except (ValueError, SyntaxError):
                                    # å¦‚æœéƒ½å¤±è´¥äº†ï¼Œè®°å½•é”™è¯¯ä½†ç»§ç»­å¤„ç†
                                    self.logger.warning(f"æ— æ³•è§£æcmetadata: {row[cmetadata_index]}")
                    except Exception as e:
                        self.logger.warning(f"å¤„ç†cmetadataæ—¶å‡ºé”™: {e}")
                
                writer.writerow(row)
        
        # ä½¿ç”¨ä¿®å¤åçš„æ•°æ®è¿›è¡Œå¯¼å…¥
        corrected_data.seek(0)
        cursor.copy_expert(
            "COPY langchain_pg_embedding FROM STDIN WITH (FORMAT CSV, HEADER)",
            corrected_data
        )